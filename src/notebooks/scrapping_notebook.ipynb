{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-07T10:48:45.223279Z","iopub.status.busy":"2024-06-07T10:48:45.222730Z","iopub.status.idle":"2024-06-07T10:48:46.089095Z","shell.execute_reply":"2024-06-07T10:48:46.087721Z","shell.execute_reply.started":"2024-06-07T10:48:45.223246Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import subprocess\n","from src.utility import get_root"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T10:48:46.092618Z","iopub.status.busy":"2024-06-07T10:48:46.091599Z","iopub.status.idle":"2024-06-07T10:48:52.530687Z","shell.execute_reply":"2024-06-07T10:48:52.529413Z","shell.execute_reply.started":"2024-06-07T10:48:46.092577Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  4%|▍         | 1/23 [00:00<00:11,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper Estimating the Carbon Footprint of BLOOM a 176B Parameter Language Model - [Luccioni et al. (2022)]\n","Saved output/Estimating the Carbon Footprint of BLOOM a 176B Parameter Language Model - [Luccioni et al. (2022)].txt\n","Successfully loaded paper Energy and Policy Considerations for Deep Learning in NLP - [Strubell et al. (2019)]\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▊         | 2/23 [00:00<00:08,  2.49it/s]"]},{"name":"stdout","output_type":"stream","text":["Saved output/Energy and Policy Considerations for Deep Learning in NLP - [Strubell et al. (2019)].txt\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 3/23 [00:01<00:09,  2.08it/s]"]},{"name":"stdout","output_type":"stream","text":["Failed to load paper Carbon Emissions and Large Neural Network Training - [Patterson, et al. (2021)]\n","Successfully loaded paper Quantifying the Carbon Emissions of Machine Learning - [Lacoste et al. (2019)]\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 4/23 [00:01<00:07,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["Saved output/Quantifying the Carbon Emissions of Machine Learning - [Lacoste et al. (2019)].txt\n","Successfully loaded paper Unraveling the Hidden Environmental Impacts of AI Solutions for Environment Life Cycle Assessment of AI Solutions - [Ligozat et al. (2022)]\n","Saved output/Unraveling the Hidden Environmental Impacts of AI Solutions for Environment Life Cycle Assessment of AI Solutions - [Ligozat et al. (2022)].txt\n"]},{"name":"stderr","output_type":"stream","text":[" 26%|██▌       | 6/23 [00:02<00:06,  2.61it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper Measuring the Carbon Intensity of AI in Cloud Instances - [Dodge et al. (2022)]\n","Saved output/Measuring the Carbon Intensity of AI in Cloud Instances - [Dodge et al. (2022)].txt\n","Successfully loaded paper Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models - [Anthony et al. (2020)]\n","Saved output/Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models - [Anthony et al. (2020)].txt\n","Failed to load paper Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning - [Henderson et al. (2022)]\n"]},{"name":"stderr","output_type":"stream","text":[" 43%|████▎     | 10/23 [00:03<00:03,  3.90it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper Eco2AI: carbon emissions tracking of machine learning models as the first step towards sustainable AI - [Budennyy et al. (2022)]\n","Saved output/Eco2AI: carbon emissions tracking of machine learning models as the first step towards sustainable AI - [Budennyy et al. (2022)].txt\n","Failed to load paper The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink - [Patterson et al. (2022)]\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 11/23 [00:03<00:03,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper Bridging Fairness and Environmental Sustainability in Natural Language Processing - [Hessenthaler et al. (2022)]\n","Saved output/Bridging Fairness and Environmental Sustainability in Natural Language Processing - [Hessenthaler et al. (2022)].txt\n","Failed to load paper Toward Sustainable HPC: Carbon Footprint Estimation and Environmental Implications of HPC Systems - [Li et al. (2023)]\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████    | 14/23 [00:04<00:02,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Failed to load paper Towards Sustainable Artificial Intelligence: An Overview of Environmental Protection Uses and Issues - [Pachot et al. (2022)]\n","Successfully loaded paper Sustainable AI: Environmental Implications, Challenges and Opportunities - [Wu et al. (2022)]\n","Saved output/Sustainable AI: Environmental Implications, Challenges and Opportunities - [Wu et al. (2022)].txt\n","Successfully loaded paper Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning - [Luccioni et al. (2023)]\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|██████▉   | 16/23 [00:04<00:01,  4.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Saved output/Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning - [Luccioni et al. (2023)].txt\n","Successfully loaded paper Making AI Less \"Thirsty\": Uncovering and Addressing the Secret Water Footprint of AI Models - [Li et al. (2023)]\n","Saved output/Making AI Less \"Thirsty\": Uncovering and Addressing the Secret Water Footprint of AI Models - [Li et al. (2023)].txt\n"]},{"name":"stderr","output_type":"stream","text":[" 78%|███████▊  | 18/23 [00:05<00:01,  3.87it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper A Survey on Green Deep Learning - [Xu et al. (2021)]\n","Saved output/A Survey on Green Deep Learning - [Xu et al. (2021)].txt\n","Successfully loaded paper Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference - [Stojkovic et al. (2024)]\n","Saved output/Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference - [Stojkovic et al. (2024)].txt\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 19/23 [00:05<00:01,  3.20it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training - [Liu et al. (2024)]\n","Saved output/Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training - [Liu et al. (2024)].txt\n"]},{"name":"stderr","output_type":"stream","text":[" 96%|█████████▌| 22/23 [00:06<00:00,  4.42it/s]"]},{"name":"stdout","output_type":"stream","text":["Failed to load paper Beyond Efficiency: Scaling AI Sustainably - [Wu et al. (2024)]\n","Successfully loaded paper Power Hungry Processing: Watts Driving the Cost of AI Deployment? - [Luccioni et al. (2023)]\n","Saved output/Power Hungry Processing: Watts Driving the Cost of AI Deployment? - [Luccioni et al. (2023)].txt\n","Successfully loaded paper LLMCarbon: Modeling the End-To-End Carbon Footprint of Large Language Models - [Faiz et al. (2023)]\n","Saved output/LLMCarbon: Modeling the End-To-End Carbon Footprint of Large Language Models - [Faiz et al. (2023)].txt\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 23/23 [00:07<00:00,  3.21it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper Perseus: Removing Energy Bloat from Large Model Training - [Chung et al. (2023)]\n","Saved output/Perseus: Removing Energy Bloat from Large Model Training - [Chung et al. (2023)].txt\n","Finished fetching texts.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import json\n","import re\n","from collections import namedtuple\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from tqdm import tqdm\n","import requests\n","from bs4 import BeautifulSoup\n","import os\n","\n","# Define the namedtuple for storing paper information\n","Paper = namedtuple('Paper', ['id', 'title'])\n","\n","def get_paper_links_from_github(readme_url):\n","    response = requests.get(readme_url)\n","    if response.status_code == 200:\n","        readme_content = response.text\n","\n","        # Extract paper links and titles from the README.md\n","        paper_links_with_titles = re.findall(r'\\* (.*?) \\[(.*?)\\]\\((https://arxiv\\.org/abs/\\d{4}\\.\\d{5}(?:v\\d+)?)\\)', readme_content)\n","        \n","        # Create Paper objects with extracted titles and IDs\n","        papers = [Paper(id=link[2].split('/')[-1], title=f\"{link[0]} [{link[1]}]\") for link in paper_links_with_titles]\n","        return papers\n","    else:\n","        print(\"Failed to fetch README.md from GitHub\")\n","        return []\n","\n","# Fetch text content from arXiv-Vanity\n","def get_text_from_arxiv_vanity(paper):\n","    url = f\"https://www.arxiv-vanity.com/papers/{paper.id}/\"\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        text_content = \"\"\n","        abstract_selector = \"div.ltx_abstract\"\n","        section_selectors = [f\"#S{i}\" for i in range(1, 16)] + [f\"#Ch{i}\" for i in range(1, 16)]\n","\n","        # Get abstract\n","        abstract_element = soup.select_one(abstract_selector)\n","        if abstract_element:\n","            text_content += \"Abstract:\\n\" + abstract_element.get_text() + \"\\n\\n\"\n","        \n","        # Get sections\n","        for selector in section_selectors:\n","            section_element = soup.select_one(selector)\n","            if section_element:\n","                text_content += section_element.get_text() + \"\\n\\n\"\n","            else:\n","                break\n","        \n","        return text_content.strip()\n","    return f\"Failed to fetch text for paper {paper.id}\"\n","\n","# Save the fetched content to a text file\n","def save_paper_text(title, text_content):\n","    # Ensure directory exists\n","    if not os.path.exists(\"output\"):\n","        os.makedirs(\"output\")\n","\n","    file_path = os.path.join(\"output\", f\"{title}.txt\")\n","    with open(file_path, 'w', encoding='utf-8') as f:\n","        f.write(text_content)\n","    print(f\"Saved {file_path}\")\n","\n","if __name__ == \"__main__\":\n","    readme_url = \"https://raw.githubusercontent.com/samuelrince/awesome-green-ai/main/README.md\"\n","\n","    # Extract paper links and titles from the README.md\n","    papers = get_paper_links_from_github(readme_url)\n","    if not papers:\n","        print(\"No papers found.\")\n","        exit(1)\n","\n","    # Parallelize the fetching of text content\n","    with ThreadPoolExecutor(max_workers=8) as executor:\n","        futures = {executor.submit(get_text_from_arxiv_vanity, paper): paper for paper in papers}\n","        \n","        for future in tqdm(as_completed(futures), total=len(futures)):\n","            paper = futures[future]\n","            text_content = future.result()\n","            if text_content and not text_content.startswith(\"Failed\"):\n","                print(f\"Successfully loaded paper {paper.title}\")\n","                save_paper_text(paper.title, text_content)\n","            else:\n","                print(f\"Failed to load paper {paper.title}\")\n","\n","    print(\"Finished fetching texts.\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T10:48:52.532896Z","iopub.status.busy":"2024-06-07T10:48:52.532242Z","iopub.status.idle":"2024-06-07T10:48:58.577450Z","shell.execute_reply":"2024-06-07T10:48:58.576323Z","shell.execute_reply.started":"2024-06-07T10:48:52.532863Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  5%|▌         | 1/20 [00:00<00:14,  1.35it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper **Energy and Policy Considerations for Deep Learning in NLP**\n","Saved output/__Energy and Policy Considerations for Deep Learning in NLP__.txt\n","Successfully loaded paper Estimating the Carbon Footprint of BLOOM a 176B Parameter Language Model\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|█         | 2/20 [00:00<00:07,  2.30it/s]"]},{"name":"stdout","output_type":"stream","text":["Saved output/Estimating the Carbon Footprint of BLOOM a 176B Parameter Language Model.txt\n","Successfully loaded paper Quantifying the Carbon Emissions of Machine Learning\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 4/20 [00:01<00:04,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["Saved output/Quantifying the Carbon Emissions of Machine Learning.txt\n","Successfully loaded paper Unraveling the Hidden Environmental Impacts of AI Solutions for Environment Life Cycle Assessment of AI Solutions\n","Saved output/Unraveling the Hidden Environmental Impacts of AI Solutions for Environment Life Cycle Assessment of AI Solutions.txt\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 5/20 [00:01<00:05,  2.63it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models\n","Saved output/Carbontracker_ Tracking and Predicting the Carbon Footprint of Training Deep Learning Models.txt\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 6/20 [00:02<00:04,  2.86it/s]"]},{"name":"stdout","output_type":"stream","text":["Failed to load paper Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 8/20 [00:03<00:04,  2.82it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper Chasing Carbon: The Elusive Environmental Footprint of Computing\n","Saved output/Chasing Carbon_ The Elusive Environmental Footprint of Computing.txt\n","Successfully loaded paper Measuring the Carbon Intensity of AI in Cloud Instances\n","Saved output/Measuring the Carbon Intensity of AI in Cloud Instances.txt\n","Successfully loaded paper Eco2AI: carbon emissions tracking of machine learning models as the first step towards sustainable AI\n"]},{"name":"stderr","output_type":"stream","text":[" 55%|█████▌    | 11/20 [00:03<00:01,  5.50it/s]"]},{"name":"stdout","output_type":"stream","text":["Saved output/Eco2AI_ carbon emissions tracking of machine learning models as the first step towards sustainable AI.txt\n","Successfully loaded paper Bridging Fairness and Environmental Sustainability in Natural Language Processing\n","Saved output/Bridging Fairness and Environmental Sustainability in Natural Language Processing.txt\n","Successfully loaded paper Sustainable AI: Environmental Implications, Challenges and Opportunities\n","Saved output/Sustainable AI_ Environmental Implications, Challenges and Opportunities.txt\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|██████    | 12/20 [00:04<00:02,  3.03it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper Making AI Less \"Thirsty\": Uncovering and Addressing the Secret Water Footprint of AI Models\n","Saved output/Making AI Less _Thirsty__ Uncovering and Addressing the Secret Water Footprint of AI Models.txt\n"]},{"name":"stderr","output_type":"stream","text":[" 65%|██████▌   | 13/20 [00:04<00:02,  2.75it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper Power Hungry Processing: Watts Driving the Cost of AI Deployment?\n","Saved output/Power Hungry Processing_ Watts Driving the Cost of AI Deployment_.txt\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 15/20 [00:05<00:01,  3.03it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\n","Saved output/Counting Carbon_ A Survey of Factors Influencing the Emissions of Machine Learning.txt\n","Successfully loaded paper A Synthesis of Green Architectural Tactics for ML-Enabled Systems\n","Saved output/A Synthesis of Green Architectural Tactics for ML-Enabled Systems.txt\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████  | 16/20 [00:05<00:01,  2.14it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper LLMCarbon: Modeling the End-To-End Carbon Footprint of Large Language Models\n","Saved output/LLMCarbon_ Modeling the End-To-End Carbon Footprint of Large Language Models.txt\n","Successfully loaded paper **A Systematic Review of Green AI**\n","Saved output/__A Systematic Review of Green AI__.txt\n"]},{"name":"stderr","output_type":"stream","text":[" 90%|█████████ | 18/20 [00:06<00:00,  2.56it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper A first look into the carbon footprint of federated learning\n","Saved output/A first look into the carbon footprint of federated learning.txt\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 19/20 [00:06<00:00,  2.48it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper A framework for energy and carbon footprint analysis of distributed and federated edge learning\n","Saved output/A framework for energy and carbon footprint analysis of distributed and federated edge learning.txt\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 20/20 [00:07<00:00,  2.75it/s]"]},{"name":"stdout","output_type":"stream","text":["Successfully loaded paper **A Survey on Green Deep Learning**\n","Saved output/__A Survey on Green Deep Learning__.txt\n","Finished fetching texts.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import re\n","from collections import namedtuple\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from tqdm import tqdm\n","import requests\n","from bs4 import BeautifulSoup\n","import os\n","\n","# Define the namedtuple for storing paper information\n","Paper = namedtuple('Paper', ['id', 'title'])\n","\n","def get_paper_links_from_github(readme_url):\n","    response = requests.get(readme_url)\n","    if response.status_code == 200:\n","        readme_content = response.text\n","\n","        # Extract paper links and titles from the README.md\n","        paper_links_with_titles = re.findall(r'- (.*?) \\([^()]*\\) \\[\\[Paper\\]\\]\\((https://arxiv\\.org/(?:pdf|abs)/\\d{4}\\.\\d{5}(?:v\\d+)?\\.pdf)\\)', readme_content)\n","        \n","        # Create Paper objects with extracted titles and IDs\n","        papers = [Paper(id=link[1].split('/')[-1].replace('.pdf', ''), title=link[0]) for link in paper_links_with_titles]\n","        return papers\n","    else:\n","        print(\"Failed to fetch README.md from GitHub\")\n","        return []\n","\n","# Fetch text content from arXiv-Vanity\n","def get_text_from_arxiv_vanity(paper):\n","    url = f\"https://www.arxiv-vanity.com/papers/{paper.id}/\"\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        text_content = \"\"\n","        abstract_selector = \"div.ltx_abstract\"\n","        section_selectors = [f\"#S{i}\" for i in range(1, 16)] + [f\"#Ch{i}\" for i in range(1, 16)]\n","\n","        # Get abstract\n","        abstract_element = soup.select_one(abstract_selector)\n","        if abstract_element:\n","            text_content += \"Abstract:\\n\" + abstract_element.get_text() + \"\\n\\n\"\n","        \n","        # Get sections\n","        for selector in section_selectors:\n","            section_element = soup.select_one(selector)\n","            if section_element:\n","                text_content += section_element.get_text() + \"\\n\\n\"\n","            else:\n","                break\n","        \n","        return text_content.strip()\n","    return f\"Failed to fetch text for paper {paper.id}\"\n","\n","# Save the fetched content to a text file\n","def save_paper_text(title, text_content):\n","    # Ensure directory exists\n","    if not os.path.exists(\"output\"):\n","        os.makedirs(\"output\")\n","\n","    # Sanitize title to be used as a filename\n","    safe_title = re.sub(r'[\\\\/*?:\"<>|]', \"_\", title)\n","    file_path = os.path.join(\"output\", f\"{safe_title}.txt\")\n","    with open(file_path, 'w', encoding='utf-8') as f:\n","        f.write(text_content)\n","    print(f\"Saved {file_path}\")\n","\n","if __name__ == \"__main__\":\n","    readme_url = \"https://raw.githubusercontent.com/ejhusom/green-ai/main/README.md\"\n","\n","    # Extract paper links and titles from the README.md\n","    papers = get_paper_links_from_github(readme_url)\n","    if not papers:\n","        print(\"No papers found.\")\n","        exit(1)\n","\n","    # Parallelize the fetching of text content\n","    with ThreadPoolExecutor(max_workers=8) as executor:\n","        futures = {executor.submit(get_text_from_arxiv_vanity, paper): paper for paper in papers}\n","        \n","        for future in tqdm(as_completed(futures), total=len(futures)):\n","            paper = futures[future]\n","            text_content = future.result()\n","            if text_content and not text_content.startswith(\"Failed\"):\n","                print(f\"Successfully loaded paper {paper.title}\")\n","                save_paper_text(paper.title, text_content)\n","            else:\n","                print(f\"Failed to load paper {paper.title}\")\n","\n","    print(\"Finished fetching texts.\")\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed 2021.sustainlp-1.2.pdf\n","Processed Advanced Science - 2021 - Lannelongue - Green Algorithms  Quantifying the Carbon Footprint of Computation.pdf\n","Processed A_Practical_Guide_to_Quantifying_Carbon_Emissions.pdf\n","Processed A simplified machine learning product carbon footprint evaluation.pdf.pdf\n","Processed CHASING LOW-CARBON ELECTRICITY FOR PRACTICAL AND SUSTAINABLE DNN TRAINING.pdf\n","Processed Environmental assessment of projects involving AI.pdf\n","Processed Estimating the environmental impact of Generative-AI.pdf\n","Processed GreenAI - ROY SCHWARTZ, JESSE DODGE.pdf\n","Processed Kaack_2021_Aligning.pdf\n","Processed MEASURING THE ENVIRONMENTAL IMPACTS OF AI COMPUTING.pdf\n","Processed New universal sustainability metrics to assess edge intelligence.pdf\n","Processed Timeshifting strategies for carbon-efficient long-running large language model training.pdf\n","Processed Zeus: Understanding and Optimizing GPU Energy.pdf\n","Processed A Survey on Green Deep Learning.pdf\n"]}],"source":["import fitz  # PyMuPDF\n","import os\n","\n","def pdf_to_text(pdf_path, txt_path):\n","    # Open the PDF file\n","    pdf_document = fitz.open(pdf_path)\n","    text = \"\"\n","\n","    # Iterate through each page\n","    for page_num in range(len(pdf_document)):\n","        # Get the page\n","        page = pdf_document.load_page(page_num)\n","        # Extract text from the page\n","        text += page.get_text()\n","\n","    # Write the extracted text to a text file\n","    with open(txt_path, 'w', encoding='utf-8') as txt_file:\n","        txt_file.write(text)\n","\n","def process_pdfs_in_folder(folder_path, txt_folder):\n","    # Iterate through all files in the folder\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith('.pdf'):\n","            pdf_path = os.path.join(folder_path, filename)\n","            txt_path = os.path.join(txt_folder, filename[:-4] + '.txt')\n","            pdf_to_text(pdf_path, txt_path)\n","            print(f\"Processed {filename}\")\n","\n","txt_folder = 'output'\n","folder_path = os.path.join(get_root(), 'data', 'pdfs')\n","process_pdfs_in_folder(folder_path, txt_folder)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T10:48:58.579854Z","iopub.status.busy":"2024-06-07T10:48:58.579503Z","iopub.status.idle":"2024-06-07T10:48:59.378789Z","shell.execute_reply":"2024-06-07T10:48:59.377610Z","shell.execute_reply.started":"2024-06-07T10:48:58.579826Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["No duplicates found.\n"]}],"source":["import os\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def read_paper_files(directory=\"output\"):\n","    papers = {}\n","    for filename in os.listdir(directory):\n","        if filename.endswith(\".txt\"):\n","            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n","                papers[filename] = f.read()\n","    return papers\n","\n","def find_duplicates(papers, threshold=0.9):\n","    filenames = list(papers.keys())\n","    texts = list(papers.values())\n","\n","    vectorizer = TfidfVectorizer().fit_transform(texts)\n","    vectors = vectorizer.toarray()\n","\n","    similarity_matrix = cosine_similarity(vectors)\n","\n","    duplicates = set()\n","    duplicate_pairs = []\n","    for i in range(len(filenames)):\n","        for j in range(i + 1, len(filenames)):\n","            if similarity_matrix[i, j] > threshold:\n","                if len(filenames[i]) < len(filenames[j]):\n","                    duplicates.add(filenames[j])\n","                    duplicate_pairs.append((filenames[i], filenames[j]))\n","                else:\n","                    duplicates.add(filenames[i])\n","                    duplicate_pairs.append((filenames[j], filenames[i]))\n","\n","    return duplicates, duplicate_pairs\n","\n","def delete_files(files, directory=\"output\"):\n","    for filename in files:\n","        file_path = os.path.join(directory, filename)\n","        if os.path.exists(file_path):\n","            os.remove(file_path)\n","            print(f\"Deleted {filename}\")\n","\n","if __name__ == \"__main__\":\n","    paper_texts = read_paper_files()\n","    duplicate_files, duplicate_pairs = find_duplicates(paper_texts)\n","\n","    if duplicate_files:\n","        print(\"Found duplicates:\")\n","        for dup1, dup2 in duplicate_pairs:\n","            print(f\"{dup2} is similar to {dup1}\\n\")\n","\n","        delete_files(duplicate_files)\n","    else:\n","        print(\"No duplicates found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T11:07:12.507581Z","iopub.status.busy":"2024-06-07T11:07:12.504834Z","iopub.status.idle":"2024-06-07T11:07:31.610340Z","shell.execute_reply":"2024-06-07T11:07:31.608246Z","shell.execute_reply.started":"2024-06-07T11:07:12.507520Z"},"trusted":true},"outputs":[{"ename":"ReadTimeout","evalue":"HTTPSConnectionPool(host='www.arxiv-vanity.com', port=443): Read timed out. (read timeout=None)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/connection.py:653\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    651\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 653\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/connection.py:806\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    804\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 806\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/util/ssl_.py:465\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/util/ssl_.py:509\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/lib64/python3.11/ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/lib64/python3.11/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n","File \u001b[0;32m/usr/lib64/python3.11/ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/connectionpool.py:847\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    845\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 847\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/util/retry.py:470\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/util/util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/connectionpool.py:469\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/urllib3/connectionpool.py:370\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n","\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='www.arxiv-vanity.com', port=443): Read timed out. (read timeout=None)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilar_paper\u001b[38;5;241m.\u001b[39mpaper\u001b[38;5;241m.\u001b[39mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is similar to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(similar_paper\u001b[38;5;241m.\u001b[39msimilar_to\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m text_content \u001b[38;5;241m=\u001b[39m \u001b[43mget_text_from_arxiv_vanity\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilar_paper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m save_paper_text(similar_paper\u001b[38;5;241m.\u001b[39mpaper\u001b[38;5;241m.\u001b[39mtitle, text_content)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[27], line 30\u001b[0m, in \u001b[0;36mget_text_from_arxiv_vanity\u001b[0;34m(paper)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text_from_arxiv_vanity\u001b[39m(paper):\n\u001b[1;32m     29\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.arxiv-vanity.com/papers/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 30\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     32\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n","File \u001b[0;32m~/VS Code projects/Green-AI-1/.venv/lib64/python3.11/site-packages/requests/adapters.py:713\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n","\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='www.arxiv-vanity.com', port=443): Read timed out. (read timeout=None)"]}],"source":["import os\n","import json\n","import requests\n","from bs4 import BeautifulSoup\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from tqdm import tqdm\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","from collections import namedtuple\n","\n","# Load the Universal Sentence Encoder\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","\n","Paper = namedtuple('Paper', ['id', 'title', 'abstract', 'journal_ref'])\n","\n","\n","def get_greenai_articles(data_file):\n","    with open(data_file, 'r') as f:\n","        for line in f:\n","            paper_dict = json.loads(line)\n","            id_value = paper_dict.get('id', \"\")\n","            title = paper_dict.get('title', \"\").lower()\n","            abstract = paper_dict.get('abstract', \"\").lower()\n","            yield Paper(id_value, title, abstract, paper_dict.get('journal_ref'))\n","\n","def get_text_from_arxiv_vanity(paper):\n","    url = f\"https://www.arxiv-vanity.com/papers/{paper}/\"\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        text_content = \"\"\n","        abstract_selector = \"div.ltx_abstract\"\n","        section_selectors = [f\"#{prefix}{i}\" for prefix in [\"S\", \"Ch\"] for i in range(1, 16)]\n","\n","        # Get abstract\n","        abstract_element = soup.select_one(abstract_selector)\n","        if abstract_element:\n","            text_content += \"Abstract:\\n\" + abstract_element.get_text() + \"\\n\\n\"\n","        \n","        # Get sections\n","        for selector in section_selectors:\n","            section_element = soup.select_one(selector)\n","            if section_element:\n","                text_content += section_element.get_text() + \"\\n\\n\"\n","            else:\n","                break\n","        \n","        return text_content.strip()\n","    return f\"Failed to fetch text for paper {paper}\"\n","\n","def read_paper_file(filename):\n","    with open(filename, 'r') as file:\n","        lines = file.readlines()\n","        content = ' '.join([line.strip() for line in lines])\n","    return content\n","\n","def encode_abstract(abstract):\n","    return embed([abstract])[0]\n","\n","def encode_abstracts_parallel(abstracts):\n","    with ThreadPoolExecutor() as executor:\n","        abstract_embeddings = list(tqdm(executor.map(encode_abstract, abstracts), total=len(abstracts)))\n","    return abstract_embeddings\n","\n","def find_similar_papers(query_abstract_embedding, existing_abstracts, existing_filenames, upper_thresh=0.92, lower_thresh=0.72):\n","    similar_papers = {}\n","    similarities = cosine_similarity([query_abstract_embedding], existing_abstracts)[0]\n","    for i, sim in enumerate(similarities):\n","        if sim < upper_thresh and sim > lower_thresh:\n","            paper_title = os.path.splitext(os.path.basename(existing_filenames[i]))[0]\n","            similar_papers[paper_title] = sim\n","    return similar_papers\n","\n","if __name__ == \"__main__\":\n","    root = get_root()\n","    data_file = os.path.join(root, 'data/arxiv/arxiv-metadata-oai-snapshot.json')\n","\n","    # Fetch new papers from ArXiv\n","    papers_generator = get_greenai_articles(data_file)\n","    \n","    print(\"ArXiv generator is loaded\")\n","    # Load existing papers\n","    existing_paper_folder = \"output\"\n","    existing_filenames = [os.path.join(existing_paper_folder, filename) for filename in os.listdir(existing_paper_folder) if filename.endswith(\".txt\")]\n","\n","    existing_abstracts = []\n","    for filename in tqdm(existing_filenames):\n","        text_content = read_paper_file(filename)\n","        existing_abstracts.append(encode_abstract(text_content))  # Encode the text to get the numerical representation\n","    \n","    SimilarPaper = namedtuple(\"SimilarPaper\", ['paper', 'similar_to'])\n","    similar_papers = []\n","    # Loop through new papers to find similar ones in existing papers\n","    for paper in tqdm(papers_generator):\n","        abstract_embedding = encode_abstract(paper.abstract)\n","        similar_to = find_similar_papers(abstract_embedding, existing_abstracts, existing_filenames)\n","        \n","        if similar_to:\n","            print(paper.title)\n","            similar_papers.append(SimilarPaper(paper, similar_to))\n","    if similar_papers:\n","        print(\"Found similar papers:\")\n","        for similar_paper in similar_papers:\n","            print(f\"{similar_paper.paper.title} is similar to: {', '.join(similar_paper.similar_to.keys())}\")\n","            print(\"\\n\")\n","            text_content = get_text_from_arxiv_vanity(similar_paper.paper.id)\n","            save_paper_text(similar_paper.paper.title, text_content)\n","            print(\"\\n\")\n","    else:\n","        print(\"No similar papers found.\")"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T10:51:19.292957Z","iopub.status.busy":"2024-06-07T10:51:19.292530Z","iopub.status.idle":"2024-06-07T10:51:19.433823Z","shell.execute_reply":"2024-06-07T10:51:19.432719Z","shell.execute_reply.started":"2024-06-07T10:51:19.292920Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found duplicates:\n","Quantifying the Carbon Emissions of Machine Learning - [Lacoste et al. (2019)].txt is similar to Quantifying the Carbon Emissions of Machine Learning.txt\n","\n","Estimating the Carbon Footprint of BLOOM a 176B Parameter Language Model - [Luccioni et al. (2022)].txt is similar to Estimating the Carbon Footprint of BLOOM a 176B Parameter Language Model.txt\n","\n","Measuring the Carbon Intensity of AI in Cloud Instances - [Dodge et al. (2022)].txt is similar to Measuring the Carbon Intensity of AI in Cloud Instances.txt\n","\n","Unraveling the Hidden Environmental Impacts of AI Solutions for Environment Life Cycle Assessment of AI Solutions - [Ligozat et al. (2022)].txt is similar to Unraveling the Hidden Environmental Impacts of AI Solutions for Environment Life Cycle Assessment of AI Solutions.txt\n","\n","Energy and Policy Considerations for Deep Learning in NLP - [Strubell et al. (2019)].txt is similar to __Energy and Policy Considerations for Deep Learning in NLP__.txt\n","\n","Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models - [Anthony et al. (2020)].txt is similar to Carbontracker_ Tracking and Predicting the Carbon Footprint of Training Deep Learning Models.txt\n","\n","Eco2AI: carbon emissions tracking of machine learning models as the first step towards sustainable AI - [Budennyy et al. (2022)].txt is similar to Eco2AI_ carbon emissions tracking of machine learning models as the first step towards sustainable AI.txt\n","\n","Sustainable AI: Environmental Implications, Challenges and Opportunities - [Wu et al. (2022)].txt is similar to Sustainable AI_ Environmental Implications, Challenges and Opportunities.txt\n","\n","Bridging Fairness and Environmental Sustainability in Natural Language Processing - [Hessenthaler et al. (2022)].txt is similar to Bridging Fairness and Environmental Sustainability in Natural Language Processing.txt\n","\n","A Survey on Green Deep Learning - [Xu et al. (2021)].txt is similar to __A Survey on Green Deep Learning__.txt\n","\n","Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning - [Luccioni et al. (2023)].txt is similar to Counting Carbon_ A Survey of Factors Influencing the Emissions of Machine Learning.txt\n","\n","Making AI Less \"Thirsty\": Uncovering and Addressing the Secret Water Footprint of AI Models - [Li et al. (2023)].txt is similar to Making AI Less _Thirsty__ Uncovering and Addressing the Secret Water Footprint of AI Models.txt\n","\n","Power Hungry Processing: Watts Driving the Cost of AI Deployment? - [Luccioni et al. (2023)].txt is similar to Power Hungry Processing_ Watts Driving the Cost of AI Deployment_.txt\n","\n","LLMCarbon: Modeling the End-To-End Carbon Footprint of Large Language Models - [Faiz et al. (2023)].txt is similar to LLMCarbon_ Modeling the End-To-End Carbon Footprint of Large Language Models.txt\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    duplicate_files, duplicate_pairs = find_duplicates(paper_texts)\n","\n","    if duplicate_files:\n","        print(\"Found duplicates:\")\n","        for dup1, dup2 in duplicate_pairs:\n","            print(f\"{dup2} is similar to {dup1}\\n\")\n","\n","        delete_files(duplicate_files)\n","    else:\n","        print(\"No duplicates found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":612177,"sourceId":8581546,"sourceType":"datasetVersion"}],"dockerImageVersionId":30715,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}

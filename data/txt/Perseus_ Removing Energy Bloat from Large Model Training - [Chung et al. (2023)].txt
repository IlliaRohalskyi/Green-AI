Abstract:

Abstract
Training large AI models on numerous GPUs consumes a massive amount of energy.
We observe that not all energy consumed during training directly contributes to end-to-end training throughput, and a significant portion can be removed without slowing down training, which we call energy bloat.
In this work, we identify two independent sources of energy bloat in large model training, intrinsic and extrinsic, and propose Perseus, a unified optimization framework that mitigates both.
Perseus obtains the â€œiteration timeâ€“energyâ€ Pareto frontier of any large model training job using an efficient iterative graph cut-based algorithm and schedules energy consumption of its forward and backward computations across time to remove intrinsic and extrinsic energy bloat.
Evaluation on large models like GPT-3 and Bloom shows that Perseus reduces energy consumption of large model training by up to 30%, enabling savings otherwise unobtainable before.




1 Introduction

As deep neural networks (DNNs) continue to grow in model and dataset sizeÂ [29, 25], the energy consumption of large model training is increasing as well.
For instance, training GPT-3Â [11] reportedly consumed 1,287 megawatt-hour (MWh) in 2021Â [48].
The recent surge in the development of generative AI (GenAI) and large language models (LLMs) shows no signs of deceleration in AI energy demands.


Optimizing the energy consumption of large model training is, therefore, crucial to minimizing the operational costs associated with AI, with additional benefits.
For example, low-carbon electricity, which is preferred especially in the context of sustainability commitmentsÂ [1, 4, 6, 7], is still very much a scarce and contended resourceÂ [50].
Reducing energy consumption allows more workloads to fit within the limited supply of low-carbon electricityÂ [10].
Furthermore, reducing energy consumption without slowdown reduces power draw, either allowing more aggressive datacenter power oversubscription or conversely reducing power throttling for existing workloadsÂ [35, 54, 46].


Despite many recent works on increasing the throughput of large model trainingÂ [26, 41, 19, 43, 73, 34, 27], reining in the growing energy consumption remains an open challengeÂ [55, 48].
In this paper, we aim to identify and remove energy bloat â€“ the portion of energy consumption that can be removed without throughput loss â€“ in large model training.
We identify two independent sources of energy bloat: intrinsic and extrinsic, and propose a single optimization framework that minimizes both.


Intrinsic energy bloat comes from computation imbalance when a large model is distributed across multiple GPUs with pipeline parallelism (Â§2.2).
Balancing the amount of computation in each pipeline stage is an important problem for distributed execution planningÂ [41, 19, 23, 73], but perfectly balancing every stage is not always possible because layers in a DNN are coarse-grained tensor operations with varying amounts of computation.
When all stages do not have the same computation intensity, those not on the critical path of computation run needlessly fast â€“ i.e., they consume energy that does not contribute to the overall training throughput.
Such intrinsic energy bloat opens up the opportunity to precisely slow down each non-critical computation in the pipeline such that the length of the critical path does not change.


Extrinsic energy bloat, in contrast, arises when multiple pipelines run in parallel in a synchronous fashion to scale out training to massive datasets, and one or more pipelines run slower than the others (Â§2.3).
Root causes behind such slowdowns are varied, including power/thermal throttlingÂ [75, 47, 35, 54, 46], I/O bottlenecks in the storage/networkÂ [39, 71], hardware/software failuresÂ [61, 27], etc., and the likelihood of their presence increases with the scale and duration of trainingÂ [22, 28, 64].
All pipelines running faster than the straggler pipeline are needlessly fast and wasting energy that does not affect the overall training throughput.
Extrinsic energy bloat allows us to slow down entire pipelines (while still keeping their intrinsic energy bloat low) without delaying gradient synchronization.


In this work, we propose Perseus that relies on a unified optimization framework to remove both intrinsic and extrinsic energy bloat during large model training (Â§3).
At its core, Perseus efficiently pre-characterizes the entire â€œiteration timeâ€“energyâ€ Pareto frontier, which enables it to minimize intrinsic energy bloat under normal operating conditions and to mitigate extrinsic energy bloat arising from straggler pipelines.
Existing works fall short on both fronts.
EnvPipeÂ [13] is limited only to intrinsic bloat reduction with a point solution that leads to suboptimal energy reduction.
ZeusÂ [68], in contrast, ignores intrinsic bloat altogether as it only considers single-GPU training, which also leads it to identify suboptimal â€œiteration timeâ€“energyâ€ frontiers at best.


We prove that optimally solving our optimization problem is in fact NP-hard and propose an efficient algorithm that provides an exact solution for a relaxed problem (Â§4).
At its core, our solution represents an iteration of large model training as a directed acyclic graph (DAG).
Each node in the DAG represents either a forward or backward computation in a particular stage, and nodes are connected with edges that represent computation dependencies.
Importantly, each node is annotated with its planned execution time and energy consumption, which we call the energy schedule.
Perseusâ€™s optimization algorithm starts from the energy schedule that consumes the least possible amount of energy, and iteratively speeds up the execution time of the schedule while incurring the least possible energy increase, until no more speed up is possible.
This generates all energy schedules on the â€œiteration timeâ€“energyâ€ Pareto frontier.
Minimizing intrinsic and/or extrinsic energy bloat is then as simple as choosing the appropriate energy schedule from the Pareto frontier.


While solving the optimization problem depends on accurate fine-grained computation energy profiling, modern GPUs provide energy consumption information only in coarse-grained time intervals.
Perseus introduces an online profiling technique that can accurately profile the energy consumption of fine-grained computations with low overhead (Â§5).


Evaluation on a variety of large models including GPT-3Â [11], BERTÂ [15], T5Â [51], BloomÂ [66] and a scaled-up version of Wide-ResNetÂ [69] shows that Perseus is able to reduce per-iteration energy consumption by up to 30% with negligible or no slowdown (Â§6).


Overall, we make the following contributions in this paper:


â€¢

We identify intrinsic and extrinsic energy bloat in large model training, fundamentally caused by computation time imbalance at different levels.



â€¢

We propose Perseus, an open-source unified optimization framework that aims to remove both types of energy bloat using an efficient graph cut-based algorithm.



â€¢

We evaluate Perseus on a diverse set of large model workloads and show that it significantly reduces energy bloat.








2 Motivation

First, we provide necessary background regarding large model training (Â§2.1).
Then, we introduce intrinsic (Â§2.2) and extrinsic (Â§2.3) energy bloat present in large model training.



2.1 Large Model Training

The rapid increase in DNN model size in recent years made data, tensor, and pipeline parallelismÂ [26, 41, 43, 19, 73, 34] essential ingredients for large model training.
Especially, pipeline parallelism partitions a large model into multiple stages and assigns each stage to different GPUs.
The training batch is split into multiple microbatches, and their forward and backward computations are pipelined through those stages.
Then, in order to scale out to more GPUs, such pipelines are replicated multiple times to perform data parallel training, where each pipeline computes a subset of the training batch and synchronizes gradients with one other at the end of each iteration.
Due to the synchronization barrier in the end, the next iteration can only begin when all pipelines have finished.




2.2 Intrinsic Energy Bloat




(a) 





(b) 



Figure 1: One training iteration of GPT-3 1.3B with 4 pipeline stages (partitioned with minimum imbalance ratio) and 6 microbatches on NVIDIA A100 GPUs drawn to scale.
For example, F5 and B5 in the S2 row denote Forward and Backward for the fifth microbatch on Stage 2.
The critical path of computation is traced with a blue line.
Colors are average power consumption.
The blue background indicates that GPUs consume power when blocking on communication.





Model
Size
Imbalance Ratio


4 stages
8 stages


GPT-3Â [11]
3B
1.13
1.25


7B
1.11
1.23


13B
1.08
1.17


175B
1.02
1.03


BloomÂ [66]
3B
1.13
1.25


7B
1.13
1.25


176B
1.05
1.10


BERTÂ [15]
0.1B
1.33
2.00


0.3B
1.17
1.33


T5Â [51]
0.2B
1.19
1.50


0.7B
1.05
1.11


2.9B
1.06
1.16



Wide-ResNet50Â [69]

0.8B
1.23
1.46



Wide-ResNet101Â [69]

1.5B
1.09
1.25



Table 1: Imbalance ratio between the longest and the shortest stages for various models on NVIDIA A100 GPUs. 1.00 would mean perfect balance.


We profile GPT-3 1.3B on NVIDIA A100 GPUs and visualize the timeline of one training iteration in FigureÂ 1(a).
In addition to the familiar bubbles found in any pipeline schedule [41], we observe gaps in between forward and backward computations, where the GPU is simply blocking on communication with an adjacent stage.
Such gaps exist because the computation time of each pipeline stage is not perfectly balanced.
Partitioning stages in a balanced manner is an important problem in distributed execution planningÂ [26, 41, 19, 73], but perfect balancing is difficult because DNNs are essentially a sequence of coarse-grained tensor operations with varying size.


To understand the amount of possible pipeline stage imbalance, we exhaustively enumerated all possible pipeline partitions111For Transformer-based models, we partition at the granularity of Transformer layers. For Wide-ResNet, we partition at the granularity of bottleneck layers, which are three Convolution layers wrapped with a Skip connection. to find the one with the smallest imbalance ratio, defined as the ratio of the longest stage forward computation latency to the shortest.
TableÂ 1 lists the minimum imbalance ratio for various models, which shows that perfect balance is difficult to achieve.
AppendixÂ A explains the partitioning details of each model and where imbalance comes from.


Given some amount of stage imbalance, as we have seen in FigureÂ 1(a), not all forward and backward computations are on the critical path of computation.
This means that non-critical computations running at their maximum speed is not contributing to faster iteration time, and thus simply wasting energy.
We call this intrinsic energy bloat, which can be reduced by precisely slowing down each non-critical computation so that they do not affect the critical path (FigureÂ 1(b)).
Although seemingly simple, we prove that this problem is NP-hard by reduction from the 0/1 Knapsack problem (AppendixÂ B).
Visualizations for other models are in AppendixÂ C.




2.3 Extrinsic Energy Bloat

During large model training, numerous replicas of the same pipeline run in a data parallel fashion in order to train on more data with more GPUs, and every pipeline must synchronize gradients at the end of each iteration before moving on to the next.
When one pipeline runs slower, all other pipelines must wait until the straggler pipeline finishes.
FigureÂ 2(a) illustrates this situation.
Since the straggler pipeline determines end-to-end iteration time, all other pipelines running at their fastest possible iteration time are wasteful.
We call this extrinsic energy bloat, because unlike intrinsic energy bloat, the cause of energy bloat is extrinsic to the pipeline.


Perseus finds the energy-optimal iteration time for the non-straggler pipeline, again by precisely slowing down computations in the pipeline (FigureÂ 2(b)).
However, unlike other problem settings that do not consider energy consumption, we show in SectionÂ 3.1 that fully utilizing all the slack time created by the straggler may not be energy-optimal!
Perseus can automatically make such decisions and implements the energy-optimal slowdown.





(a) 





(b) 



Figure 2: Toy example illustrating one straggler pipeline running together with a non-straggler pipeline using data parallelism.
The non-straggler wastes energy by running as fast as possible.
Perseus can precisely slow down non-straggler pipelines to reduce such extrinsic energy bloat.


Stragglers arise from numerous sources, with many introducing sizable energy bloat according to recent reports.
Thermal or power throttling in a datacenter may lower the GPUâ€™s frequency to prevent hardware or the power supply unit from being damaged, resulting in 10â€“50% slowdownÂ [75, 47, 35, 54, 46].
I/O bottlenecks in the storage or network, especially during data loading for image, video, or recommendation models, can be longer than GPU computation by up to 4Ã—4\timesÂ [39, 71], thereby acting like a persistent (virtual) straggler pipeline.
Recent failure-resilient large model training frameworksÂ [61, 27] deploy heterogeneous pipelines with different number of stages or amount of compute after failures, introducing non-uniform iteration times across pipelines until recovery.
As the scale of large model training jobs increase, the probability of the job encountering one or more causes of stragglers increasesÂ [22, 28, 64].


Stragglers due to events like thermal/power throttling, consistent I/O bottlenecks, and fault-tolerant planning can persist across many iterationsÂ [46, 39, 27].
A training system can identify such stragglers and calculate the extent of slowdown by comparing iteration times across data-parallel pipelines.
In this work, we assume that such knowledge is available and focus on how to plan time and energy consumption across time and allow quick adaptation given such information.






3 Perseus Overview

Perseus is an energy optimization system for large model training.
In this section, we present our unified optimization framework that aims to remove both types of energy bloat (Â§3.1) and walk through the workflow of Perseus (Â§3.2).



3.1 Optimization Approach




(a) 




(b) 





(c) 



Figure 3: Three cases of where the straggler pipelineâ€™s iteration time Tâ€²superscriptğ‘‡â€²T^{\prime} can be. The first case is when there are no stragglers. Tminsubscriptğ‘‡T_{\min} and Tâˆ—superscriptğ‘‡T^{*} are the shortest and longest iteration time on the â€œiteration timeâ€“energyâ€ Pareto frontier. The black dot is the default mode of execution where all computations run at the maximum speed. The green dot is the energy-optimal iteration time of the non-straggler pipeline.


Intuitively, slowing down computations selectively in a training pipeline without affecting its critical path will keep the same iteration time while reducing its energy consumption (Â§2.2).
Furthermore, when stragglers emerge, slowing down computations in a non-straggler pipeline without making it a straggler itself will reduce energy consumption even more (Â§2.3).
We formalize these two intuitions into a unified optimization framework and derive a universal prescription for a non-straggler pipelineâ€™s energy-optimal iteration time.


By controlling the execution speed of individual computations in the pipeline, our goal is to minimize the energy consumption of a pipeline, and we can safely slow down a pipelineâ€™s iteration time until the stragglerâ€™s iteration time Tâ€²superscriptğ‘‡â€²T^{\prime}:




minFsubscriptğ¹\displaystyle\min_{F}
Energyâ€‹(F)Energyğ¹\displaystyle\quad\textrm{Energy}(F)

(1)



s.t.
Timeâ€‹(F)â‰¤Tâ€²Timeğ¹superscriptğ‘‡â€²\displaystyle\quad\textrm{Time}(F)\leq T^{\prime}




where Fğ¹F is the set of GPU frequencies to run each forward and backward computation in the pipeline, and Timeâ€‹(F)Timeğ¹\textrm{Time}(F) and Energyâ€‹(F)Energyğ¹\textrm{Energy}(F) are the iteration time and energy consumption of the pipeline when executed with Fğ¹F, respectively.
Changing Fğ¹F will lead to different values of Timeâ€‹(F)Timeğ¹\textrm{Time}(F) and Energyâ€‹(F)Energyğ¹\textrm{Energy}(F) on the â€œiteration timeâ€“energyâ€ 2D plane.
However, we are only interested in (Timeâ€‹(F)Timeğ¹\textrm{Time}(F), Energyâ€‹(F)Energyğ¹\textrm{Energy}(F)) points that are on the â€œiteration timeâ€“energyâ€ Pareto frontier, a monotonically decreasing tradeoff curve.


FigureÂ 3 depicts the three cases where the stragglerâ€™s iteration time Tâ€²superscriptğ‘‡â€²T^{\prime} can possibly be w.r.t.Â the â€œiteration timeâ€“energyâ€ Pareto frontier.
Tmâ€‹iâ€‹nsubscriptğ‘‡ğ‘šğ‘–ğ‘›T_{min} is the shortest iteration time on the frontier, which coincides with the iteration time of running every computation at the maximum speed, and Tâˆ—superscriptğ‘‡T^{*} is the iteration time with minimum energy consumption.
Together, Tminsubscriptğ‘‡T_{\min} and Tâˆ—superscriptğ‘‡T^{*} bookend the Pareto frontier.




1.

When there are no stragglers (FigureÂ 3(a)), we simply operate on the point on the Pareto frontier with the same iteration time, which reduces intrinsic energy bloat.



2.

When a moderately slow straggler is detected (FigureÂ 3(b)), we additionally reduce extrinsic energy bloat by slowing down non-straggler pipelines until Tâ€²superscriptğ‘‡â€²T^{\prime}, using up all the slack time created by the straggler.



3.

Finally, when the stragglerâ€™s iteration time goes beyond the minimum-energy point Tâˆ—superscriptğ‘‡T^{*} on the frontier (FigureÂ 3(c)), we stop at Tâˆ—superscriptğ‘‡T^{*} instead of Tâ€²superscriptğ‘‡â€²T^{\prime}.
This is because slowing down beyond Tâˆ—superscriptğ‘‡T^{*} will instead increase energy.





The three cases can be merged into one universal prescription for the pipelineâ€™s energy-optimal iteration time:




Topt=minâ¡(Tâˆ—,Tâ€²).subscriptğ‘‡optsuperscriptğ‘‡superscriptğ‘‡â€²\displaystyle T_{\textrm{opt}}=\min(T^{*},T^{\prime}).

(2)



Therefore, if we pre-characterize the entire â€œiteration timeâ€“energyâ€ Pareto frontier, we know the solution of EquationÂ 1 for any feasible value of Tâ€²superscriptğ‘‡â€²T^{\prime}.
Then our system will be able to react quickly to emerging stragglers by simply looking up the set of frequencies Foptsubscriptğ¹optF_{\textrm{opt}} that leads to iteration time Toptsubscriptğ‘‡optT_{\textrm{opt}}.
We present our algorithm to efficiently characterize the entire â€œiteration timeâ€“energyâ€ Pareto frontier in SectionÂ 4.




3.2 Perseus Architecture

Energy Schedule.

Perseus represents each iteration of the training pipeline as a static directed acyclic graph (DAG), where nodes are forward and backward computations and edges are dependencies in between computations.
Each node on the computation DAG is annotated with its planned time and energy consumption, which we call the energy schedule.
Perseus realizes an energy schedule by executing each computation with a specific GPU frequency.



System Components.

FigureÂ 4 shows the high-level architecture of Perseus.
Perseus is split into a framework-independent server and a framework-integrated client.
The server runs our timeâ€“energy optimization algorithm, produces all energy schedules on the â€œiteration timeâ€“energyâ€ Pareto frontier, and caches them in a table for fast lookup.
The client profiles computations of the pipelines online during training, and realizes the energy schedule by changing the GPUâ€™s frequency during runtime.


Figure 4: Perseus architecture and workflow.



Training Lifecycle Using Perseus.

In Perseus, a training job is specified by its computation DAG.
When the job begins execution,  1 the Perseus client invokes its Online Time/Energy Profiler (Â§5) to measure the time and energy consumption of each forward and backward computation in each stage while training is running, and sends the profile to the server.


As training proceeds,  2 the server asynchronously characterizes the â€œiteration timeâ€“energyâ€ Pareto frontier of the training job (Â§4), after which  3 the Pareto-optimal energy schedule corresponding to the shortest possible iteration time Tminsubscriptğ‘‡T_{\min} is deployed to the client.
Energy schedules are realized by the clientâ€™s Asynchronous Frequency Controller, integrated into the training frameworkâ€™s pipeline execution engine (Â§5).
Pareto-optimal energy schedules are saved in a lookup table format index by Tâ€²superscriptğ‘‡â€²T^{\prime}.


During training,  4 the hardware infrastructure (e.g., data center rack power manager) notifies the Perseus server of a straggler pipeline and how much slowdown it will experience.
The Perseus server can  5 quickly react to this by looking up the Pareto-optimal energy schedule corresponding to that straggler iteration time, and deploying it to the clients.







4 Algorithm Design

In this section, we describe our algorithm to obtain the â€œiteration timeâ€“energyâ€ Pareto frontier for a training pipeline in detail.
We start by formulating the problem (Â§4.1).
Then, we provide an overview of our algorithm (Â§4.2) and describe the core subroutine in our algorithm (Â§4.3).
Finally, we extend our algorithm to support tensor parallelism, single-choice operations, and diverse pipeline schedules (Â§4.4).



4.1 Problem Formulation

Expression for Energy Consumption.

The energy consumption of a pipeline is not only from computation; it is the sum of three parts:
(1) Computation;
(2) Blocking on communication between computations; and
(3) Blocking on communication until the straggler pipeline finishes.
Formally,





âˆ‘ieiâ€‹(fi)+Pblockingâ€‹(Nâ‹…Tâˆ’âˆ‘itiâ€‹(fi))+Pblockingâ‹…Nâ‹…(Tâ€²âˆ’T)subscriptğ‘–subscriptğ‘’ğ‘–subscriptğ‘“ğ‘–subscriptğ‘ƒblockingâ‹…ğ‘ğ‘‡subscriptğ‘–subscriptğ‘¡ğ‘–subscriptğ‘“ğ‘–â‹…subscriptğ‘ƒblockingğ‘superscriptğ‘‡â€²ğ‘‡\displaystyle\sum_{i}{e_{i}(f_{i})}+P_{\textrm{blocking}}(N\cdot T-\sum_{i}{t_{i}(f_{i})})+P_{\textrm{blocking}}\cdot N\cdot(T^{\prime}-T)

(3)




=âˆ‘i(eiâ€‹(fi)âˆ’Pblockingâ‹…tiâ€‹(fi))Â¯1+Pblockingâ‹…Nâ‹…Tâ€²Â¯2absent1Â¯subscriptğ‘–subscriptğ‘’ğ‘–subscriptğ‘“ğ‘–â‹…subscriptğ‘ƒblockingsubscriptğ‘¡ğ‘–subscriptğ‘“ğ‘–2Â¯â‹…subscriptğ‘ƒblockingğ‘superscriptğ‘‡â€²\displaystyle=\underset{\leavevmode\hbox to8.53pt{\vbox to8.53pt{\pgfpicture\makeatletter\hbox{\hskip 4.26347pt\lower-4.26347pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{ }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{ }\nullfont\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }{
{{}}\hbox{\hbox{{\pgfsys@beginscope\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\pgfsys@moveto{4.06348pt}{0.0pt}\pgfsys@curveto{4.06348pt}{2.24422pt}{2.24422pt}{4.06348pt}{0.0pt}{4.06348pt}\pgfsys@curveto{-2.24422pt}{4.06348pt}{-4.06348pt}{2.24422pt}{-4.06348pt}{0.0pt}\pgfsys@curveto{-4.06348pt}{-2.24422pt}{-2.24422pt}{-4.06348pt}{0.0pt}{-4.06348pt}\pgfsys@curveto{2.24422pt}{-4.06348pt}{4.06348pt}{-2.24422pt}{4.06348pt}{0.0pt}\pgfsys@closepath\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@stroke\pgfsys@invoke{ }
}{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.5pt}{-3.22221pt}\pgfsys@invoke{ }\hbox{{\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{ }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\hbox{{1}}
}}\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope}}}
\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope}}}
}
\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope{{{}}}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}{\underline{\sum_{i}{\left(e_{i}(f_{i})-P_{\textrm{blocking}}\cdot t_{i}(f_{i})\right)}}}+\underset{\leavevmode\hbox to8.53pt{\vbox to8.53pt{\pgfpicture\makeatletter\hbox{\hskip 4.26347pt\lower-4.26347pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{ }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{ }\nullfont\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }{
{{}}\hbox{\hbox{{\pgfsys@beginscope\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{}{}{}{{}\pgfsys@moveto{4.06348pt}{0.0pt}\pgfsys@curveto{4.06348pt}{2.24422pt}{2.24422pt}{4.06348pt}{0.0pt}{4.06348pt}\pgfsys@curveto{-2.24422pt}{4.06348pt}{-4.06348pt}{2.24422pt}{-4.06348pt}{0.0pt}\pgfsys@curveto{-4.06348pt}{-2.24422pt}{-2.24422pt}{-4.06348pt}{0.0pt}{-4.06348pt}\pgfsys@curveto{2.24422pt}{-4.06348pt}{4.06348pt}{-2.24422pt}{4.06348pt}{0.0pt}\pgfsys@closepath\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@stroke\pgfsys@invoke{ }
}{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{-2.5pt}{-3.22221pt}\pgfsys@invoke{ }\hbox{{\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{ }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\hbox{{2}}
}}\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope}}}
\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope}}}
}
\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope{{{}}}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}{\underline{\vphantom{\sum_{i}}P_{\textrm{blocking}}\cdot N\cdot T^{\prime}}}




where Pblockingsubscriptğ‘ƒblockingP_{\textrm{blocking}} is the power consumption of the GPU when it is blocking on communication, Nğ‘N is the number of pipeline stages, and tiâ€‹(fi)subscriptğ‘¡ğ‘–subscriptğ‘“ğ‘–t_{i}(f_{i}) and eiâ€‹(fi)subscriptğ‘’ğ‘–subscriptğ‘“ğ‘–e_{i}(f_{i}) are the time and energy consumption of computation iğ‘–i with frequency fisubscriptğ‘“ğ‘–f_{i}, respectively.


EquationÂ 3 shows that the â€œiteration timeâ€“energyâ€ Pareto frontier of a pipeline depends on the stragglerâ€™s iteration time Tâ€²superscriptğ‘‡â€²T^{\prime} in  2.222This makes sense because if the straggler slows down more and more, the non-straggler will consume more energy waiting for the straggler to finish.
Specifically, the Pareto frontier for any Tâ€²superscriptğ‘‡â€²T^{\prime} is the Pareto frontier of Timeâ€‹(F)Timeğ¹\textrm{Time}(F) vs.  1 shifted upwards by  2.
Here,  1 does not depend on Tâ€²superscriptğ‘‡â€²T^{\prime}.
Therefore, if we characterize the Pareto frontier of Timeâ€‹(F)Timeğ¹\textrm{Time}(F) vs.  1, that frontier can be used to find Toptsubscriptğ‘‡optT_{\textrm{opt}} and Foptsubscriptğ¹optF_{\textrm{opt}} for any value of Tâ€²superscriptğ‘‡â€²T^{\prime} as shifting the frontier upwards does not affect iteration time nor the frequency assignments that lead to each iteration time.
Thus, we define



Energyâ€‹(F)=âˆ‘i(eiâ€‹(fi)âˆ’Pblockingâ‹…tiâ€‹(fi))Energyğ¹subscriptğ‘–subscriptğ‘’ğ‘–subscriptğ‘“ğ‘–â‹…subscriptğ‘ƒblockingsubscriptğ‘¡ğ‘–subscriptğ‘“ğ‘–\textrm{Energy}(F)=\sum_{i}{\left(e_{i}(f_{i})-P_{\textrm{blocking}}\cdot t_{i}(f_{i})\right)}

(4)


and characterize the Pareto frontier of Timeâ€‹(F)Timeğ¹\textrm{Time}(F) vs. Energyâ€‹(F)Energyğ¹\textrm{Energy}(F).



Finding the Pareto Frontier.

Finding one point on the â€œiteration timeâ€“energyâ€ Pareto frontier with iteration time Tâ€²superscriptğ‘‡â€²T^{\prime} is equivalent to solving the following optimization problem:




minFsubscriptğ¹\displaystyle\min_{F}
Energyâ€‹(F)Energyğ¹\displaystyle\quad\textrm{Energy}(F)

(5)



s.t.
Timeâ€‹(F)â‰¤Tâ€²Timeğ¹superscriptğ‘‡â€²\displaystyle\quad\textrm{Time}(F)\leq T^{\prime}




We call this problem Pipeline Energy Minimization (PEM).



Theorem 1.


Pipeline Energy Minimization is NP-hard.



Proof.

Reduction from 0/1 Knapsack. Details in AppendixÂ B.
âˆ



The complete Pareto frontier can be obtained by solving PEM for all Tâ€²âˆˆ[Tmin,Tâˆ—]superscriptğ‘‡â€²subscriptğ‘‡superscriptğ‘‡T^{\prime}\in[T_{\min},T^{*}], which is clearly intractable.
Therefore, we seek an appropriate relaxation of the problem.


One of the reasons PEM is NP-hard is because it is a discrete optimization problem where the possible choices of computation time and energy are discrete, which is in turn because the possible choices of GPU frequencies are discrete.
However, if choices were continuous, the problem is exactly and efficiently solvableÂ [57].
This is akin to integer linear programs becoming tractable when relaxed to linear programs.


We fit the exponential function (aâ‹…ebâ€‹t+câ‹…ğ‘superscriptğ‘’ğ‘ğ‘¡ğ‘a\cdot e^{bt}+c) to Pareto-optimal computation time and energy measurements to make the choices continuous.
We choose the exponential function due to its inherent flexibility and natural fit to data.
We show in SectionÂ 6 that this relaxation produces high-quality approximate solutions that lead to actual savings.
After relaxation, the problemâ€™s optimization variables are the time and energy consumption planned for each computation in the pipeline, or the energy schedule.





4.2 Algorithm Overview

Iteratively Discovering the Pareto Frontier.

Figure 5: Starting from the Pareto-optimal energy schedule that consumes the minimum energy, we iteratively reduce its iteration time to trace up the Pareto frontier.


While the relaxed Pipeline Energy Minimization problem is no longer NP-Hard, solving the problem for each Tâ€²âˆˆ[Tmin,Tâˆ—]superscriptğ‘‡â€²subscriptğ‘‡superscriptğ‘‡T^{\prime}\in[T_{\min},T^{*}] from scratch is inefficient.
Instead, what if we can tweak an energy schedule that is already Pareto-optimal to generate the next energy schedule adjacent to it on the Pareto frontier?
Then, we can start from one end of the Pareto frontier and trace along the frontier to the other end, and every energy schedule we encounter will be Pareto-optimal.


We visualize our strategy of navigating the Pareto frontier in FigureÂ 5.
We start from the energy schedule that consumes the minimum energy at Tâˆ—superscriptğ‘‡T^{*}, which is achieved simply by running every computation with the minimum energy.333The minimum energy consumption for each computation type can be queried from computation time/energy profiling information (Â§5).
This energy schedule is Pareto-optimal because there are no other plans that achieve the same amount of energy consumption with faster iteration time.
Then, we iteratively reduce the pipelineâ€™s iteration time by unit time Ï„ğœ\tau (e.g., 1 ms) while increasing total energy minimally, which gives us the next Pareto-optimal energy schedule.444Ï„ğœ\tau is the unit time parameter that trades off the running time of Perseusâ€™s scheduler and the granularity of energy schedules discovered by Perseus.
This process is repeated until iteration time reaches Tminsubscriptğ‘‡T_{\min}.


We note that starting from the energy schedule that consumes the maximum energy (i.e., the black dot) is incorrect.
That energy schedule is not Pareto-optimal because, although it will execute with the least amount of time, stage imbalance leaves room for energy reduction (Â§2.2).



TL;DR.



1


2Â 


3


Input: DAG ğ’¢ğ’¢\mathcal{G} of computations iâˆˆğ’¢ğ‘–ğ’¢i\in\mathcal{G}
Amount of time to reduce in one iteration Ï„ğœ\tau
Iteration time with all max frequencies Tminsubscriptğ‘‡T_{\min}


Output: Set of Pareto-optimal energy schedules ğ’«ğ’«\mathcal{P}


4


5Â 


6


â–·â–·\trianglerightÂ Begin with the minimum energy schedule 


7
pâ†â†ğ‘absentp\leftarrow Minimum energy for all computations


8
ğ’«â†{p}â†ğ’«ğ‘\mathcal{P}\leftarrow\{p\}


9


10whileÂ IterationTimeâ€‹(ğ’¢,p)>TminIterationTimeğ’¢ğ‘subscriptğ‘‡\mathrm{IterationTime}(\mathcal{G},p)>T_{\min}Â do

Â Â Â Â Â Â 
â–·â–·\trianglerightÂ Reduce time by Ï„ğœ\tau with minimal energy increase (Â§4.3) 


11Â Â Â Â Â Â 
pâ†â†ğ‘absentp\leftarrow ReduceTime(ğ’¢ğ’¢\mathcal{G}, pğ‘p, Ï„ğœ\tau)


12Â Â Â Â Â Â 
ğ’«â†ğ’«âˆª{p}â†ğ’«ğ’«ğ‘\mathcal{P}\leftarrow\mathcal{P}\cup\{p\}


13Â Â Â Â Â Â 



14 end while


15


â–·â–·\trianglerightÂ Every schedule in ğ’«ğ’«\mathcal{P} is Pareto-optimal 


16
return ğ’«ğ’«\mathcal{P}


17Â 


18




AlgorithmÂ 1 Iteratively Discovering the Pareto Frontier.


AlgorithmÂ 1 provides an overview of our optimization process.
First, the energy schedule with the minimum energy consumption is constructed by planning every computation to run with minimum energy (lineÂ 1).
Starting from there, the iteration time of the schedule is iteratively reduced by unit time Ï„ğœ\tau while incurring minimal energy increase (lineÂ 1; SectionÂ 4.3).
This is repeated until the total iteration time of the schedule can no longer be reduced, and every energy schedule encountered in the process is Pareto-optimal.





4.3 Reducing Time with Minimum Energy Increase

Figure 6: A simplified example of how to reduce iteration time by unit time Ï„ğœ\tau.
Given a 1F1B pipeline schedule with 2 stages and 3 microbatches ( 1), it is first transformed to an equivalent representation of computation DAG ( 2).
Then the Critical DAG ( 3) is obtained by considering every and only the computations on the critical path.
Our key observation is that any valid s-t cut on the Critical DAG will reduce the iteration time by unit time Ï„ğœ\tau.
Cut A and Cut B are two examples of valid s-t cut ( 4).
Either reducing the one computation associated with Cut A (
5A
) or reducing the two computations associated with Cut B (
5B
) reduces the iteration time by Ï„ğœ\tau.



In this section, we describe our core subroutine ReduceTime (AlgorithmÂ 1, lineÂ 1).
FigureÂ 6 provides visualizations of the process.
The entire procedure is given in AlgorithmÂ 2.


Node- and Edge-Centric Computation DAGs.

Originally, Perseusâ€™s representation of the computation DAG is node-centric, which has forward and backward computations as nodes and their dependencies as edges.
As a setup for subsequent steps, we convert this into an edge-centric computation DAG where computations are edges and dependencies are nodes (i.e., all incoming edges must complete before any outgoing edge can begin).
This conversion can be done by splitting each node on the original computation DAG into two nodes and connecting the two with an edge annotated with the computation on the original node.



Removing Non-Critical Computations.

Our goal is to reduce the execution time of the computation DAG by Ï„ğœ\tau, which is equivalent to reducing the length of all critical paths by Ï„ğœ\tau.
Since computations that are not on any critical path (i.e., non-critical computations) do not affect the length of the critical path, we simply remove them from the computation DAG.



Finding Computations to Speed Up.

Which computations on the DAG should we speed up in order to reduce the length of all critical paths by Ï„ğœ\tau?
The key observation is that any s-t cut on the computation DAG represents a way to reduce the execution time of the DAG by Ï„ğœ\tau.
Specifically, by speeding up the computations on all cut edges by Ï„ğœ\tau, the entire computation DAG can be sped up exactly by Ï„ğœ\tau.


FigureÂ 6 shows two examples of this.
We have already  1 transformed an energy schedule into  2 a computation DAG where computations are represented by edges, and  3 removed all non-critical computations.
Here,  4 shows two valid s-t cuts: Cut A and Cut B.
 5A speeds up the Backward computation cut by Cut A from 3â€‹Ï„3ğœ3\tau to 2â€‹Ï„2ğœ2\tau, and the iteration time of the energy schedule was reduced by Ï„ğœ\tau.
Similarly,  5B speeds up the Forward computation and Backward computation defined by Cut B from 5â€‹Ï„5ğœ5\tau to 4â€‹Ï„4ğœ4\tau and from 7â€‹Ï„7ğœ7\tau to 6â€‹Ï„6ğœ6\tau respectively, and the iteration time of the energy schedule was also reduced by Ï„ğœ\tau.
Especially, in the second case, iteration time was only reduced because computations on two parallel critical paths were sped up together.



Solving with Minimum Cut.

We have shown with examples that any s-t cut represents a way to reduce the duration of the computation DAG by Ï„ğœ\tau.
Then, a natural question is, which cut brings the smallest possible energy increase?


We can precisely map the capacity of an s-t cut to the amount of energy increase from speeding up cut edges by finding the amount of energy increase each computation will incur with the slope of the exponential function fit for that computation and defining it to be the edgeâ€™s flow capacity.
Then, our problem reduces to minimum cut, which we can solve with maximum flow.
AppendixÂ D has further details.



Converting to GPU Frequencies.

After finding the minimum capacity (minimum energy increase) cut, we will modify the durations of the computations involved in the cut, which results in a new energy schedule.
Finally, we will convert the energy schedule into GPU frequencies that can be realized by the Perseus client.
For each computation iğ‘–i, we convert its execution time tğ‘¡t to the slowest GPU frequency that will execute faster than tğ‘¡t. This is because when computations are tightly packed by our algorithm, while slightly speeding up a computation is acceptable, slowing down any computation on the critical path will directly slow down the entire DAG, increasing intrinsic energy bloat.



Time Complexity Analysis.

Our optimization algorithm has polynomial runtime.
Let Nğ‘N and Mğ‘€M respectively denote the number of stages and microbatches.
Then, the computation DAG will have Oâ€‹(Nâ€‹M)ğ‘‚ğ‘ğ‘€O(NM) number of nodes and edges, and maximum flow with Edmonds-Karp runs in Oâ€‹(N3â€‹M3)ğ‘‚superscriptğ‘3superscriptğ‘€3O(N^{3}M^{3}).
While for general DAGs the total number of steps is known to be exponential to the size of the DAGÂ [58], we prove that for DAGs that represent pipeline computations, the number of steps is Oâ€‹(N+M)ğ‘‚ğ‘ğ‘€O(N+M), yielding a final polynomial time complexity of Oâ€‹((N+M)â€‹N3â€‹M3)ğ‘‚ğ‘ğ‘€superscriptğ‘3superscriptğ‘€3O((N+M)N^{3}M^{3}).
See AppendixÂ E for proof.


In reality, commonly used number of stages (Nğ‘N) are 4 to 8 as too many increase pipeline bubble ratioÂ [43, 14].
The number of microbatches (Mğ‘€M) is typically around 4â€‹N4ğ‘4NÂ [26, 59], but recently with high data parallel degree, far less have been reported even for high-performance settingsÂ [14].
As such, the algorithm runtime is negligible in practical scenarios (Â§6.5), given that large model training time is typically several weeksÂ [48].




1


2Â 


3


Input: DAG ğ’¢ğ’¢\mathcal{G} with computations iğ‘–i
Current energy schedule pğ‘p
Amount of iteration time to reduce Ï„ğœ\tau


Output: Pareto-optimal schedule with reduced time pâ€²superscriptğ‘â€²p^{\prime}


4


5Â 


6


FunctionÂ ReduceTimeReduceTime\mathrm{ReduceTime}(ğ’¢ğ’¢\mathcal{G}, pğ‘p, Ï„ğœ\tau):

Â Â Â Â Â Â 
â–·â–·\trianglerightÂ Construct edge-centric computation DAG ( 2) 


7Â Â Â Â Â Â 
ğ’¢â†â†ğ’¢absent\mathcal{G}\leftarrow Split nodes into two and connect with edge


8Â Â Â Â Â Â 
ğ’¢â†â†ğ’¢absent\mathcal{G}\leftarrow Crash bipartite node set to one node


9Â Â Â Â Â Â 


Â Â Â Â Â Â â–·â–·\trianglerightÂ Remove non-critical computations ( 3) 


10Â Â Â Â Â Â 
Annotate earliest & latest start times for âˆ€iâˆˆğ’¢for-allğ‘–ğ’¢\forall i\in\mathcal{G}


11Â Â Â Â Â Â 
forÂ iâˆˆğ’¢ğ‘–ğ’¢i\in\mathcal{G}Â do

Â Â Â Â Â Â Â Â Â Â Â Â 
â–·â–·\trianglerightÂ Critical computations should have zero slack 


12Â Â Â Â Â Â Â Â Â Â Â Â 
ifÂ iğ‘–i has different earliest and latest startÂ then


13Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 
Remove iğ‘–i from ğ’¢ğ’¢\mathcal{G}


14Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 



15Â Â Â Â Â Â Â Â Â Â Â Â  end if


16Â Â Â Â Â Â Â Â Â Â Â Â 


17Â Â Â Â Â Â  end for


18Â Â Â Â Â Â 

Â Â Â Â Â Â â–·â–·\trianglerightÂ Find set of computations to modify ( 4) 


19Â Â Â Â Â Â 
Sğ‘†S, Tâ†â†ğ‘‡absentT\leftarrow FindMinCut(ğ’¢ğ’¢\mathcal{G}, pğ‘p)


20Â Â Â Â Â Â 


Â Â Â Â Â Â â–·â–·\trianglerightÂ Modify computation durations ( 5) 


21Â Â Â Â Â Â 
Iâ†{iâˆ£iâ€‹Â inÂ â€‹Sâˆ’Tâ€‹Â cut}â†ğ¼conditional-setğ‘–ğ‘–Â inÂ ğ‘†ğ‘‡Â cutI\leftarrow\{i\mid i\textrm{ in }S-T\textrm{ cut}\}


22Â Â Â Â Â Â 
Modify duration of âˆ€iâˆˆIfor-allğ‘–ğ¼\forall i\in I by Ï„ğœ\tau


23Â Â Â Â Â Â 


Â Â Â Â Â Â â–·â–·\trianglerightÂ Assign frequencies from planned computation times 


24Â Â Â Â Â Â 
pâ€²â†â†superscriptğ‘â€²absentp^{\prime}\leftarrow minâ¡fisubscriptğ‘“ğ‘–\min f_{i} that runs no slower than planned


25Â Â Â Â Â Â 



26Â Â Â Â Â Â return pâ€²superscriptğ‘â€²p^{\prime}


27


28


29Â 


30




AlgorithmÂ 2 ReduceTime: Reducing the execution time of a computation DAG by Ï„ğœ\tau with minimal energy increase.





4.4 Extensions

In this section, we present extensions to our optimization algorithm useful for planning large model training.


Tensor Parallelism.

It is trivial to extend our algorithm to tensor parallelism, another essential ingredient of large model training.
The observation is that tensor parallel techniques split operations in equal sizes.
Therefore, GPUs that execute different portions of the same operation consume the same amount of time and energy.
This allows Perseus to operate only on one tensor parallelism copy for each stage, decide the energy schedule for that copy, and replicate it to other copies in the same stage.
We show that Perseus works well for hybrid parallelism in SectionÂ 6.4.



Single-Choice Operations.

Apart from computation and blocking on communication, there are other operations in the training pipeline that may take non-trivial latencies.
For instance, especially on fast GPUs like A100, loading and copying input data into VRAM or communication over slower links can take considerable latency.
However, the time and energy consumption of these operations are not affected by the GPUâ€™s frequency.
Perseus can take the latency of such operations into account during planning by abstracting them as a node in the DAG with only one frequency choice.



Other Pipeline Schedules.

There are various schedules for pipeline parallel training, including GPipeÂ [26], 1F1BÂ [42], interleaved 1F1BÂ [56], and early recomputation 1F1BÂ [34].
As long as the computations on the schedule can be expressed as a DAG, Perseus can optimize its energy consumption without modification.
If there is stage imbalance, we believe that any pipeline schedule will have intrinsic energy bloat.







5 Implementation

The Perseus server is implemented in 1,500 lines of PythonÂ [3].
The client is implemented in 300 lines of Python as a library that can be imported into training frameworks.


As a reference, we have integrated the Perseus client with MerakÂ [34], whose open source implementation takes the best of Megatron-LMÂ [5] for high-performance tensor parallelism for transformer models and DeepSpeedÂ [2] for its generic pipeline execution engine.
Activation recomputationÂ [12] is enabled by default to allow large batch sizes to fit in GPUs.


Online Time/Energy Profiler.

While accurate timing measurement is well-supported, modern GPUs (NVIDIA A40, A100, and H100) update their cumulative energy consumption counterÂ [8] only once every 100 ms, which is similar to the timescale of one microbatch computation.
This makes energy measurement noisy, and no ground truth is available.


At the very moment the cumulative energy counter increases, its value should be accurate.
Therefore, Perseus polls the energy counter in the background and records the timestamp of when the counter increased and its value.
Then, collected (time, energy) points are interpolated with a linear line.
The main training process only records the start and end timing of each computation, allowing Perseus to match timestamps with the interpolated energy curve to calculate each computationâ€™s energy consumption.


The GPUâ€™s frequency is scanned from the highest to the lowest at iteration granularity.
As an optimization, when computation energy increases for five consecutive frequencies, profiling is terminated.
Beyond that point, frequencies will consume more time and energy, making them suboptimal.



Asynchronous Frequency Controller.

The controller lives in a separate process that communicates with the main training process through a pipe, because setting the GPUâ€™s frequency through NVMLÂ [8] should not block computation on the main process.
While pipeline execution engine implementations differ widely, many have separate code blocks for forward and backward.
Therefore, the controller exposes one method, set_frequency, which takes either "forward" or "backward" and sets the GPUâ€™s frequency as planned.






6 Evaluation

We evaluate Perseus on five workloads and compare it against EnvPipe and Zeus.
Our key findings are the following.




â€¢

Perseus can effectively reduce intrinsic and extrinsic energy bloat.
Training on real GPUs shows up to 28.5% energy savings using Perseus (Â§6.2).



â€¢

In emulated large-scale training scenarios, Perseus significantly outperforms the baselines by consistently providing up to 30% energy savings.
We also observe that there is a tradeoff between energy savings and scale (Â§6.3).



â€¢

Energy bloat reduction was possible because Perseus can enumerate efficient energy schedules on the â€œiteration timeâ€“energyâ€ Pareto frontier (Â§6.4).



â€¢

Perseus reduces energy bloat with low overhead (Â§6.5).






6.1 Experimental Setup

Testbed.

We run our evaluation workloads in a GPU cluster, where each node is equipped with an AMD EPYC 7513 CPU, 512 GB DRAM, and four NVIDIA A40-48G GPUs.
For A100 results, we use the node provided by Chameleon CloudÂ [30], equipped with two Intel Xeon Platinum 8380 CPUs, 512 GB DRAM, and four NVIDIA A100-80G PCIe GPUs.



Workloads and experiment parameters.

We evaluate Perseus with various workloads spanning from GPT-3Â [11], BloomÂ [66], BERTÂ [15], T5Â [51], to Wide-ResNetÂ [69].
We use model variants with 1.3B to 6.7B parameters to run the models in our testbed, and scale them up to 176B parameters in large-scale emulation.
We chose the microbatch size and number of microbatches that yield the highest throughput given the global batch size.
We use the minimum imbalance stage partitioning method described in SectionÂ 2.2 for all workloads.
AppendixÂ A lists complete model configurations, parameters, and stage partitioning details.



Baselines.

We mainly compare against two prior works:


â€¢

EnvPipeÂ [13] reduces only intrinsic energy bloat while trying to minimize slowdown in pipeline execution.
We compare Perseusâ€™s amount of energy bloat reduction against that of EnvPipe (Â§6.2, Â§6.3).



â€¢

ZeusÂ [68] characterizes the timeâ€“energy tradeoff of single GPU training.
We compare Perseusâ€™s â€œiteration timeâ€“energyâ€ Pareto frontier against that of Zeus (Â§6.4).








6.2 Reducing Energy Bloat

We start with overall energy bloat reduction â€“ both intrinsic (Â§6.2.1) and extrinsic (Â§6.2.2) â€“ achieved by Perseus and EnvPipe by running various models on our testbed.
We compare Perseus against EnvPipe, which reduces energy consumption while trying to minimize slowdown.
Both solutions use the same amount of GPU resources.



6.2.1 Intrinsic Energy Bloat Reduction







Model
Energy Savings (%)
Slowdown (%)


Perseus
EnvPipe
Perseus
EnvPipe




GPT-3 1.3B
13.2
8.8
0.1
0.1


BERT 1.3B
12.9
8.0
0.5
0.0


T5 3B
10.6
7.4
1.3
3.4


Bloom 3B
11.7
8.9
0.2
0.2


Wide-ResNet 1.5B
3.2
3.7
2.3
4.1



(a) Four stage pipeline parallelism on A100 GPUs








Model
Energy Savings (%)
Slowdown (%)


Perseus
EnvPipe
Perseus
EnvPipe




GPT-3 2.7B
21.1
21.7
0.2
5.6


BERT 1.3B
15.7
16.5
0.0
9.7


T5 3B
28.5
19.3
0.0
0.0


Bloom 3B
22.4
19.9
0.0
0.0


Wide-ResNet 1.5B
20.4
16.5
0.2
0.5



(b) Eight stage pipeline parallelism on A40 GPUs



Table 2: Intrinsic energy reduction and slowdown comparison between Perseus and EnvPipe.


TableÂ 2 compares the energy savings achieved by Perseusâ€™s minimum iteration time energy schedule (leftmost point of the â€œiteration timeâ€“energyâ€ frontier) and that by EnvPipe.


We make two observations regarding Perseus.
First, models show varying amounts of energy savings because their stage imbalance vary (TableÂ 1).
For instance, unlike other models, Wide-ResNet 1.5B on A100 after minimum imbalance stage partitioning has nearly perfect stage balance, leaving little room for non-critical computations to slow down.


Second, A40 demonstrates more energy savings compared to A100.
This is because the dynamic clock frequency range of A100 (210â€“1410 MHz) is smaller than that of A40 (210â€“1740 MHz).
Thus, tuning down the GPUâ€™s frequency yields a relatively smaller change in computation time and energy compared to those at the maximum frequency.
However, we expect the upcoming NVIDIA H100 GPU to have better savings because its maximum frequency is 1755 MHz for the PCIe version and 1980 MHz for SXMÂ [44].


EnvPipe in general provides lower energy savings, primarily due to its assumption that the final stage of a pipeline is always the heaviest, which is not always true.
Additionally, it sometimes considerably degrades iteration time because it is not aware of single-choice operations inside the pipeline (Â§4.4) and can slow down some computations too much.




6.2.2 Intrinsic + Extrinsic Energy Bloat Reduction







Model
Method
Energy Savings (%) given Tâ€²/Tsuperscriptğ‘‡â€²ğ‘‡T^{\prime}/T


# Params
1.05
1.1
1.2
1.3
1.4
1.5


GPT-3
Perseus
14.7
15.9
15.5
15.0
14.6
14.3


1.3B
EnvPipe
8.7
8.5
8.3
8.1
7.9
7.7


Bloom
Perseus
13.6
15.6
15.2
14.7
14.3
14.0


3B
EnvPipe
8.8
8.7
8.4
8.2
8.0
7.8


BERT
Perseus
14.9
16.9
16.4
15.9
15.5
15.0


1.3B
EnvPipe
7.9
7.8
7.5
7.3
7.1
6.9


T5
Perseus
15.3
18.0
17.9
17.4
16.9
16.5


3B
EnvPipe
8.4
8.2
8.0
7.8
7.6
7.4


Wide-ResNet
Perseus
9.4
12.7
12.6
12.3
12.0
11.6


1.5B
EnvPipe
4.9
4.8
4.7
4.5
4.4
4.3



(a) Four stage pipeline parallelism on A100 GPUs








Model
Method
Energy Savings (%) given Tâ€²/Tsuperscriptğ‘‡â€²ğ‘‡T^{\prime}/T


# Params
1.05
1.1
1.2
1.3
1.4
1.5


GPT-3
Perseus
24.5
26.0
25.9
25.2
24.6
24.0


2.7B
EnvPipe
22.9
22.6
22.0
21.4
20.9
20.4


Bloom
Perseus
25.5
26.4
25.9
25.2
24.6
24.0


3B
EnvPipe
19.6
19.3
18.8
18.3
17.8
17.4


BERT
Perseus
20.0
22.6
24.1
23.4
22.8
22.2


1.3B
EnvPipe
19.2
18.9
18.3
17.8
17.4
16.9


T5
Perseus
27.9
27.3
26.2
25.2
24.3
23.4


3B
EnvPipe
18.4
18.0
17.3
16.6
16.0
15.4


Wide-ResNet
Perseus
24.3
26.2
26.3
25.7
25.0
24.4


1.5B
EnvPipe
16.4
16.2
15.8
15.4
15.0
14.6



(b) Eight stage pipeline parallelism on A40 GPUs



Table 3: Energy savings of Perseus given varying degrees of straggler slowdown (Tâ€²/Tsuperscriptğ‘‡â€²ğ‘‡T^{\prime}/T).
EnvPipe cannot provide additional savings because it does not consider and cannot be applied to optimize extrinsic bloat.


When stragglers create extrinsic energy bloat, the amount of energy savings for a non-straggler pipeline depends on how much energy reduction its timeâ€“energy frontier yields for longer iteration times.
TableÂ 3 shows the amount of energy savings given varying degrees of straggler slowdown.
As described in SectionÂ 3.1, non-stragglersâ€™ iteration time is set to be minâ¡(Tâˆ—,Tâ€²)superscriptğ‘‡superscriptğ‘‡â€²\min(T^{*},T^{\prime}), and energy is reduced from (1) slowing down the pipeline itself and (2) blocking on communication for a shorter amount of time, waiting for the straggler.
Slowing down the non-stragglers beyond Tâˆ—superscriptğ‘‡T^{*} increases total energy consumption; hence, Perseus does not slow down pipelines further than Tâˆ—superscriptğ‘‡T^{*}.


The percentage of energy savings increases for lower Tâ€²superscriptğ‘‡â€²T^{\prime} values and slowly wanes when Tâ€²>Tâˆ—superscriptğ‘‡â€²superscriptğ‘‡T^{\prime}>T^{*}.
This reduction is due to longer blocking time.
That is, the absolute amount of energy savings in Joules is the largest at Tâˆ—superscriptğ‘‡T^{*}, and constant afterward.
However, energy consumption during blocking time (Tâ€²âˆ’Tâˆ—superscriptğ‘‡â€²superscriptğ‘‡T^{\prime}-T^{*}) increases for larger Tâ€²superscriptğ‘‡â€²T^{\prime} values, lowering the percentage of energy savings for Tâ€²>Tâˆ—superscriptğ‘‡â€²superscriptğ‘‡T^{\prime}>T^{*}.


Finally, the point of maximum energy savings is different for each model.
This is because different models have different Tâˆ—superscriptğ‘‡T^{*} values, which is determined by how much each stageâ€™s computation slows down on the minimum-energy frequency.





6.3 Large-Scale Emulation

Because we do not have access to a GPU cluster required to run huge models like GPT-3 175B, we use emulation grounded on fine-grained profiling for large-scale evaluation.


Emulation Methodology.







Total


# GPUs



# Pipelines



# Microbatches


per Pipelines






Global


Batch Size





1024
16
96
1536


2048
32
48


4096
64
24


8192
128
12



Table 4: Experiment parameters for GPT-3 175B and Bloom 176B.
Each pipeline has tensor parallel degree 8 and 8 pipeline stages.


We profile the time and energy consumption of each layer (e.g., Transformer decoder) in GPT-3 175B and Bloom 176B in bfloat16 to construct the time and energy profile of each stage.
Then, we run our optimization algorithm to obtain a theoretical â€œiteration timeâ€“energyâ€ frontier, and use it to report emulated savings.
We evaluate the changes in the amount of energy savings in a strong scaling setup presented in TableÂ 4.
We do not consider weak scaling, where per-pipeline batch is constant and increasing the global batch size proportionally, because varying the global batch size can affect the model qualityÂ [31, 21].
We find that, compared to actual measurements of smaller scale models on A100, the emulator always underestimates the actual percentage of energy savings by 20.2% on average (see AppendixÂ F for details).
Meaning, the savings reported by our emulation can likely be considered an empirical lower bound for actual savings.
We used A100 SXM GPUs for the A100 results in this section, which is more representative of large scale scenarios (AppendixÂ G provides PCIe results).



Results Summary.




(a) 





(a) 





(b) 



Figure 7: Energy savings breakdown of large models emulation with straggler slowdown (Tâ€²/Tsuperscriptğ‘‡â€²ğ‘‡T^{\prime}/T) 1.20 and 1,024 GPUs.


FigureÂ 7 shows the amount of energy bloat reduction for GPT-3 175B and Bloom 176B large models when slowdown degree is 1.2 on emulated 1,024 GPUs as a representative.
EnvPipe can only reduce intrinsic bloat as it does not provide an â€œiteration timeâ€“energyâ€ frontier; even for intrinsic bloat, it reduces less than Perseus.
In contrast, Perseus reduces per-iteration energy consumption by up to 30% by reducing both intrinsic and extrinsic energy bloat.



Intrinsic Bloat Reduction.




Model
GPU Type



Energy Savings (%)


given # Microbatches





12
24
48
96


GPT-3 175B
A100
15.20
14.19
13.62
13.32


A40
11.81
10.22
9.34
8.88


Bloom 176B
A100
10.47
7.06
5.23
4.28


A40
6.97
4.49
3.12
2.41



Table 5: Perseusâ€™s intrinsic energy bloat reduction of each non-straggler pipeline for GPT-3 175B and Bloom 176B with various number of microbatches. All scenarios run with 8 pipeline stages.


TableÂ 5 presents the changes of intrinsic energy bloat saving of a single pipeline in the case of GPT-3 175B and Bloom 176B with respect to various number of microbatches.
For all models, as more and more microbatches are added to the pipeline, the amount of intrinsic bloat decreases.
This is fundamentally due to the ratio of microbatches in the pipelineâ€™s warm-up and flush phase (beginning and end) vs.Â steady state phase (middle).
Microbatches in the former phase are able to slow down until their minimum energy frequency, yielding large energy savings.
However, microbatches in the latter (middle of the pipeline) cannot slow down to their full potential when the amount of stage imbalance is not large, thereby yielding modest savings.
When the number of microbatches in the pipeline increases, only the number of steady state microbatches increases, and energy reduction becomes more and more dominated by the average energy savings of steady state microbatches.



Extrinsic Bloat Reduction.




(a) 







(a) 





(b) 





(c) 





(d) 



Figure 8: Perseusâ€™s extrinsic energy savings during a single iteration by slowing down non-straggler pipelines. When Tâ€²>Tâˆ—superscriptğ‘‡â€²superscriptğ‘‡T^{\prime}>T^{*}, slowing non-stragglers down starts increasing energy consumption, hence non-stragglers do not slow down further than Tâˆ—superscriptğ‘‡T^{*}, whose location is denoted by a star. More blocking time after Tâˆ—superscriptğ‘‡T^{*} decreases overall energy savings.
Please note the different Y-axes.


We simulate stragglers in training large models with hybrid parallelism in various strong scaling configurations (TableÂ 4), where a straggler is slower than others in varying degrees from 1.05 to 1.50.
Other non-stragglers, after finishing their computation, must wait until the straggler finishes computation, where extrinsic energy bloat emerges.
We measure the amount of energy bloat and Perseusâ€™ extrinsic energy savings.


FigureÂ 8 shows the changes of extrinsic energy bloat reduction given varying degrees of straggler slowdown and various strong scale configurations.
The trend where energy saving increases until Tâ€²<Tâˆ—superscriptğ‘‡â€²superscriptğ‘‡T^{\prime}<T^{*} and wanes afterward, is consistent with what was observed in SectionÂ 6.2.2.


An interesting observation here is that there is a tradeoff between scale and energy savings: more pipelines have less percentage of energy savings or less amount of energy savings per pipeline.
It may seem intuitive to assume that more pipelines brings more energy savings, as there is only one straggler pipeline that cannot be optimized while all the other pipelines optimize their energy consumption.
However, this holds only in weak-scaling configuration, i.e., per-pipeline batch size is constant (increasing the global batch size proportionally to the number of pipelines) and it is not the case in strong-scaling configuration, where the global batch size is constant and per-pipeline batch size is decreased as more pipelines deployed.
With less number of microbatches, the ratio of pipeline bubble (time that GPUs are idle) at the beginning and end of each pipeline iteration, which cannot be eliminated by intrinsic energy bloat reduction, increases, resulting in less energy savings.





6.4 Iteration Timeâ€“Energy Frontier Comparison




(a) 




(a) 





(b) 




(c) 



Figure 9: Iteration timeâ€“energy frontiers for GPT-3, achieved by Perseus and the two baselines derived from ZeusÂ [68]. Perseus Pareto-dominates all other approaches. The dotted vertical line is the iteration time of running all GPUs at their maximum power limit, which is the default mode of operation.
Please note the different Y-axes.


The energy bloat reductions in SectionsÂ 6.2 andÂ 6.3 were made possible by the â€œiteration timeâ€“energyâ€ frontier obtained using Perseusâ€™s optimization algorithm.
Here, we further examine the frontier with different parallelization configurations and models and compare against ZeusÂ [68], which is an energy optimization framework for a single-GPU training with the training timeâ€“energy frontier.
We implemented two Zeus-based baselines to make it work in parallelization configurations and generated â€œiteration timeâ€“energyâ€ frontiers.


1.

ZeusGlobal: Scans one global power limit for all stages.



2.

ZeusPerStage: Sets one power limit per stage that balances forward computation time.





We run training and measurement under three parallelization configurations:
(a) four stage pipeline parallelism on A100;
(b) eight stage pipeline parallelism on A40; and
(c) hybrid parallelism (data parallelism 2, tensor parallelism 2, pipeline parallelism 4) on A40.
FigureÂ 9 shows the frontiers of Perseus and Zeus for different sizes of GPT-3 under three parallelization configurations.
Frontiers for other models are in AppendixÂ H.


Perseus Pareto-dominates Zeus.
ZeusGlobal is unaware of pipeline stage imbalances and slows down every stage equally, unable to reduce intrinsic energy bloat.
While ZeusPerStage can balance the forward computation time of each stage, it is unaware of the critical path of the DAG, slowing down critical computations.
In contrast, Perseus can precisely slow down non-critical computations, tightly packing computation.




6.5 Overhead of Perseus

Profiling.

Our online profiling method (Â§5) introduces extra training time by running some iterations at a lower frequency.
For our A100 workloads, the average percentage of slowdown for such iterations was 8.2%, and the average extra training time introduced by profiling was 13 minutes.
This is negligible compared to how long large model training can take.



Algorithm Runtime.

The average time spent on running Perseusâ€™s optimization algorithm (Â§4) across the five A100 workloads was 6.5 minutes, with the longest being Bloom 3B (15.7 minutes).
For our largest scale emulation experiment (GPT-3 175B on A100 with 8,192 GPUs), the algorithm ran for 87 seconds.
While the runtime of the algorithm will increase with larger DAGs for larger models, we believe the overhead be justified because training time also increases with the scale of the training job.
Finding the energy-optimal iteration time and energy schedule given the stragglerâ€™s iteration time for extrinsic energy bloat reduction is instant.







7 Related Works

Large Model Training.

Many recent works focus on enabling and accelerating large model training using 3D parallelism (data, tensor, and pipeline parallelism).
GPipeÂ [26] and PipeDreamÂ [41] were the first to introduce pipeline parallelism and explicitly discussed the difficulty of perfectly balancing computation time across stages.
Megatron-LMÂ [56, 43] is a Transformer-based large model training framework that provides efficient manually designed execution plans.
Later on, modern training frameworks that focus on large scale trainingÂ [53, 59, 36, 37, 40, 34] were introduced to support various models at scale. DeepSpeedÂ [53] introduced ZeRO redundancy optimizer that shards model states, which is recently widely used for large model trainingÂ [52, 72].
AlpaÂ [73] and GSPMD[70] are automatic parallelization framework for general DNNs.
Finally, some recent works have looked into fault-tolerant large model training as well [61, 27, 9].
Unfortunately, energy consumption is not an optimization metric for any of the major large model training frameworks.



DNN Training and Energy Consumption.

A recent line of work has highlighted the enormous amount of energy consumption and carbon emission of DNN training, including those that present observations and estimationsÂ [48, 60, 33, 16, 38] and those that propose optimization methods for training time, energy consumption, and carbon footprintÂ [74, 63, 68, 67, 32, 13].


ZeusÂ [68] is a recent work that observes the tradeoff between GPU computation time and energy consumption, but focuses on simple single-GPU training.
EnvPipeÂ [13], on the other hand, aims to reduce the energy consumption of large model training with minimum slowdown.
However, its heuristic assumes that the last pipeline stage is always the bottleneck, leading to suboptimal savings and an infinite loop during optimization.
Perseus Pareto-dominates both Zeus and EnvPipe by viewing large model training as a computation DAG and introducing a principled optimization algorithm.
Perseus is also the first to introduce the notion of extrinsic energy bloat in large model training and optimize both simultaneously.






8 Conclusion

We presented Perseus, an energy optimization system for large model training.
Perseus builds on top of observation that there are fundamental computation imbalance at different levels in large model training that causes intrinsic and extrinsic energy bloat.
We introduced a principled graph cut-based algorithm that simultaneously reduces both.


Perseus advances the state-of-the-art of DNN training energy optimization by establishing a new timeâ€“energy Pareto frontier for large model training.
The integration of Perseus into the training workflow has strong implications for the future of AI development.
Importantly, reducing energy bloat leads to practically no latency and throughput degradation, which has the potential to greatly enhance the sustainability of distributed training in the midst of recent proliferation of LLMs and GenAI.
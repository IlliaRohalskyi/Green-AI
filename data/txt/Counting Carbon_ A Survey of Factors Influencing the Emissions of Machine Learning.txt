Abstract:

Abstract.
Machine learning (ML) requires using energy to carry out computations during the model training process. The generation of this energy comes with an environmental cost in terms of greenhouse gas emissions, depending on quantity used and the energy source. Existing research on the environmental impacts of ML has been limited to analyses covering a small number of models and does not adequately represent the diversity of ML models and tasks. In the current study, we present a survey of the carbon emissions of 95 ML models across time and different tasks in natural language processing and computer vision. We analyze them in terms of the energy sources used, the amount of CO2Â emissions produced, how these emissions evolve across time and how they relate to model performance. We conclude with a discussion regarding the carbon footprint of our field and propose the creation of a centralized repository for reporting and tracking these emissions.




1. Introduction

In recent years, machine learning (ML) models have achieved high performance in a multitude of tasks such as image classification, machine translation, and object detection. However, this progress also comes with a cost in terms of energy, since developing and deploying ML models requires access to computational resources such as Graphical Processing Units (GPUs) and therefore energy to power them. In turn, producing this energy comes with a cost to the environment, given that energy generation often entails the emission of greenhouse gases (GHG) such as carbon dioxide (CO2)Â (Rodhe, 1990). On a global scale, electricity generation represents over a quarter of the global GHG emissions, adding up to 33.1 gigatonnes of CO2 in 2019Â (IEA, 2019).
Recent estimates put the contribution of the information and communications technology (ICT) sector â€“ which includes the data centers, devices and networks used for training and deploying ML models â€“ at 2â€“6Â % of global GHG emissions, although the exact number is still debatedÂ (International Telecommunication Union, 2020; Malmodin and LundÃ©n, 2018; onÂ EnergyÂ Efficiency, 2020). In fact, there is limited information about the overall energy consumption and carbon footprint of our field, how it is evolving, and how it correlates with performance on different tasks.


The goal of the current paper is to analyze the main factors influencing the carbon emissions of our field, to study the evolution across time, and to contribute towards a better understanding of the carbon emissions generated by ML models trained on different tasks and as a function of their performance. As such, our research aims to answer the following research questions:


(1)

What are the main sources of energy used for training ML models?



(2)

What is the order of magnitude of CO2 emissions produced by training ML models?



(3)

How do the CO2Â emissions produced by training ML models evolve over time?



(4)

Does more energy and CO2 lead to better model performance?





We start our article with a survey of related work in SectionÂ 2, followed by a presentation of our methodology in SectionÂ 3. In SectionÂ 4 we present our analysis, and we conclude with our proposals for future work, including a centralized hub for reporting the carbon footprint of machine learning..





2. Related work

Measuring the environmental impact of ML models is a relatively new undertaking, but one that has been gathering momentum in recent years. In the current section, we present several directions pursued in this domain, from empirical studies of specific models to the development of efficient algorithms and hardware.


Empirical studies on carbon emissions

A large proportion of research has focused on estimating the carbon emissions of specific model architectures and/or comparing the carbon emissions of two or more models and approaches. The first paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.Â (Strubell etÂ al., 2019). This perspective has since been explored further via analyses of the carbon footprint of different neural network architecturesÂ (Luccioni etÂ al., 2022; Patterson etÂ al., 2021, 2022) and the relative efficiency of different methodsÂ (Yusuf etÂ al., 2021; Naidu etÂ al., 2021). These empirical studies are very recent (post-2019), remain relatively sparse and biased towards certain research areas (i.e. Natural Language Processing), and there are many aspects of the emissions of model training that remain unexplored. In sum, there is a need for a more broad and multi-faceted analysis in order to better understand the scale and variation of carbon emissions in our community.



Tools and approaches for measuring carbon emissions

Developing standardized approaches for estimating the carbon emissions of model training has also been the focus of much workÂ (Lottick etÂ al., 2019; Schmidt etÂ al., 2021; Henderson etÂ al., 2020; Lannelongue etÂ al., 2021; Lacoste etÂ al., 2019; TrÃ©baol, 2020; Anthony etÂ al., 2020). As a result, there are several tools that exist for this purpose, such as Code Carbon and the Experiment Impact Tracker, which can be used during the model training process, or the ML CO2 Calculator, which can be used after training, all of which provide an estimate of the amount of carbon emitted. However, a recent study on different carbon estimation tools concluded that the estimates produced by different tools vary significantly and consistently under-report emissionsÂ (Bannour etÂ al., 2021). To date, there is no single, accepted approach for estimating the carbon emissions of the field, making standardized reporting and comparisons difficultÂ (Luccioni etÂ al., 2022).



Broader impacts of ML models

Several papers have been written in recent years regarding the broader societal impacts of ML models, which includes their environmental footprint. This spans research on how the size and computational demands of ML models in generalÂ (Thompson etÂ al., 2020) and large language models in particularÂ (Bender etÂ al., 2021; Bommasani etÂ al., 2021) have grown in recent years. Many strategies and directions forward have been proposed, ranging from advocating for more environmentally-conscious practice of AIÂ (Schwartz etÂ al., 2020) to adopting a sustainability mindset for the communityÂ (Wu etÂ al., 2021). However, while the documentation of aspects such as bias and safety has begun to be described in reports and articles accompanying certain recent ML models (e.g.Â (Brown etÂ al., 2020; Hoffmann etÂ al., 2022)), environmental impacts have yet to be consistently tracked and reported. Notable exceptions include recent language models such as OPTÂ (Zhang etÂ al., 2022), T0Â (Sanh etÂ al., 2021) and BLOOMÂ (Luccioni etÂ al., 2022).



Efficient algorithms and hardware

A related and complementary direction of research is the development of more efficient model architectures and approaches. For instance, approaches such as EyerissÂ (Chen etÂ al., 2019) and DistilBERTÂ (Sanh etÂ al., 2019) have made significant progress in terms of computing efficiency, enabling faster training and inference, which results in less energy usage and, indirectly, less carbon emissions, during model training. This research is gathering attention within the community, with workshops such as SustaiNLP and EMC2 growing in scope and popularity, although efficiency has yet to be a central consideration when it comes to evaluating and comparing models. However, energy-efficient benchmarks such as HULKÂ (Zhou etÂ al., 2020) have also been proposed, which take computational requirements and environmental impacts into account during model evaluation, allowing a comparison of models based on multiple criteria.



Other aspects of the carbon impact of ML

Finally, efforts have been made to quantify other factors that have an influence on the overall carbon footprint of the field of ML, including in-person versus virtual conference attendanceÂ (Skiles etÂ al., 2021), the manufacturing of computing hardwareÂ (Gupta etÂ al., 2021), life cycle analysis of the entire ML development and deployment cycleÂ (Ligozat etÂ al., 2021), as well as some initial studies regarding the carbon footprint of model deployment in production settingsÂ (Luccioni etÂ al., 2022). The relative contribution of each of these factors is still unclear, which suggests that further research is needed in order to further disentangle these factors.






3. Methodology

As stated in SectionÂ 1, the goal of this paper is descriptive â€“ to observe the evolution of the carbon emissions of our field of ML across time and to analyze the different aspects of the carbon emissions produced by training ML models. In this section, we present the different aspects and details of our methodology.



3.1. Data collection

In order to gather data from a diverse set of ML models from a variety of domains and tasks, we leveraged the dataset collected by Thompson et al.Â (Thompson etÂ al., 2020) in the scope of a recent study on the computational requirements of ML. From this dataset, we equally sampled 500 papers published from 2012 to 2021 spanning 5 tasks: Image Classification, Object Detection, Machine Translation, Question Answering and Named Entity Recognition. We then contacted the first author of each of the papers and asked them to provide missing training details regarding their model (See Supplementary MaterialsÂ A.1 for the email text). We were able to collect information for a total of 95 models from 77 papers (since some of the papers trained more than one model), which represents an author response rate of 15.4Â %.


Table 1. Summary of the models analyzed in our study



Task
Dataset
Number of Models
Publication dates




Image Classification
ImageNetÂ (Deng etÂ al., 2009)

35
2012-2021


Machine Translation
WMT2014Â (Bojar etÂ al., 2014)

30
2016-2021


Named Entity Recognition
CoNLL 2003Â (Sang and DeÂ Meulder, 2003)

11
2015-2021


Question Answering
SQuAD 1.1Â (Rajpurkar etÂ al., 2016)

10
2016-2021


Object Detection
MS COCOÂ (Lin etÂ al., 2014)

9
2019-2021





The models in our sample cover a diversity of tasks spanning nine years of research in the field and a variety of conferences and journals. They all represent novel architectures at the time of publication, achieving high performance in their respective tasks: on average, the models are within 8Â % of SOTA performance according to Papers With Code leaderboards
at the time of their publication
. This sample represents the largest amount of information regarding the carbon footprint of ML model training to date, and provides us with opportunities to analyze it from a variety of angles, which we present in SectionÂ 4. In the remaining of this section, we describe our method for estimating carbon emissions.




3.2. Estimating carbon emissions

The unit of measurement typically used for quantifying and comparing carbon emissions is CO2Â equivalents. This unit allows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of CO2Â  emitted per kilowatt hour of electricity generated (gCO2eq/kWh)Â 111For instance, methane is 28 times more potent than CO2Â based on its 100-year global warming potential, so energy generation emitting 1 gram of methane per kWh will emit 28 grams of CO2eqÂ per kWh..


The amount of CO2eqÂ (Cğ¶C) emitted during model training can be decomposed into three relevant factors: the power consumption of the hardware used (Pğ‘ƒP), the training time (Tğ‘‡T) and the carbon intensity of the energy grid (Iğ¼I); or equivalently, the energy consumed (Eğ¸E) and the carbon intensity:


(1)

C=PÃ—TÃ—I=EÃ—I.ğ¶ğ‘ƒğ‘‡ğ¼ğ¸ğ¼C=P\times T\times I=E\times I.



For instance, a model trained on a single GPU consuming 300Â W for 100 hours on a grid that emits 500Â gCO2eq/kWhÂ will emit 0.3â€‹kWÃ—100â€‹hÃ—500â€‹g/kWh=15000â€‹g=15â€‹kg0.3kW100h500g/kWh15000g15kg0.3~{}\text{kW}\times 100~{}\text{h}\times 500~{}\text{g/kWh}=15000~{}\text{g}=15~{}\text{kg} of CO2eq. The same model trained on a less carbon-intensive energy grid, emitting only 100Â gCO2eq/kWh, will only emit 0.3Ã—100Ã—100=3000â€‹g=3â€‹kg0.31001003000g3kg0.3\times 100\times 100=3000~{}\text{g}=3~{}\text{kg} of CO2eq, i.e. five times less overall.
In our email to authors, we asked them to provide the details we needed to carry out this calculation, i.e the location of the computer or server where their model was trained (either cloud or local), the hardware used, and the total model training time. We describe how we estimate each of the relevant factors in the paragraphs below:


Carbon Intensity

Based on the training location provided by authors, we were able to estimate the carbon intensity of the energy grid that was utilized, based on publicly-available sources such as the International Energy Agency and the Energy Information Administration. The granularity of information available ranges widely depending on the location â€“ whereas in countries such as the United States, it is available at a sub-state (sometimes even at a sub-zip code) level, in others such as China, only country-level information is available. The carbon intensity figures that we use are yearly averages for the year the model was trained, given that these can evolve over time. In cases when the authors indicated that they used a computing infrastructure internal to a company, we consulted company reports and publications (e.g.Â (Patterson etÂ al., 2021; Facebook, 2020)) to obtain more precise information regarding the carbon intensity, including the usage of local renewable energy sources. In cases when models were trained on commercial cloud computing platforms such as Google Cloud or Amazon Web Services (AWS), we used the information provided by the companies themselves to estimate emission factorsÂ (Google, 2022; Amazon Web Services, 2021).



Hardware power

In order to calculate the power consumption of the hardware used for model training, we refer to its Thermal Design Power, or TDP, which indicates the energy it needs under the maximum theoretical load. That is, the higher the TDP, the more power is consumed. While in practice GPUs are not always fully utilized during all parts of the training process, gathering more precise information regarding real-time power consumption is only possible by using a tool like Code Carbon during the training processÂ (Schmidt etÂ al., 2021). Nonetheless, the TDP-based approach is often used in practice when estimating the carbon emissions of AI model trainingÂ (Patterson etÂ al., 2021) and it remains a fair approximation of the actual energy consumption of many hardware models. We provide more information about TDP and the hardware used for training the models in our sample in SectionÂ A.2 of the Appendix.



Training Time

Training time was computed as the total number of hardware hours, which is different from the "wall time" of ML model training, since most models were trained on multiple units at once. For instance, if training a model used 16 GPUs for 24 hours, this equals a training time of 384 GPU hours; a model using 8 GPUs for 48 hours will therefore have an equivalent training time.







4. Data analysis

In the sections below, we present several aspects regarding the carbon footprint of training ML models, examining the main sources of energy used for training (Â§Â 4.1), the order of magnitude of CO2Â emissions produced (Â§Â 4.2), the evolution of these emissions over time (Â§Â 4.3) and the relationship between carbon emissions and model performance (Â§Â 4.4)Â 222We have made the data used for our analysis available in a GitHub repository..



4.1. What are the main sources of energy used for training ML models?

The primary energy source used for powering an electricity grid is the single biggest influence on the carbon intensity of that grid, in the face of the large differences between energy sources. For instance, renewable energy sources like hydroelectricity, solar and wind have low carbon intensity (ranging from 11 to 147 gCO2eq/kWh), whereas non-renewable energy sources like coal, natural gas and oil are generally orders of magnitude more carbon-intensive (ranging from 360 to 680 gCO2eq/kWh)Â (IEA, 2019; SchlÃ¶mer etÂ al., 2014). That means that the energy source that powers the hardware to train ML models can result in differences of up to 60 times more CO2eqÂ in terms of total emissions.


Table 2. Main Energy Sources for the models analyzed and their carbon intensitiesÂ (IEA, 2019; United States Energy Information Administration, 2021)



Main energy source
Number of Models
Low-Carbon?



Average Carbon Intensity


(gCO2eq/kWh)







Coal
38
No
512.3


Natural Gas
23
No
350.5


Hydroelectricity
19
Yes
100.6


Oil
12
No
453.6


Nuclear
3
Yes
147.2





In TableÂ 2, we show the principal energy source used by the models from our sample, as well as its average carbon intensity.
We found that the majority of models (61) from our sample used high-carbon energy sources such as coal and natural gas as their primary energy source. whereas less than a quarter of the models (34) used low-carbon energy sources like hydroelectricity and nuclear energyÂ 333Although the sustainability of nuclear energy is debated, it is one of the least carbon-intensive sources of electricity that currently exists. More information about nuclear energy and its long-term impacts on the environment can be found inÂ (Apergis etÂ al., 2010) and Â (Suman, 2018).. While the average carbon intensity used for training the models from our sample (372Â gCO2eq/kWh) is lower than the average global carbon intensity (475Â gCO2eq/kWh), this still leaves much to improve in terms of carbon emissions of our field by switching to renewable energy sources (we discuss this further in SectionÂ 5).


In FigureÂ 1, we show the model training locations reported by authors on a country-level, with the median carbon intensity of each country indicated below. In terms of the model training locations reported by authors, we found a very imbalanced distribution, with the vast majority of models being trained in a small number of countries â€“ half of the models in our sample were trained in the United States (48), followed by China (18), with the rest of the models distributed across 9 other countries, with only a few papers in each. Regarding the primary energy sources, based on this country-level analysis of energy grids used for training the models in our sample, we found that most common countries where model training was carried out (e.g. the US and China), are on the high end of the carbon spectrum, with emissions of 350 gCO2eq/kWhÂ and above. On the other end, the countries with the lowest carbon intensity in our sample are Canada (which ranges between 1.30 and 52.89Â gCO2eq/kWh,Â depending on the province) and Spain (which has a single national energy grid with a median carbon intensity of 220.26Â gCO2eq/kWh), but they only represents a total of 7 models from our sample. This is similar to patterns in emissions worldwide, where a small number of highly industrialized countries produce the majority of the worldâ€™s greenhouse gasesÂ (Friedrich etÂ al., 2020).


Figure 1. Map with the countries where the models in the data were trained, as reported by the authors. The colors code the median carbon intensity of the energy used by the models trained in each country. The legend indicates the number of models trained in each country, as well as a colored patch marking the main energy source â€“ see bottom of the legend for the values.


Another observation that can be made based on our data is that none of the models from our sample were trained in either Africa nor South America â€“ in fact, the majority of the models from our sample (76) were trained in countries representing the Global North. This is consistent with previous work examining the â€˜digital divideâ€™ in ML and observing the centralization of power in the field, which hinders researchers from underrepresented locations and groups from contributing to the field, given the attribution of computing resourcesÂ (Birhane etÂ al., 2021; Ahmed and Wahed, 2020; Ahia etÂ al., 2021). Generally speaking, emissions, matters of equity and accessibility are closely connected to those around climate change, and the centralization of resources remains a major problemÂ (Mattoo and Subramanian, 2012; Morgan and Waskow, 2014).




4.2. What is the order of magnitude of CO2Â emissions produced by training ML models?

As explained in SectionÂ 3, there is a linear relationship between the energy consumed and the carbon emissions produced, with the energy source (discussed in the Section above) influencing the magnitude of this relationship. In FigureÂ 2, we plot the energy consumed (X axis, logarithmic scale) and the CO2Â emitted (Y axis, logarithmic scale) of every model in our data set, color-coded with the main energy source, which are the same as those presented in TableÂ 2. First, we can observe differences of several orders of magnitude in the energy used by models in our sample, ranging from just about 10 kWh to more than 10,000 kWh, which results in similar differences in the total quantity of CO2 emitted. As expected, the relationship between energy consumed and carbon emitted is largely linear. However, FigureÂ 2 also shows that models trained with cleaner energy sources, such hydroelectricity, largely deviate from the main trend, with orders of magnitude less carbon emissions compared to models trained using coal and gas. In other words, models trained with low carbon-intensive energy sources, result in much less carbon emissions, ceteris paribus.


Figure 2. Estimated energy consumed (kWh) and CO2Â (kg) by each model in the data set, plotted in a log-log scale. Colors indicate the principal energy source, and the size of the dot carbon intensity. While the relationship between energy and carbon emissions is mostly linear, the data show that models trained with less carbon-intensive energy (e.g. hydroelectric) emit orders of magnitude less carbon than those trained using more carbon-intensive energy (e.g. coal).


For instance, honing in on the central bottom portion of FigureÂ 2, it can be seen that the models trained using hydroelectricity (the blue dots) are about two orders of magnitude lower in terms of carbon emissions than models that consumed similar amounts of energy from more carbon-intensive sources such as coal (in brown) and gas (in orange), given that the Y axis is on a logarithmic scale. Furthermore, the size of the dots varies as a function of the carbon intensity of the electricity grid used, illustrating two parallel groups of models, both exhibiting a largely linear trend, with the more carbon intensive models positioned higher than the lower carbon models for similar amounts of energy consumed. This further supports the analysis carried out in SectionÂ 4.1, suggesting that the primary energy source used for training ML models has a strong impact on the overall resulting emissions from model training, and that choosing a low-carbon energy grid can play a significant role towards reducing the carbon emissions of ML model training.


Besides the primary energy source, carbon emissions are a function of power consumed by the hardware used and the training time. The choice of hardware has a relatively small influence on the large variation of carbon emissions that we observe in our sample , given that the TDP ranges from 180 W to 300 W, while the carbon emissions span from 105superscript10510^{5} kgCO2eq to even less than 10Â kgCO2eq (see SectionÂ A.2 of the appendix for further details). While using renewable energy can reduce up to 1,000 the carbon emissions for the same amount of energy used, the remaining factor responsible for the large variation in both energy and carbon emissions in our sample is therefore the training time.




4.3. How do the CO2Â emissions produced by training ML models evolve over time?

Some recent analyses have predicted that the carbon emissions of our field will increase in the future, estimating that achieving further progress on benchmarks such as ImageNet will require emitting thousands of tons ofÂ CO2Â (Thompson etÂ al., 2020), whereas others have predicted a plateau in future emissions due to increased hardware efficiency and carbon offsettingÂ (Patterson etÂ al., 2022). Therefore, one of the goals of our study was to observe the evolution of carbon emissions over time and study whether there are clear trends. Given that the papers from our study span from 2012 to the present time, we aimed to specifically compare whether new generations of ML models from our sample consistently used more energy and emitted more carbon than previous ones.


Figure 3. CO2Â emitted (in kg) by the all models included in the data set, on a logarithmic scale. Each small marker corresponds to a model and the large markers indicate the 99 % trimmed mean within each task and year(s) of publication. The error lines cover the bootstrapped 99 % confidence intervals. The gray line corresponds to the average over all tasks.


In FigureÂ 3, we show the carbon emissions emitted by every model from our sample, disaggregated by task and by year. While we cannot claim that the models and papers in our data set are fully representative of the whole machine learning field, a sample of 95 models spanning 9 years can offer interesting insights. The first observation, related to the conclusions from the sections above, is that there is a large variability in the carbon emissions from ML models. Second, we do not observe a consistent trend by which carbon emissions have systematically increased for each individual task. This is the case, for instance, of image classification models (in blue) and question answering models (in yellow) from our sample. However, the carbon emissions of machine translation models have peaked in 2019 and has since decreased.


If we look at the aggregated data from all tasks (grand average curve, in light gray), we can observe that overall, the carbon emissions per model have increased by a factor of about 100 (two orders of magnitude) from 2012 to recent years, with slight fluctuations, as in 2020. It is important to note that the vertical axis of FigureÂ 3 is on a logarithmic scale, in order to reflect the non-linearity introduced by the much larger models from recent years, even though they do not represent a majority in the sample. In fact, the last three years of our sample (2019-2021), have seen models that have emitted orders of magnitude more carbon than before: e.g. there are several vertical outliers in tasks such as Image Classification (shown in blue) and Question Answering (in yellow) that have set new records in terms of the total amount of emissions produced by model training, responsible for about 104superscript10410^{4} and 105superscript10510^{5} kilograms ofÂ CO2eq. There are several possible explanations for this, ranging from the widespread adoption of Transformers, which are using increasing amounts of both labeled and unlabeled dataÂ (Vaswani etÂ al., 2017), as well as computationally-expensive techniques such as NASÂ (Zoph and Le, 2016), which result in more carbon emissionsÂ (Strubell etÂ al., 2019). It is hard to disentangle the influence of different factors on the overall carbon emissions of ML models, as well as the relative contributions of different parts of the pre-training and fine-tuning process â€“ this requires further work, which we discuss in SectionÂ 5.3 â€“ however, it is worth noting the evolution of emissions in recent years, among the papers of our sample.




4.4. Does more energy and CO2 lead to better model performance?

A final perspective from which we analyze the carbon emissions of ML models is by comparing the amount of carbon emitted by models to their performance on benchmark tasks such as image classification, machine translation and question answering.
We compare the emissions of the models from our sample and their performance on four tasks: image recognition on ImageNetÂ (Deng etÂ al., 2009) (35 models), machine translation for English-French and English-German on the 2014 WMT Translation tasksÂ (Bojar etÂ al., 2014) (30 models), question answering on the SQuAD 1.1 datasetÂ (Rajpurkar etÂ al., 2016) (10 models), and named entity recognition on the CoNLL 2003 datasetÂ (Sang and DeÂ Meulder, 2003) (11 models)Â 444We also had data from a fifth task, object detection, which is represented in TableÂ 1 and FigureÂ 3, but we did not have enough distinct data points to enable a meaningful comparison.. Our goal with this analysis is to validate whether, generally speaking, the more carbon-intensive models from our sample achieved better performance on common benchmarks compared to the models with less incurred emissions.


Figure 4. Comparison of the accuracy achieved by each model trained on Machine Translation (top left, evaluated using BLEU score on the English-French and English-German WMT datasets), Image Classification (top right, measured using Top-1 accuracy on ImageNet), Question Answering (bottom left, evaluated using F1 score on SQuAD v.1) and Named Entity Recognition (bottom right, evaluated using F1 score on the CoNLL dataset) and the CO2Â emitted for training models. The black curves correspond to the Pareto fronts given the data, that is data points under the line are sub-optimal in terms of performance and CO2 emitted.Note that the x axis is in logarithmic scale. 


FigureÂ 4 shows the performance of the models in these four tasks and the associated carbon emissions; we also represent the theoretical Pareto front given the data, which corresponds to the set of Pareto-efficient solutions based on our data. We can think of the Pareto front of our metrics, the black line in the figures, as the curve connecting the models that achieved the best accuracy for a given amount of CO2eq emissions. In other words, all the data points under the Pareto lines correspond to models that obtained lower accuracy than other models in the sample despite producing the same or more carbon emissions.


Based on the comparison between carbon emissions and performance, we can observe that the only task in which better performance accuracy has systematically yielded more CO2Â is image classification on ImageNet, seen on the top right subplot of FigureÂ 4. Still, the relationship is far from being highly correlated (especially given that that the x-axis in on a logarithmic scale). For example, out of the 35 models analyzed, the top two models in terms of performance are also the most carbon-emitting. However, the third most carbon-intensive model is on the lower end of the performance (achieving Â 76 % accuracy), and we also see low-emitting models on the higher end of performance.


For other tasks, the trend is even less clear â€“ for instance, for the 30 models evaluated on the WMT translation task (top left plot of FigureÂ 4), there is no clear link between CO2Â emissions and BLEU score, for neither English-French or English-German â€“ although the WMT English-French task seems to incur more carbon emissions than the English-German one, which can be explained in part by the fact that the WMT English-French data set is almost 4 times larger than the English-German one, which can require a longer training time and thus a higher energy consumption. For the final two NLP tasks, question answering and named entity recognition, we have less data points (10 for the former and 11 for the latter), and the connection between carbon emissions and accuracy is very unclear. For both tasks, many models from both the high and low ends of the range of CO2Â emissions achieve comparable performance on the SQuAD dataset (bottom-left plot) as well as the CoNLL dataset (bottom-right plot).


Despite the lack of clear correlation between carbon intensity and model performance, there are some interesting observations to be made based on FigureÂ 4.
While we did not expect to see a strong link between these two factors, we find it worth noting that neither consuming more energy nor emitting more carbon seems to necessarily correlate with a higher accuracy, even in tasks such as Machine Translation, where Transformer models are largely seen to do better compared to other modelsÂ 555We find a similar pattern between accuracy and energy consumption, which can be seen in FigureÂ 5 in the Supplementary Materials..






5. Discussion and Future Work

In the current section, we discuss the significance and the context of our analysis, its limitations, as well as promising directions for future work to improve the transparency of carbon emissions reporting in our field.



5.1. Discussion of Results

While the total carbon footprint of the field of ML is unclear due its distributed nature and the lack of systematic reporting of emissions in different settings, in the face of the climate crisis, it is important for the ML community to acquire a better understanding of its environmental footprint and how to reduce itÂ (Ligozat etÂ al., 2021; Patterson etÂ al., 2021). Our study is the first analysis of the carbon emissions of a multitude of ML models from different perspectives ranging from energy source to performance. While our sample is only a small portion of the entire Machine Learning field, the carbon emissions associated to the models in our data set is significant: the total carbon emissions of the models analyzed in our study is about 253 tons ofÂ CO2eq, which is equivalent to about 100 flights from London to San Francisco or from Nairobi to Beijing. While this may not seem like a large amount, the increase in emissions in recent years â€“ from an average of 487 tons of Â CO2eqÂ  for models from 2015-2016 to an average of 2020 tons for models trained in 2020-2022 â€“ as well as other trends that we observed in SectionÂ 4.3, indicate that the overall emissions due to ML model are rising.


In SectionÂ 4, we have discussed that the main sources of variance in the amount of emissions associated to training machine learning models is due to the carbon intensity of the primary energy source and the training time, with the power consumption of the hardware having a smaller influence. In terms of training time, the models in our sample range from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours, pointing again to large variance in our sample. While the maximum of of 400,000 GPU hours (equivalent to about 170 days with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs)Â (Patterson etÂ al., 2021). Obviously, such long training times result in large amounts of carbon emissions, even with lower carbon intensity energy sources. By way of illustration, the model with the longest training time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the lowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq. Also, generally speaking, we can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more layers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters), whereas simpler and shallower models such as convolutional neural networks tend to be on the lower end of the emissions spectrum. Given that Transformer architectures are increasing in popularity â€“ especially in NLP but also for several Computer Vision tasks â€“ having a better idea of their energy consumption, carbon emissions, and the factors that influence them is also crucial part of analyzing the current and future state of our field.


An important observation from our analysis is that better performance is not generally achieved by using more energy. In other words, good performance can be achieved with limited carbon emissions because the progress in recent years has brought the possibility to train machine learning models efficiently. Image Classification is the task in our sample in which we observed the strongest correlation between performance and emissions. However, even in this task we also observed that small increments in carbon emissions lead to large increments in top-1 accuracy (see the left-hand-side of FigureÂ 4). This highlights the availability of efficient approaches and architectures.




5.2. Limitations

The analyses that we have carried out and the insights that they have provided us are useful towards a better understanding of the overall carbon emissions of ML model training. We are also aware of the limitations of our study: for one, we recognize that our sample is not fully representative of the field as a whole, given the diversity of models and architectures that exist and the speed at which our field is evolving. As we discussed in SectionÂ 3, despite our best efforts and several reminders, only 15% of authors from our initial sample of 500 were willing to share relevant information with us. We also recognize that there are several factors that we are missing in order to be more precise in our estimation the carbon footprint of ML models: for instance, we do not have the necessary information regarding the Power Usage Effectiveness (PUE) of the data centers used for model training (i.e. the overhead used for heating, cooling, Internet etc.), as well as the real-time energy consumption of the hardware used for training. We also do not account for carbon offsets and power purchase agreements, which intend to bring computing centers closer to carbon neutrality and which are often taken into account by providers of cloud compute in their carbon accountingÂ (Google, 2022). Despite this, the apples-to-apples carbon analysis that we carried out in the current study provides useful insights about the current state of carbon emissions in our field, as well as how this has evolved over time in the last 9 years.


Furthermore, while this study and much of the related work in this field has focused on estimating the carbon emissions of model training, there are many pieces of other overall carbon footprint of our field which are still missing: for instance, the carbon emissions of tasks such as data processing, data transfer, and data storageÂ (Ligozat etÂ al., 2021), as well as the carbon footprint of manufacturing and maintaining the hardware used for training ML modelsÂ (Gupta etÂ al., 2021), We are also lacking information regarding the carbon impact of model development and inference â€“ given that a model that is trained a single time can be deployed on-demand for millions of queries, this can ultimately add up to more emissions than those produced by the initial model trainingÂ (Luccioni etÂ al., 2022). These are all directions for future research, which we discuss in more detail below.




5.3. Future Work

There is much interesting and exciting work to be done that would help us better understand the carbon emissions and broader environmental implications of ML. This includes:


Additional empirical studies.

There is still a lot of uncertainty around, for instance, the relative contribution of added parameters of ML to their energy consumption and carbon footprint, as well as the proportion of energy used for pre-training versus fine-tuning ML models for different tasks and architectures. Furthering this research can benefit the field both from the perspective of sustainability and overall efficiency.



Widening the scope of ML life-cycle emissions.

The overwhelming majority of work in carbon accounting for ML models has been limited to model training. However, both the upstream emissions (i.e. those incurred by manufacturing and transporting the required computing equipment) as well as the downstream ones (i.e. the emissions of model deployment) warrant further exploration and better understanding.



Increased standardization and transparency in carbon emissions reporting.

As stated in SectionÂ 5.2, we put in significant efforts in contacting authors and gathering data to carry out our study, and were still lacking much of the necessary information that we would have liked to have. While certain conferences such as NeurIPS are starting to include compute information in submissions in submission checklists, there is still a lot of variability in carbon reporting, and figures can vary widely depending on what factors are included. Having a more standardized approach, such as ISO standards, to reporting the carbon emissions of ML can help better understand their evolution.



Considering the trade-off between sustainability and fairness.

The environmental impacts of ML also come with consequences in terms of fairness, given the interplay between fairness and sustainability, most recently discussed byÂ Hessenthaler etÂ al. (2022). This includes, for instance, the consideration of the environmental impacts of ML approaches when benchmarking modelsÂ (Zhou etÂ al., 2020), but also, conversely, considering the impact on robustness and bias of model distillation techniques that improve model efficiencyÂ (Hooker etÂ al., 2020; Xu and Hu, 2022). Generally speaking, given that many advances in ML from last years can be attributed to training increasingly deep and computationally expensive models, especially in fields such as natural language processing, it is important to be cognizant of the broader societal impacts of these models, be it from the perspective of their energy consumptionÂ (Dodge etÂ al., 2022; Bender etÂ al., 2021), the
attribution of computing resourcesÂ (Ahmed and Wahed, 2020; Ahia etÂ al., 2021) or the influence of corporate interests on research directionsÂ (Abdalla and Abdalla, 2021; Birhane etÂ al., 2021).




While discussions regarding the carbon footprint of our daily lives has started to become more common in many communities, alongside increased awareness of how our lifestyle choices (such as the way we travel and the food we eat) contribute to carbon emissions, we are lacking much of the necessary information necessary to regarding the impacts of the models we train. We hope that our work encourages better practices and more transparency in reporting the computational needs of the models and details of the energy used, and that our study will be a meaningful contribution towards a better understanding of our impact as ML researchers and practitioners.
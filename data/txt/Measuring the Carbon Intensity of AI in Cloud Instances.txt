Abstract:

Abstract.
The advent of cloud computing has provided people around the world with unprecedented access to computational power and enabled rapid growth in technologies such as machine learning,
the computational demands of which incur a high energy cost and a commensurate carbon footprint. As a result, recent scholarship has called for better estimates of the greenhouse gas impact of AI: data scientists today do not have easy or reliable access to measurements of this information, which precludes development of actionable tactics. We argue that cloud providers presenting information about software carbon intensity to users is a fundamental stepping stone towards minimizing emissions.
In this paper, we provide a framework for measuring software carbon intensity, and propose to measure operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit. We provide measurements of operational software carbon intensity for a set of modern models covering natural language processing and computer vision applications, and a wide range of model sizes, including pretraining of a 6.1 billion parameter language model.
We then evaluate a suite of approaches for reducing emissions on the Microsoft Azure cloud compute platform: using cloud instances in different geographic regions, using cloud instances at different times of day, and dynamically pausing cloud instances when the marginal carbon intensity is above a certain threshold. We confirm previous results that the geographic region of the data center plays a significant role in the carbon intensity for a given cloud instance, and find that choosing an appropriate region can have the largest operational emissions reduction impact. We also present new results showing that the time of day has meaningful impact on operational software carbon intensity.Finally, we conclude with recommendations for how machine learning practitioners can use software carbon intensity information to reduce environmental impact.




1. Introduction

Climate change is an increasing threat to life on our planet, which disproportionately impacts the most disadvantaged communities and fragile ecosystemsÂ (Masson-Delmotte etÂ al., 2018). One of the main drivers of climate change is carbon dioxide, or CO2, which contributes to the greenhouse effect by trapping the heat from the sun within the atmosphere without letting it dissipate. CO2 (and other types of greenhouse gases, such as methane and ozone) are emitted by many sources, some natural but most man-made, such as the burning of oil and gas for transportation and heating or for industrial processes such as smelting. In 2018, it was estimated that global data center energy use represented close to 1% of global energy usageÂ (Masanet etÂ al., 2020). While it is not yet known what proportion of data center use is for training artificial intelligence (AI) models, it is undeniable that AI and its sub-fields have grown dramatically in recent years, with no sign of slowing downÂ (Schwartz
etÂ al., 2020; Thompson
etÂ al., 2020). While a number of papers have addressed the CO2 emissions produced by AI (e.g., Â (Lacoste etÂ al., 2019; Ligozat etÂ al., 2021; Patterson etÂ al., 2021, 2022)), the extent and provenance of CO2 emissions in the field is still under-explored. Nonetheless, a common theme of previous work is that it aims to estimate the emissions produced by training AI models, or carrying out the accompanying neural architecture search (NAS) process, based on coarse measures such as CO2 emissions of electricity used in the region where the computations were carried out (e.g., (Strubell
etÂ al., 2019)), or post-hoc analyses using information that is not publicly available (e.g., Â (Patterson etÂ al., 2021)).


With an increasing amount of AI model training being done on cloud compute instances, reducing the emissions generated by these workloads will be key to reducing our carbon footprint as a field. However, to reduce greenhouse gas emissions from cloud computing, we need consider the role of two types of actors: the cloud provider (such as Microsoft Azure, Googleâ€™s GCP, or Amazonâ€™s AWS) and the user who reserves and uses cloud resources (e.g., an AI researcher training a model on a cloud instance, or a company hosting a website). Typically, the providerâ€™s motivation is to build a system where users can access the computing power and storage that best meets their needs.
The user, on the other hand, is motivated by some end task which requires computing power, such as running a set of experiments or putting a model into production. Often the user will first consider the minimal computational requirements to achieve their goals, then later ease-of-use features relating to transfer speed or extra storage depending on available budget.
Driven by these motivations, providers and users can each take actions to meet their goals: providers can build data centers and set up APIs to enable usersâ€™ access and accounting, while users can choose their cloud provider, which region to use, and the number and type of cloud instances required for their end task at a given point in time. Based on these stakeholders and motivations, in this work we address the following research questions: 1) how should we measure and report operational carbon costs of AI workloads? And 2) can we shift computation spatially and temporally to mitigate emissions?



In this article, we introduce the first tool to estimate the real-time CO2 emissions impact of instances on a cloud computing platform.
The tool calculates operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit. Using the tool, we explore several case studies on the Microsoft Azure cloud compute platform spanning the areas of natural language processing (NLP) and computer vision, estimating the carbon intensity of training a variety of commonly used machine learning models. We also explore two avenues for users of cloud instances to reduce their CO2 using this tool by: (1) Changing the region of compute and (2) changing the time of day during which the model is run. While the former has been recognized by prior work (Lacoste etÂ al., 2019; Google, 2021b), we are the first to address the latter to the best of our knowledge. Further, our tool makes it possible to automatically schedule jobs in order to reduce their carbon footprint by leveraging these differences in carbon intensity due to time and geographic location. Finally, we provide guidance regarding what should be measured and how, following the Green Software Foundationâ€™s guidelines regarding Software Carbon Intensity (SCI), and suggest future areas of research to improve the state of carbon estimation and reporting in AI.





2. Related work

Attention was first drawn to the environmental impact of AI research by the seminal work of Strubell et al.Â (Strubell
etÂ al., 2019), which quantified the emissions produced by training a Transformer model with Neural Architecture search, finding it to be comparable to the lifetime carbon emissions of five cars. Patterson etÂ al. (2021) presented some updated analyses of similar experiments, including popular architectures like T5Â (Raffel etÂ al., 2019) and BERTÂ (Devlin
etÂ al., 2019), analyzing CO2 emissions as a factor of their energy consumption, carbon intensity of training servers, etc. Other work such as Green AIÂ (Schwartz
etÂ al., 2020) delved further into inequality of access to computational resources within the research community, and advocated for the inclusion of efficiency evaluation alongside accuracy as a primary evaluation criterion. Much existing and ongoing work on quantifying the environmental footprint of ML has been focused on estimating the CO2 emissions of model training. This is a more straightforward endeavor compared to other stages both upstream and downstream from the training process, given that it is well-defined in time and its emissions can be measured in real-time with tools like Code CarbonÂ (Schmidt etÂ al., 2021) and Carbon TrackerÂ (Anthony
etÂ al., 2020) or estimated post-hoc using tools such as ML CO2 Impact TrackerÂ (Lacoste etÂ al., 2019).
Our tool builds upon this work by making carbon tracking on cloud instances possible, enabling a larger portion of ML model training work to profit from fine-grained carbon estimation. However, recent work has found that their results vary significantly and are not fully representative of the true emissions incurred by trainingÂ (Bannour etÂ al., 2021). Perhaps most similar to our work, EnergyVisÂ (Shaikh etÂ al., 2021) is an interactive tool for visualizing and comparing energy consumption of ML models as a function of hardware and physical location (U.S. state), given metadata about a modelâ€™s energy use per epoch. Other studies have gone beyond simply tracking the emissions from training models, aiming to quantify the emissions resulting from manufacturing computing hardwareÂ (Gupta etÂ al., 2021), the broader impacts of sustainable AIÂ (Wu etÂ al., 2021), and the methodologies used to assess those impactsÂ (Ligozat etÂ al., 2021; Kaack etÂ al., 2021). Building upon this research, efforts have also been made to certify systems as being socially- and environmentally-consciousÂ (Gupta
etÂ al., 2020), working towards comparing both the environmental costs and potential benefits of AI models in order to paint a more holistic picture of AI.


Major technology companies have also been increasingly committed to reducing their emissions, largely via the purchase of Renewable Energy Credits (RECs), which involves directly buying quantities of energy produced by renewable sources, translating into carbon reductions under the assumption that the clean energy is displacing an equivalent amount of electricity produced by non-renewable methodsÂ (Gillenwater, 2008). Many cloud providers, from Google Cloud Platform to Microsoft Azure, therefore claim that they are now â€œcarbon-neutral,â€ given that they offset the entirety of the emissions of their cloud centers, though we must be wary of the precise provenance of RECs, and the details of how each organization defines â€œzeroâ€ net emissions (Rogelj
etÂ al., 2021). This is complemented by efforts to mitigate the actual CO2 emissions of the compute regions themselves, with several server locations partially powered by renewable energy sources such as solar and windÂ (Microsoft Azure, 2021; Services, 2021; Google, 2021a) and giving users the necessary tools to pick compute regions with a smaller carbon footprintÂ (Hertogh, 2021; Google, 2021b), which are often tied to the amount of low-carbon energy that is being purchased, and not the grid emissions intensity. It is important to note that the decision on when and where to deploy a workload should be based on a grid emissions signal, not the amount of emissions offset through market-based measures (e.g., green power purchase agreements (PPAs), renewable energy certificates (RECs), or other carbon offset mechanisms): purchasing clean energy is not the same as consuming clean energy.





3. Reporting AI Carbon Intensity

Carbon accounting and reporting is becoming increasingly common in ML, with conferences such as NeurIPS requesting that submissions report their emissionsÂ (NeurIPS 2021
Conference, 2021) and recent work reporting the emissions incurredÂ (Thoppilan
etÂ al., 2022; Sanh
etÂ al., 2021). However, it has yet to become the norm in our field, and we are still lacking systematic information regarding the environmental footprint of training ML models and how we can reduce it. In this paper, we argue that if members of the ML community had access to information about the CO2 emissions of their actions, they could adapt their decisions to reduce these emissions while still meeting the computational needs for their end tasks.
In addition, providers building tools that enable users to track their CO2 emissions directly aligns with providersâ€™ goals, as it will inform usersâ€™ decisions without being overly burdensome. Any cloud provider that discloses this information to users will, in fact, be improving those customersâ€™ experiences, and likely increase usage of the platform. More specifically, we propose that, for a cloud user who wants to estimate their carbon footprint, the most salient information providers can report is the CO2 emissions generated by their cloud instances.
Arguably the single most important contribution of this paper is the simplest: a presentation of the software carbon intensity (SCI) as a proxy for carbon emissions for a given cloud instance as it is running.



3.1. Methodology: Computing CO2 Intensity

In this section we describe a method for estimating carbon intensity for cloud instances.
At a high level, this involves tracking electricity consumption of hardware related to a single cloud instance, and mapping that electricity usage to CO2 emissions by using a grid-based carbon intensity.


As developed by the Green Software Foundation, the Software Carbon Intensity (Sâ€‹Câ€‹Iğ‘†ğ¶ğ¼SCI) is a rate, carbon emissions per one functional unit, or R. The equation used to calculate the Sâ€‹Câ€‹Iğ‘†ğ¶ğ¼SCI value of a software system is therefore:


(1)

Sâ€‹Câ€‹I=((Eâˆ—I)+M)â€‹perâ€‹Rğ‘†ğ¶ğ¼ğ¸ğ¼ğ‘€perğ‘…\displaystyle SCI=((E*I)+M)\mathrm{\ per\ }R



where:


â€¢

E=ğ¸absentE= Energy consumed by a software system. Specifically, we focus on energy consumption of Graphical Processing Units, or GPUs.
The units used are kilowatt-hours (kWh).




â€¢

I=ğ¼absentI= Location-based marginal carbon emissions for the grid that powers the datacenter. WattTime provides measurements of grams of carbon dioxide equivalent per kilowatt-hour of electricity (gCO2eq/kWh)



â€¢

M=ğ‘€absentM= Embodied carbon (also referred to as â€œembedded carbonâ€) is the amount of carbon emitted during the creation, usage, and disposal of a hardware device. When software runs on a device, a fraction of the total embodied emissions of the device is allocated to the software.



â€¢

R=ğ‘…absentR= Functional unit. In this instance, we are defining the functional unit as one machine learning training job, but it is extensible to other scenarios.



The equation can be further refined to:


(2)

Sâ€‹Câ€‹I=(O+M)â€‹perâ€‹Rğ‘†ğ¶ğ¼ğ‘‚ğ‘€perğ‘…\displaystyle SCI=(O+M)\mathrm{\ per\ }R



where O=Eâˆ—Iğ‘‚ğ¸ğ¼O=E*I calculates the operational emissions based on energy consumption (Eğ¸E) multiplied by the location-based and time-specific carbon intensity measurement (Iğ¼I). Once more this can be further refined to simply:


(3)

Sâ€‹Câ€‹I=Câ€‹perâ€‹Rğ‘†ğ¶ğ¼ğ¶perğ‘…\displaystyle SCI=C\mathrm{\ per\ }R



where C=O+Mğ¶ğ‘‚ğ‘€C=O+M is the software carbon intensity for a given cloud instance. In this paper, we focus on measuring operational emissions Oğ‘‚O, and leave measurement and accounting for embodied emissions due to specialized ML hardware such as GPUs to future work (see Â§8).


The objective of the Green Software Foundationâ€™s Software Carbon Intensity (SCI) specification is to calculate and reduce a SCI score, based on carbon emissions reductions, rather than the currently-used market-based neutralization. Specifically, the SCI uses a â€consequentialâ€ carbon accounting approach, which aims to quantify the marginal change in emissions caused by decisions or interventions. This differs from the commonly used â€attributionalâ€ carbon accounting approach, which uses average carbon intensity data, meaning it does not provide the most actionable information to help reduce carbon emissions. Due to the myriad potential pitfalls of relying on market-based measures in place of actual reduction in emissions (Rogelj
etÂ al., 2021), it is not possible to reduce the SCI through carbon neutralization or carbon offsets. We assert that cloud providers should provide the SCI to developers and data scientists to help them make choices that reduce the carbon footprint of their ML workloads.




3.2. The Scope of our Tool: GPU Computation of a Single Cloud Instance

Data centers typically comprise many computer systems and hardware components, including storage, GPUs, CPUs, and networking components.
We can break down the electricity usage for data centers into: 1) electricity that is used for a single cloud instance, and 2) electricity that is used for the benefit of the whole data center. In this work we focus on the former, a single cloud instance; because of this, a reader should understand that our estimates of the electricity consumption and emissions are underestimates.111There is related work on estimating and reducing the electricity of data centers in general, e.g., (Gao, 2014; Lazic etÂ al., 2018).


Electricity Consumption from a Single Cloud Instance

The most accurate and popular AI models today are typically (deep) neural networks, which are most performant on specialized, highly parallelized, and often energy-intensive hardwareÂ (Thompson
etÂ al., 2020).
The most common scenario is for AI workloads to run on graphics processing units (GPUs), which provide significant acceleration compared to CPUs (central processing units) but are more power-hungry (often consuming 250W-350W, compared to CPU consumption of 10-150W). Due to specialization to the matrix multiply operations at the core of neural network computations and a high rate of parallelization, GPUs can perform many more of these types of computations in the same amount of time as a CPU, but this increased computation throughput comes at an increased energy cost.
Thus in ML applications based on deep learning, the majority of the electricity consumption is due to the GPUÂ (Buildcomputers.net, 2021; Torbet, 2019). While this result is fairly uncontroversial, we ran an experiment to confirm it. To do so, we trained a BERT-base modelÂ (Devlin
etÂ al., 2019) on a single NVIDIA TITAN X GPU (12 GB) in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs) to measure the relative electricity consumption of different components. We trained the model using the original code provided byÂ Devlin
etÂ al. (2019) on the language model pre-training task for 12 hours on one GPU, sampling the instantaneous energy use of the GPU, CPU and DRAM for each socket throughout that period, then averaging to get average power draw per component in watts. GPU energy draw was measured using nvidia-smi and CPU and DRAM power draw were obtained using Intelâ€™s RAPL. Our measurements, in watts, are presented in TableÂ 1. As expected the GPU accounts for almost 3/4 of electricity consumption.





Hardwa.
GPU
CPU0

CPU1

DRAM0

DRAM1

Total




Watts
187.1
22.9
9.3
23.0
9.3
251.6


Fraction
74%
9%
4%
9%
4%
100%



Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.



Focus on GPUs

In cloud datacenters, the CPUs, RAM, storage, and motherboards are often shared across multiple instances; while this provides the flexibility that makes the cloud so useful, it leads to technical limitations that make it difficult (and in some cases impossible) to properly estimate electricity consumption from these sources for a single instance.
However, GPUs are typically not shared across instances, and in fact for large AI workloads itâ€™s often the case that multiple GPUs are attached to a single instance, leading to an even greater proportion of the total energy consumption being used by the GPUs.
Thus, it is relatively easy to measure the GPU electricity consumption for a single instance, while it is not for other components.
For this reason, and because they typically consume the majority of electricity in AI workloads, in this work we only measure GPU electricity consumption.
We recognize this is a first step towards a more complete measurement, and provide further discussion in the next section.222We note that our conclusions drawn from experiments and analyses on time-shifting and location-shifting are still applicable with tools that measure more electricity than just the GPU.



Other sources of CO2 

Data centers have a number of electricity uses that are important, but will not be covered by our tool.
According to the U.S. Department of Energy: â€œThe electricity consumed in these data centers is mainly by the equipment (50%) and HVAC (25%â€“40%)â€ (US Department of
Energy, 2021). Such other sources of emissions can be accounted for using methods such as Power Usage Effectiveness (PUE), which can be used to describe the proportion of electricity consumption by the computing equipment vs. other sources. For a given datacenter, this can be turned into a factor which can be multiplied against the electricity consumption of computing equipment to get an estimate of the total consumption.
Some companies have highlighted particularly low PUEs, such as Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021,333https://www.google.com/about/datacenters/efficiency/ compared to an average global PUE of 1.59Â (Ascierto and
Lawrence, 2020).


Other factors, such as the emissions produced by maintenance workers driving to and from the data center, emissions from manufacturing the computer systems, and emissions from building the structure in which the data center is housed444One of the largest single source of CO2 emissions, contributing to 7%-8% of global emissions, is the production of cementÂ (International Energy Authority (IEA), 2020). are non-negligible but beyond the scope of this paper.
Finally, for workloads that do not use GPUs (e.g., storage or web hosting) we recommend users choose low emissions regions and times of day, as they will not have access to single-instance emissions calculations.
We leave it open for future research to address how to appropriately allocate CO2 emissions from such data center-wide processes to individual reserved cloud instances.







4. Electricity consumption for AI Workloads




Model
BERT
BERT
6B
Dense
Dense
Dense
ViT
ViT
ViT
ViT
ViT



finetune
pretrain
Transf.
121
169
201
Tiny
Small
Base
Large
Huge


GPU
4â‹…â‹…\cdotV100
8â‹…â‹…\cdotV100
256â‹…â‹…\cdotA100
1â‹…â‹…\cdotP40
1â‹…â‹…\cdotP40
1â‹…â‹…\cdotP40
1â‹…â‹…\cdot V100
1â‹…â‹…\cdotV100
1â‹…â‹…\cdotV100
4â‹…â‹…\cdotV100
4â‹…â‹…\cdotV100


Hours
6
36
192
0.3
0.3
0.4
19
19
21
90
216


kWh
3.1
37.3
13,812.4
0.02
0.03
0.04
1.7
2.2
4.7
93.3
237.6



Table 2. For the 11 models in our analysis: the type of GPU, the number of GPUs of that type, the number of hours, and the energy used in kWh. For example, our BERT language modeling (BERT LM) experiment used 8 V100 GPUs for 36 hours and used a total of 37.3 kWh. We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.


As outlined in Â§3.1, calculating software carbon intensity begins with recording the electricity consumption, which can then be mapped to emissions based on the emissions of the grid being used. In this section, we present data on electricity consumption for experiments training 11 different models, covering natural language processing (NLP) and computer vision applications, ranging from less than an hour on a single GPU up to more than 8 days on 256 GPUs. We outline both the experiments themselves and their electricity consumption, and in the following section we use the electricity consumption and carbon intensity tool described in the previous section to calculate their software carbon intensity.



4.1. NLP

BERT Training

We monitored the energy consumption while training a BERT-small modelÂ (Devlin
etÂ al., 2019) for approximately 36 hours on 8 NVIDIA V100 GPUs. That training run consumed over 37 kWh of electricity.



BERT Finetuning

We tracked the energy consumption while finetuning the BERT-small model on a standard natural language inference taskÂ (Williams
etÂ al., 2017, MNLI) for approximately 6 hours on 4 NVIDIA V100 GPUs. Our finetuning run consumed around 3.2 kWh of electricity, i.e., less than one tenth that due to BERT-small pre-training.



6 Billion Parameter Transformer

We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8)âˆ—13.8=103.560813.8103.5(60/8)*13.8=103.5 MWh, or 103,500 kWh â€” almost 2800 times more than training the BERT-small model!





4.2. Computer Vision

DenseNets

We trained three sizes of DenseNetsÂ (Iandola etÂ al., 2014) on MNISTÂ (LeCun
etÂ al., 1998). The jobs lasted between 20 and 25 minutes and consumed between 20 and 38Wh (or 0.02 to 0.04 kWh) of electricity, which is negligible compared to the other models.



Vision Transformers

We evaluated the energy consumption during the training of five sizes of Vision TransformersÂ (Dosovitskiy etÂ al., 2020) on ImageNetÂ (Deng
etÂ al., 2009). For the smallest ViT experiment (ViT tiny), training lasted around 19 hours on a single V100 and consumed approximately 1.7 kWh. For the largest one (ViT huge), training lasted more than 9 days on a 4 V100s and consumed approximately 237 kWh. The full list of models can be found in Table 2.







5. Emissions by Region and Time of day

Using the methodology presented above, we provide some of the first measurements of the differences of actual datacenters from a major cloud provider. Importantly, what we have is a time series of marginal emissions: for example, if a job were to run from 1 pm to 5 pm in the US West region with a cloud instance that has four fully-utilized GPUs, both the energy consumed and the marginal carbon intensity during that time is what we want to record. This time-series data can estimate the cumulative emissions for that experiment at the end.


Figure 1. Carbon emissions that would be emitted from training BERT (language modeling on 8 V100s for Â 36 hours) in 16 different regions (one region per line) at different times throughout the year. Each line is relatively flat, indicating the emissions in a single region during different months are relatively similar. There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful (Â 7k grams vs.Â 26k grams, for the most efficient vs.Â least efficient regions).



5.1. Region

How much does the choice of datacenter region impact the emissions? And for a single region, how much variation occurs throughout the year? We address these questions in FigureÂ 1, which shows carbon emissions that would be emitted from training BERT (see Â§4 for more details) on 8 V100 GPUs for 36 hours in 16 different regions (one region per line) at different times throughout the year.


Figure 2. Emissions for our 11 experiments described in Â§4. For each model we show a vertical blue bar, where the top of the bar is the max, the bottom is the min, and the black line represents the average emissions (across regions and time of year). First and fourth quartiles are represented by the light blue at the top and bottom of each vertical blue bar. The largest training runs (e.g., 6 billion parameter LM) releases a significant amount of emissions, no matter the region (and recall the 6 billion parameter LM is only trained for Â 13% of a full run, so a full run would emit about an order of magnitude more emissions than reported here). The smallest experiments emit very little. Presented on a log scale, with references on the right indicating equivalent sources of emissions per the United States Environmental Protection Agency (2021).


What do emissions look like across the 11 experiments described in Â§4? In FigureÂ 2 we show results for all 11 experiments, which cover two BERT experiments (finetuning and language modeling), partial training of a 6.1 billion parameter Transformer, 3 sizes of DenseNets, and five sizes of Vision Transformers. Each experiment is represented by a vertical blue bar showing the range of emissions that would be emitted for that experiment across different regions. The top of the blue bar is the emissions from running that experiment in the region with the most emissions, the bottom is the emissions from running that experiment in the region with the least emissions, the black line represents the average, and the light blue regions are the top and bottom quartiles.


In FigureÂ 2 we also include estimates of equivalent sources of emissions per the United States Environmental Protection Agency (2021).
One phone charge is estimated to emit 8.22Ã—10âˆ’68.22superscript1068.22\times 10^{-6} metric tons (using US national weighted average CO2 marginal emission rate for delivered electricity), one mile driven is estimated to emit 3.98Ã—10âˆ’43.98superscript1043.98\times 10^{-4} metric tons (using average US passenger vehicle, which gets 22.5 miles per gallon of gasoline), one gallon of gasoline consumed is estimated to emit 8.887Ã—10âˆ’38.887superscript1038.887\times 10^{-3} metric tons, one barrel of crude oil consumed is estimated to emit 0.430.430.43 metric tons, one average US home energy use is estimated to emit 8.308.308.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil), and one rail car of coal is estimated to emit 181.29181.29181.29 metric tons.


The largest experiment in our set is the 6 billion parameter transformer, and that model is only partially trained (as described in Â§4, it is only trained for about 13% of the time needed to converge). Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year). Perhaps unsurprisingly, even the most efficient region of those we examined for that experiment still leads to more emissions than a full barrel of oil. If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).


Comparing against previous work on measuring emissions can be challenging without full information about data and model parallelism, GPU utilization, the number of weight updates, and other relevant factors; while we donâ€™t have experiments covering the same models as previous work on estimating CO2, we can make approximate comparisons along three dimensions: a) kWh per GPU hour, b) CO2 grams per GPU hour, and c) CO2 grams per kWh.
Here we compare against (Patterson etÂ al., 2021) and (Patterson etÂ al., 2022) which report information about training especially large models.
Their estimates also include additional sources of CO2, like PUE (Power Usage Effectiveness) of their datacenters, so we expect their kWh per GPU hour and CO2 per GPU hour to be higher than our estimates (which only count the GPU electricity consumption).


Across our experiments, we find kWh per GPU hour to range from 0.07 to 0.28, compared to Patterson etÂ al. (2021) with 0.22 to 0.47, and Patterson etÂ al. (2022) with 0.36.
We find CO2 (grams) per GPU hour in the most efficient region to average 34, and in the least efficient region to average 128, where Patterson etÂ al. (2021) found a range of 63 to 202, and Patterson etÂ al. (2022) found 32.
We find CO2 (grams) per kWh in the most efficient region to average 200, and in the least efficient region to average 755. The estimates from Patterson etÂ al. (2021) range between 427 and 545 (except GShard 600B with 200), and Patterson etÂ al. (2022) found 88. In short, we find most of their estimates to be within the range of ours, with the exception of Patterson etÂ al. (2022), which specifically aimed to choose a region that was more CO2 efficient.







Hour
0:00
03:00
06:00
09:00
12:00
15:00
18:00
21:00


BERT
Central
Day 1
2,381
2,341
2,210
2,252
2,354
2,391
2,410
2,403


finetune
US
Day 2
2,330
2,249
2,204
2,299
2,320
2,317
2,339
2,344




Day 3
2,430
2,339
2,257
2,313
2,393
2,374
2,317
2,331



Table 3. How do emissions vary throughout different times of day? We present the emissions produced by the BERT finetuning experiment described in Â§4 had it run at different times in the Central US region, on three separate days.




5.2. Time of Day

While the choice of region is a major source of variation in CO2 emissions, diurnal variations also play a significant role. During the day, a region may have a higher mix of renewable energy or fossil-fuel based sourceÂ (deÂ Chalendar and
Benson, 2019). As one can see in TableÂ 3, depending on the day, starting the BERT finetuning at, e.g., midnight instead of 6:00 can result in carbon emissions increasing by up to 8%. The amount of variation varies by region and time of year aswell.






6. Optimizing Cloud Workloads




(a) Flexible Start optimization for Dense 201.




(b) Flexible Start optimization for 6B parameters Transformer.



Figure 3. What proportion of emissions can we expect to save if we change the start time by up to 24 hours? For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US; for very long runs like training a 6 billion parameter language model for 8 days (b), changing the start time by up to 24 hours leads to less than 1.5% reduction at best in any region. Note: we confirmed with WattTime that emissions estimates for West US were correct, that region has large variance.


We use the tools presented so far to evaluate two algorithms for reducing emissions of AI workloads on the Microsoft Azure cloud compute platform using temporal shifting. We consider sixteen regions where workloads can be scheduled on Azure: nine in North America, six in Europe and one in Australia (see FigureÂ 3). For each region, we obtained from WattTime the historical marginal carbon emissions for the year 2020 at a 5-minute granularity. We also measured the electricity consumption per 5-minute intervals of the various models introduced in Â§4. The two optimization methods we studied are:




â€¢

Flexible Start. Start the workload at the time, in the next Nğ‘N hours, that minimizes its carbon emissions. Once the workload is launched, it is run until completion. Implementation: Consider all possible start times (in 5 minute increments) in the desired window. For each start time, compute the jobâ€™s corresponding emissions and pick the lowest.



â€¢

Pause and Resume. Assuming the workload can be stopped and restarted (a fairly weak constraint), run its computations over the next (N+job duration)ğ‘job duration(N+\mbox{job duration}) hours while minimizing its total carbon emissions. This involves pausing and resuming the job, possibly multiple times, to avoid consuming energy when carbon intensity is high. Implementation: Find the 5 minute intervals with the lowest marginal emissions during the (N+job duration)ğ‘job duration(N+\mbox{job duration}) hour window, and select enough intervals to add up to the job duration. Then simulate running the job only during those intervals and compute the corresponding emissions. We explored two sets of values for Nğ‘N: one absolute, corresponding to increasing the total duration of the job by at most {6, 12, 18, 24} hours; and a second one relative, where we allow the job to increase in duration by at most {25%, 50%, 75%, 100%}. In other words, for the second set, we allow the workload to last for at most twice its duration had it not been stopped. While arbitrary, we motivate the choice of those two sets by the extreme range of possible job duration (from minutes to weeks). Note that we assume pausing and restarting the job is immediate and does not consume additional energy: this is similar in spirit (for carbon emissions) to Spot Instances on existing cloud platforms which automatically pause an instance if its price rises above a threshold set by the user.





We find the region that the algorithms are evaluated in has a significant impact. For example, the region we labeled West US varies frequently throughout a single day between periods of high emissions and very low emissions, and thus Pause and Resume can lead to significant reductions. However, other regions do not present as much variance, and thus lead to less reduction in emissions. See FiguresÂ 3 and 4. The lack of geographic diversity in the region list is an unfortunate consequence of the unavailability of carbon intensity data from other continents; we hope such data becomes broadly available in the near future.



6.1. Evaluation of Emissions Reduction Algorithms

We evaluate how the two optimization algorithms would impact the emissions from the 11 experiments described in Â§4. In order to account for daily variations (weather, electricity demand, etc.), we report the average emissions decrease computed over 5 different start times in each month, giving a total of 60 data points.



6.1.1. Emissions Reduction by Region

Flexible Start

When evaluating the Flexible Start algorithm for a fixed duration between 6 hours and 24 hours, we find significant emissions reductions for shorter jobs (e.g., the DenseNet experiments), with minimal savings for jobs that are longer than a day; this aligns with our expectations, as short jobs can be run when emissions are lowest throughout a day, but long jobs naturally average across multiple days. See FigureÂ 3, with results for all experiments in the appendix. This analysis is designed to highlight a use case where an AI workload needs to run regularly, but the practitioner has some flexibility on when it runs (so it could, e.g., run over night, if that is when carbon intensity is lowest). This is in fact a common use case in production ML systems deployed at companies, where models are re-trained on a regular schedule to incorporate new data over time (Hazelwood
etÂ al., 2018).



Pause and Resume

When evaluating the Pause and Resume algorithm for durations up to 100% of the duration of the original experiment, we find the opposite of the Flexible Start result: short experiments like DenseNet 201 only see emissions reductions smaller than 10%, while the 6 billion transformer training run (our experiment with the largest carbon intensity) actually sees the largest decrease in emissions. See FigureÂ 4 for two examples, with results for all 11 experiments in the appendix. This analysis is designed to highlight a use case where an AI workload can be increased in duration by some proportion of the original run time.





(a) Pause and Resume optimization for Dense 201.




(b) Pause and Resume optimization for 6B parameters Transformer.



Figure 4. What proportion of emissions can we expect to save if we pause an AI workload when emissions in a region are high and resume when emissions are low, increasing the total duration by up to double the original duration? For short experiments, the doubled duration is still relatively short, and thus leads to minimal emissions reduction (see DenseNet 201 in (a)); for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%. We confirmed with WattTime that emissions estimates for West US were correct, as that region has large variance.





6.1.2. Comparable Duration Increases

In the previous section we examined the amount of emissions reduction for our two algorithms by region, and compared Pause and Resume increasing duration by a proportion of the original experiment and Flexible Start by a fixed duration.
Here we evaluate the two algorithms when they increase the duration of an AI workload by the same amount (each result is averaged across all regions and times of year). One can think of the Flexible Start algorithm as a version of Pause and Resume where there is only one start time, and no pausing allowed; thus we should expect the Flexible Start results to always lower bound the Pause and Resume ones.


We show results for both algorithms and two situations: increasing the duration of the run by 24 hours in TableÂ 4, and by 100% in TableÂ 5. In these tables we also include information about the average number of pauses per hour for the Pause and Resume algorithm. Perhaps surprisingly, we find the average number of pauses is quite low. This can be interpreted as the number of times the carbon intensity crosses above the threshold minimizing total emissions being small.





Model
BERT
BERT
6B
Dense
Dense
Dense
ViT
ViT
ViT
ViT
ViT



finetune
LM
Transf.
121
169
201
Tiny
Small
Base
Large
Huge


FS
14.5%
3.4%
0.5%
26.8%
26.4%
25.9%
5.6%
5.3%
4.2%
1.3%
0.5%


P&R
19.0%
8.5%
2.5%
27.7%
27.3%
27.1%
12.5%
12.3%
11.7%
4.7%
2.4%


Pauses / hr
0.23
0.3
0.15
0.06
0.07
0.08
0.3
0.3
0.3
0.23
0.14



Table 4. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 24h increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.


Note that the above optimizations were performed using historical data, meaning that their results are the best achievable, assuming access to an oracle predicting carbon intensity perfectly. WattTime currently provides marginal emission rate estimates and forecasts for up to 24 hours, so for short workloads, our findings will reflect gains observed in practice using the forecasts. For longer workloads, our numbers give an upper bound on the realizable gains.
For example, the Pause and Resume algorithm pauses the workload when emissions are above a threshold, and resumes when emissions are below that threshold. In our evaluation here we set this threshold such that the total run time is increased by, e.g., 24 hours; a machine learning practitioner would have to estimate how much a particular threshold would increase job duration, but would not know exactly.
The dynamic nature of the Pause and Resume optimizations suggests that well-designed scheduling algorithms should be able to get rather close to the upper-bound. We leave such algorithms to future work and hope our tools can inspire further research into that type of scheduling. Moreover, it is likely that carbon intensity forecasting will improve over the years and eventually extend beyond 24 hours, allowing time-shifting decisions to become increasingly accurate.





Model
BERT
BERT
6B
Dense
Dense
Dense
ViT
ViT
ViT
ViT
ViT



finetune
LM
Transf.
121
169
201
Tiny
Small
Base
Large
Huge


FS
7.0%
4.1%
2.6%
1.8%
2.5%
2.7%
5.0%
4.8%
3.9%
3.3%
3.0%


P&R
9.5%
11.0%
11.4%
2.0%
2.8%
3.1%
11.0%
11.0%
10.8%
11.4%
11.3%


Pauses / hr
0.42
0.29
0.27
1.5
1.88
2.0
0.31
0.32
0.31
0.27
0.26



Table 5. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 100% increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.







7. Considerations for Model Development and Deployment 

Generally speaking, we advocate that researchers and practitioners record and report the amount of emissions incurred by ML projects, starting with the initial exploratory training phases all the way through hyperparameter tuning and deployment for the final model. This can inform an Operational Lifecycle Analysis (OLCA) for a machine learning model, which would account for all phases of the ML lifecycle. In the subsections below, we outline some ways in which the proposed tool can be used at different stages of the model development and deployment process, and describe some environmental impacts due to ML modeling that are outside the scope of measurement of this tool.


We see various ways in which our tool can help guide model training, for instance via carbon-informed optimization (similarly to what Â (Kim and Wu, 2021) proposed for energy efficiency in federated learning), or for cloud-based recommendations that enable users to opt-in for carbon-aware configurations (in terms of region, time, etc.) to reduce the carbon intensity of their training workloads. We believe that tracking and reducing greenhouse gas emissions can be a very important feature for users deciding on how they will set up their cloud usage, but we recognize that there are natural trade-offs that must be considered. We therefore recommend that the measurements provided by our tool be used to guide informed decisions alongside other considerations as part of a holistic approach, and not as a single gold standard to optimize against. For example, even just within the scope of ML model development, it often takes engineering time to optimize a workload to be more efficient (i.e., use less computational resources), and a user should consider whether that time would be better spent elsewhere (e.g., transferring the workload to another region with lower average emissions). Furthermore, some projects have strict time constraints, and so scheduling jobs to only run at night would significantly delay progress, potentially leading to more emissions in other parts of the project. Thus, our suggestions are not meant as a one-size-fits-all solution which will eliminate carbon emissions, but instead as a set of options which can be referenced by users and decided upon on a case-by-case basis. Finally, there are also many additional upstream and downstream emissions considerations due to the ML model lifecycle, due to, e.g., hardware manufacturing and downstream uses or misuses of the model, that could eclipse the direct emissions due to model training alone. See Â§2 for further discussion of this crucial point.


Another important consideration is operating cost; it could be the case that Region A is lower emissions but higher cost than Region B for a particular workload, and thus a user could run their workload in Region B and have some budget left over that could be used for other reductions in emissions. A final consideration is cost of data transfer; it could be the case that Region A is lower emissions and monetary cost than Region B for a particular workload, but the energetic, environmental, or monetary cost of moving the data could exceed the benefits gained.


If we see broad adoption of such reporting tools, we may see increases in cloud use in regions which have low emissions.
In such a scenario, providers could be incentivized to build new data centers, and providers should consider the local impact of such construction.





8. Future Directions 

As mentioned in Â§7, single-instance emissions are a well-defined starting place for quantifying, mitigating, and reducing the environmental impact due to ML, but do not present a complete picture of the total emissions that should be accounted for when considering the overall carbon emissions of the ML life cycle. Here are some aspects that are yet to be accounted for (and in some cases, yet to be defined) in terms of the overall OLCA of machine learning:


Scopes of emissions

The Greenhouse Gas Protocol (GHGP) is a standard created by the World Resources Institute and the Business Council for Sustainable Development, and has seen broad adoption internationally. It defines Scope 1, Scope 2, and Scope 3 emissions as follows: Scope 1 emissions are those generated by direct actions of a company, such as running motor vehicles; Scope 2 emissions are those associated with purchase of electricity, steam, heating, or cooling; and Scope 3 emissions are those that the company indirectly participates in, such as those due to investments of the company and downstream use of products. In the present work, we have focused on the Scope 2 emissions incurred due to electricity usage by cloud providers. The current GHGP Scope 2 is an attributional guidance that precludes the use of marginal emissions rates, and primarily focuses on broad generation-based average rates. It is important to note that the GHGP Scope 2 guidance is incompatible with the proposed method; this paper illustrates the need to revisit the Scope 2 guidance to better align with consequential accounting methods.


We do not cover the Scope 1 emissions (e.g. emissions that directly result from business activities, such as stationary combustion of fuels for backup power generation in cloud datacenters), for a more detailed discussion see e.g. Gupta etÂ al. (2021), nor the Scope 3 emissions (e.g. emissions that indirectly result from all other business activities, such as those associated with the upstream raw materials extraction, manufacturing, and delivery of cloud-based IT asset infrastructure such as servers from suppliers to be used in a cloud providerâ€™s datacenters). Both of these types of emissions warrant discussion and debate by the AI communityâ€” and indeed some work has begun on the subject, e.g.,Â (Kaack etÂ al., 2021; Ligozat etÂ al., 2021)â€”but we are missing a more concrete structure for categorizing, quantifying and mitigating the different scopes of emissions in our field. This would involve the active participation of specific stakeholders to establish the tooling and reporting required to better estimate these aspects, which is a challenge in itself.



Developing certification systems for â€œGreen AIâ€

While initiatives like the Green Software Foundation are making important progress towards measuring and mitigating the carbon footprint of software in general, the decentralized and data-driven nature of ML will call for specific approaches and guidelines to ensure its efficiency. We anticipate that AI-specific initiatives, spanning both research and academia, will help establish certification systems (or badge systems) that will allow both model developers and users make more informed choices with regards to sustainability. The current framing of Scopes 1, 2, and 3 may not encompass all the emissions reasonably associated with an AI program.



Improving the carbon transparency of research and practice

Despite the existence of tools such as Code CarbonÂ (Schmidt etÂ al., 2021) and EvergyVisÂ (Shaikh etÂ al., 2021), both carbon estimation and reporting in ML publications and technical reports remain a relatively rare phenomenon. Conferences such as NeurIPS and NAACL have recently added emissions reporting as an optional part of the submission process; however, more encouragement will be necessary for this to become commonplace. Gathering more data about the environmental impact of our field is a crucial step towards identifying room for improvement and, eventually, reducing our emissions.



Supporting improved estimates of emissions rates

The estimates of emissions rates providers would benefit from more and better data being provided by electric system operators. This is particularly true in areas of the world where it is currently not possible to produce hourly marginal estimates.



Reducing AIâ€™s scope-enabled emissions

Responsible development and application of AI must account not only for the hidden costs of development, as discussed in this paper, but for the positive or negative carbon impact the application enables. AI models continue to be used for oil explorationÂ (Nordloh etÂ al., 2020), deforestationÂ (Mosin etÂ al., 2019), and miningÂ (Hyder
etÂ al., 2019), among other environmentally-detrimental practices. When considering the net impacts of an AI application, it is imperative to determine the extent to which AI is incentivizing practices that have a negative impact on the environment, or the extent to which applications are directly reducing emissions or otherwise incentivizing practices that are beneficial to the climate, and take these downstream direct and indirect effects into account in the overall environmental impact assessment of our fieldÂ (Birhane etÂ al., 2021; Kaack etÂ al., 2021).


Acknowledgements.
We thank Avi Allison (Microsoft) for insights associated with carbon accounting and Location-based Marginal Emissions (LME) data,
Henry Richardson (WattTime) for insights on LME data and the Software Carbon Intensity (SCI) specification, Abhishek Gupta for his work on the SCI specification, and
Ananya Ganesh (CU Boulder) for help in obtaining the measurements included in TableÂ 1. We also thank Alessandro Sordoni, Payal Bajaj, and Vibhav Vineet for sharing their training and inference jobs, and Jon Borchardt for help with plotting.
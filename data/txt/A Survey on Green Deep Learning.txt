A Survey on Green Deep Learning
Jingjing Xu∗
ByteDance AI Lab
xujingjing.melody@bytedance.com
jingjingxu@pku.edu.cn
Wangchunshu Zhou∗
ByteDance AI Lab
zhouwangchunshu.7@bytedance.com
Zhiyi Fu∗†
Peking University
ypfzy@pku.edu.cn
Hao Zhou
ByteDance AI Lab
zhouhao.nlp@bytedance.com
Lei Li
University of California, Santa Barbara
lilei@ucsb.edu
Abstract
In recent years, larger and deeper models are springing up and continuously push-
ing state-of-the-art (SOTA) results across various ﬁelds like natural language pro-
cessing (NLP) and computer vision (CV). However, despite promising results, it
needs to be noted that the computations required by SOTA models have been in-
creased at an exponential rate. Massive computations not only have a surprisingly
large carbon footprint but also have negative effects on research inclusiveness and
deployment on real-world applications.
Green deep learning is an increasingly hot research ﬁeld that appeals to researchers
to pay attention to energy usage and carbon emission during model training and
inference. The target is to yield novel results with lightweight and efﬁcient tech-
nologies. Many technologies can be used to achieve this goal, like model com-
pression and knowledge distillation. This paper focuses on presenting a system-
atic review of the development of Green deep learning technologies. We classify
these approaches into four categories: (1) compact networks, (2) energy-efﬁcient
training strategies, (3) energy-efﬁcient inference approaches, and (4) efﬁcient data
usage. For each category, we discuss the progress that has been achieved and the
unresolved challenges.
∗Equal Contribution
†This work is done during internship at ByteDance AI Lab.
arXiv:2111.05193v2  [cs.LG]  10 Nov 2021
Contents
1
Introduction
4
1.1
Deep Learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
Green Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2.1
Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2.2
Measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.3
Broader Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3
Outline of the Survey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2
Compact Architecture
9
2.1
Component Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.1.1
Compact Convolution
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.1.2
Efﬁcient Attention
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.1.3
Lightweight Softmax . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.1.4
Compact Embeddings
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.2
Component Assembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.2.1
Memory Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.2.2
Static Weight Sharing
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.2.3
Dynamic Weight Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.2.4
Deployment Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.3
Compact-architecture Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
3
Energy-Efﬁcient Training
18
3.1
Initialization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
3.1.1
Random Initialization
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
3.1.2
Pre-trained Models for Initialization . . . . . . . . . . . . . . . . . . . . .
19
3.2
Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.3
Progressive Training
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
3.4
Efﬁcient Hyper-parameter Optimization . . . . . . . . . . . . . . . . . . . . . . .
21
4
Energy-Efﬁcient Inference
24
4.1
Model Pruning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
2
4.2
Low-rank Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
4.3
Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
4.4
Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
5
Efﬁcient Data Usage
32
5.1
Active Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
5.2
Pre-training as Few-shot Learners
. . . . . . . . . . . . . . . . . . . . . . . . . .
33
5.2.1
Self-supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
5.2.2
Contrastive Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
5.2.3
Prompt Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
6
Conclusions and Future Directions
37
3
Chapter 1
Introduction
Deep learning, based on deep neural networks, is part of machine learning methods. In this chapter,
we ﬁrst introduce the development of deep learning in section 1.1. Then, we elucidate what is
Green deep learning, why Green deep learning matters, and how to evaluate the “greenness” of deep
learning in section 1.2.
1.1
Deep Learning
While a decade ago, artiﬁcial intelligence (AI) mainly focuses on shallow models, like structure
perceptrons (McDonald et al., 2010; Huang et al., 2012; Li & Ji, 2014) and conditional random
ﬁelds (Ghosh et al., 2011; Sutton & McCallum, 2012; Zheng et al., 2015). These shallow models
only require limited computations. Most AI approaches can be deployed on CPUs.
In recent years, powerful GPUs have become increasingly accessible, making it possible to de-
ploy larger models, which accelerates the development of deep learning. The ideas of widely-
used deep learning models have been proposed in the 1990s, such as convolutional neural networks
(CNNs) (LeCun et al., 1998) and long-short term networks (LSTMs) (Hochreiter & Schmidhuber,
1997). Conﬁned by hardware capacity and large-scale data resources, these models began to be
popular until the past few years. Collobert et al. (2011) proposed the ﬁrst systematic deep learning
framework for NLP tasks. Krizhevsky et al. (2012) proposed a convolution-based deep network,
which ranked the ﬁrst in the image classiﬁcation challenge. These studies are good pioneers that
motivate AI participants to dive into deep learning.
Deep learning methods currently have become a prime choice for AI. The promising prospect of
neural networks attracts more AI participants to engage with deep learning, in the meanwhile, deep
learning is highly competitive in yielding proﬁts when applied to real-world applications. The in-
dustry continuously develops more efﬁcient hardware and launches better programming platforms,
such as Theano, Caffe, MxNet, Tensorﬂow, and Pytorch. Advanced infrastructures further enable
AI participants to develop stronger deep models. Therefore, deep learning takes a high-speed train
since 2010. To visualize the transition process from shallow models to deep networks in the AI
community, we analyze papers from a top-tier AI conference, ACL, starting from 2010 to 2020.
We randomly select 50 works from each year and manually count the number of papers using deep
learning. As we can see from Figure 1.1, the number of papers using neural networks is growing
fast from 2012 to 2020. All papers adopt deep networks as backbone since 2020. From then, the AI
ﬁeld is fully entered into the age of deep learning.
In the age of deep learning, a hot direction is to obtain SOTA results. Following Schwartz et al.
(2020a), we call such research trend as Red AI. Recently, researchers have noticed that it was harder
to gain an advantage over SOTA results. For traditional AI ﬁelds, like CV and NLP, the improve-
ments achieved by new AI models/algorithms are diminishing. Many popular research benchmarks
are reaching their performance ceiling. Figure 1.2 and 1.3 list several examples showing how the
returns of deep learning are diminishing over time.
4
2010
2012
2014
2016
2018
2020
Year
0.0
0.2
0.4
0.6
0.8
1.0
Papers with Deep Learning / All Papers
Figure 1.1: The ratio of papers using neural networks in ACL, a top-tier NLP conference, from 2010
to 2020. We manually count how many papers use neural networks among sampled 50 papers.
The trend of red AI requires massive computations to achieve better results. For example, as reported
in Schwartz et al. (2020a), the amount of computations used to train deep learning models has
increased 300,000x in 6 years. These computations not only cause expensive ﬁnancial costs but also
contribute to an excessive carbon footprint. The former harms AI inclusiveness and the latter harms
our environment. We classify the computation source required by deep learning into the following
three categories: model size, parameter tuning, and training data.
1) With access to large-scale data resources, increasing model size is the simplest way to improve
results. For example, on WMT English-German translation, the performance can be increased from
27.3 to 28.4 while the size of the machine translation models is increased from 60M (Transformer-
base) to 180M (Transformer-large). To achieve better results, more and more AI participants would
like to increase model size as much as possible, especially for rich organizations. For example,
researchers from OpenAI ﬁrst pre-trained a large-scale text generation model, called GPT-3 with
175B parameters. It shows that a super-large model can generate human-like texts. However, ac-
cording to Strubell et al. (2019), training GPT-3 can emit almost 500M carbons, almost emissions
of ﬁve cars in their lifetime. These studies are key milestones in a long run. However, we believe
that larger models are not always better if we consider “invisible” computation cost. We are still
concerned about “the crazy love” to super-large models no matter whether the increased computa-
tions bring signiﬁcant beneﬁts. In addition, bigger models largely increase the burden of inference
serving. Amazon estimates that 90% of production ML infrastructure costs are for inference, not
training (Jain et al., 2019).
BLEU SCORE
RNN Enc-Dec Att
RNN Enc-Dec Att
GNMT+RL
GNMT+RL
Transformer Big
Transformer Big Transformer Big
Transformer Big
Noisy back-translation
Noisy back-translation
Other models
Models with highest BLEU score
2016
2017
2018
2019
2020
2021
0
10
20
30
40
Figure 1.2: Results of WMT English-German translation from 2016 to 2020.
As we can see,
the recent published results are reaching into the ceiling.
The data is collected from https:
//paperswithcode.com.
5
Figure 1.3: The image classiﬁcation results on ImageNet from 2011 to 2021. As we can see,
the recent published results are reaching into the ceiling.
The data is collected from https:
//paperswithcode.com.
2) Model experiments are also an overlooked computation consumer.
To verify the effective-
ness of a new model/algorithm, AI participants usually conduct massive experiments, including
model/algorithm implementation, baseline re-implementation, and hyper-parameter tuning. First,
baseline re-implementation is an redundant computation source. For example, the original Trans-
former paper has 18K citations. Assume each citation represents a single implementation. Each
re-implementation takes 100 hours on a single GPU (following the cost of running a Transformer-
base model on English-German translation). It means that only baseline re-implementation on a
single dataset can take 1.8M GPU hours. In addition, hyper-parameter tuning is an overlooked com-
putation source. We design a simple questionnaire to ask the ratio of experiments experiments for
hyper-parameter tuning while developing a new model/algorithm, answered by 64 AI specialists,
including researchers and engineers. All of the surveyed choose tuning hyper-parameters. To be
speciﬁc, 10.7%, 32.1%, 35.7%, 21.4% individuals take 80%-100%, 50%-80%, 30%-50%, 0%-30%
experiments to tune hyper-parameters. In sum, 42.8% individuals spend over 50% experiments for
hyper-parameter tuning.
3) Starting from shallow models, it is popular to increase the amount of training data to achieve
better generalization ability, especially in semi-supervised settings. One recent hot topic is pre-
training a super-large model on billions of raw data. In the NLP ﬁeld, ELMo (Peters et al., 2018) is
the ﬁrst well-known work to explore large-scale pre-training. Following ELMo, BERT pre-trains a
Transformer encoder on 3 billion word pieces. Researchers from OpenAI recently proposed GPT-3,
a generative model pre-trained on 45TB data. These massive training examples largely increase the
training costs compared to previous shallow models.
In all, the trend of Red AI brings heavy computation costs. These computations exacerbate the
research inequality, making it difﬁcult to involve all researchers in Red AI. Furthermore, massive
computation requirements bring huge carbon emissions. To address these problems, Green deep
learning, or Green AI, was ﬁrst proposed by Schwartz et al. (2020a) to encourage the AI community
to focus more on energy costs.
1.2
Green Deep Learning
In this section, we mainly describe what is Green deep learning, how to evaluate “greenness” in deep
learning, and why Green deep learning matters.
1.2.1
Deﬁnition
Green learning, a term ﬁrst proposed by Schwartz et al. (2020a), is gaining mounting attention.
Formally, Green deep learning, or Green AI, appeals to researchers to obtain novel results without
increasing computational cost rather, ideally reducing it. Unlike Red AI pushing state-of-the-art
results at any cost, Green deep learning encourages AI participants to achieve comparable or better
results using as few computations as possible.
6
1.2.2
Measure
In Green deep learning, computations are important evaluation metrics. Currently, the whole com-
munity lacks a comprehensive and widely-accepted measure to evaluate computations because mul-
tiple aspects can attribute to computations, including model size, training examples, and so on. A
comprehensive measure is expected for a fair comparison. Here we list several computation mea-
sures and discuss their merits and demerits.
Running time
Some studies adopt the total training time as a kind of computations measure. If
all models/algorithms adopt the same hardware and software settings, it is the most natural measure
to evaluate training/inference computations. However, since running time heavily relies on infras-
tructure settings, it is not suitable for comparing models running on different infrastructures. Even
so, we still encourage AI participants to report the running time for an intuitive understanding.
Carbon emission
Carbon emission is the most direct approach to evaluate environmental effects.
In order to quantify carbon emissions, Lacoste et al. (2019) used CO2-equivalents (CO2eq) as
the amount of CO2 which would have the equivalent global warming impact. However, the main
challenge of this measure lies in accurate estimation. First, computations via electricity consumption
are easily inﬂuenced by local infrastructures. Furthermore, it is hard for AI participants to estimate
the amount of CO2 if they do not run experiments on well-known cloud platforms. Therefore, it is
also not suitable as a standard metric to compare different models running on different regions and
different computing infrastructures.
Model size
The model size is also an important factor in deciding training and inference costs. We
encourage researchers to report model size to corporate with other measures in practice.
FLOPs
Floating-Point Operations (FLOPs) count the number of works required for running a
model when executing a speciﬁc instance. Previous studies usually adopt this metric to evaluate
efﬁciency. FLOPs are almost independent of hardware and software platforms, being the simplest
measure to conduct a fair comparison between different models. However, FLOPs are theoretical
values, and there is a gap between FLOPs and running time. In addition to the total amount of works
(FLOPs), the degree of parallelism also affects the running time.
According to these measures, we summarize evaluation strategies towards fair comparison and intu-
itive understanding.
Fair measure
Generally speaking, AI participants prefer well-performing models/algorithms with
fewer computations. Therefore, it is an important question to fairly compare computations required
for training and inference. We strongly suggest reporting FLOPs during model training and infer-
ence. Last but not least, to evaluate the wasted and redundant computations required for developing
a new model/algorithm, we also encourage researchers to report the total FLOPs during all experi-
ments, including but not limited to parameter tuning and baseline implementation.
Intuitive understanding
To increase the intuitively understanding about computations, we en-
courage researchers to report running time, model size, and carbon emission as optional results.
1.2.3
Broader Impact
First, Green deep learning can help deep learning empower real society applications better. In the
past decade, deep learning continuously pushed state-of-the-art results on research benchmarks, like
machine translation, image classiﬁcation, and so on. In the research community, the cost of deep
learning seems to be nothing compared to energy consumption of all human activities. Nowadays,
deep learning is widely applied to real society tasks, such as auto-driving, face recognition, drug
discovery, and so on. Once deep learning is involved in large-scale applications, the cost of deep
learning will be multiplied hundreds of millions of times. Furthermore, some edge applications, like
mobiles with extremely few computation resources, also require Green deep learning. Therefore,
Green deep learning is a necessary research direction in the future.
Second, Green deep learning can largely improve AI inclusiveness. We note the contributions of
rich organizations for pushing higher results on many downstream tasks. Meanwhile, we also notice
7
the dilemma of researchers from academics and developing countries on engaging Red AI research.
Most researchers only have limited computations, which could not support them to develop super-
large models with state-of-the-art results. Unfortunately, compared to ideas with state-of-the-art
results, novel and innovative ideas without state-of-the-art results are losing their sounds. For exam-
ple, news media would like to report studies with state-of-the-art results. The attractive propaganda
of No.1 also pushes the rich organizations to pour more money on super-big models. These cases
may confuse researchers on how they engage in deep learning research without strong ﬁnancial
support. We argue that state-of-the-art results are good, but not the only criteria to evaluate the qual-
ity of new models/algorithms. We encourage rich organizations to continuously explore data and
model boundaries, also encourage the AI community to pay attention to innovative ideas. In fact,
deep learning struggles many years until it outperforms shallow models with feature engineers. We
believe that the development of AI should be diverse. Green deep learning can improve AI inclusion
and motivate more AI participants to explore deep learning possibilities.
1.3
Outline of the Survey
It is a long-term goal to develop tiny yet strong networks for all AI researchers and engineers. Driven
by this target, several popular tiny networks have been proposed (Howard et al., 2017; Chollet, 2017;
Tan & Le, 2019). For example, MobileNet proposed by Howard et al. (2017) is an efﬁcient architec-
ture based on depthwise separable convolution. Similar idea has been adopted at Xception (Chollet,
2017). Recently, to explore extremely tiny networks, advanced training/inference/network surgery
methods have been proposed. For example, EdgeBERT (Tambe et al., 2021) is proposed to build an
extremely tiny network that can run on IoT devices. It adopts advanced methods like quantization,
pruning, early exit to further reduce model parameters and running computations.
In this survey, we give a systematic review of Green deep learning technologies. We ﬁrst build a
green technology taxonomy and then classify the related technologies into four categories, includ-
ing compact networks, energy-saving training strategies, energy-saving inference, and efﬁcient data
usage. In each category, we review the current progress on Green technologies and explore potential
issues.
It is important to note that building a Green technology taxonomy is challenging since there lacks a
uniﬁed standard measurement. For example, BERT requires massive computations during training.
If we only consider training costs, BERT can not be treated as a Green technology. However, BERT
can improve downstream performance with fewer training examples. If we consider its transfer
ability, BERT is absolutely a Green technology. Therefore, whether a technology is deﬁned as Green
or not is open to doubt. We will try our best to avoid giving a biased deﬁnition. If a technology
has the potential to reduce the costs of deep learning, we will include it in the green technology
taxonomy. We review Green deep learning technologies in the following categories:
• Compact Architecture Design. This part focuses on small networks. We split this chapter
into two sub-chapters, i.e., component design, and component assembling. The component
design focuses on subtle components with competitive results but much fewer computa-
tions. Component assembling describes how to build a network efﬁciently.
• Energy-efﬁcient Training Strategies. Previous studies have proposed several efﬁcient
training approaches. In this survey, we classify these studies into four categories, including
initialization, normalization, progressive training, and efﬁcient AutoML.
• Energy-efﬁcient Inference.
In this chapter, we describe approaches that aim to get a
smaller yet comparable network from a larger network for efﬁcient inference, including
model pruning, low-rank factorization, quantization, distillation.
• Efﬁcient Data Usage. This chapter lists algorithms that leverage training data efﬁciently.
We focus on two popular directions: active learning and pre-trained models as few-shot
learners.
8
Chapter 2
Compact Architecture
Developing efﬁcient neural networks has been a long-standing goal towards Green AI. In this sur-
vey, we deﬁne Green networks as neural networks that are efﬁcient in terms of computational costs.
We can generate compact networks via subtle design, model surgery, and network search. Subtle de-
sign means that we can manually deﬁne efﬁcient architectures requiring fewer computations. Model
surgery means that we can generate compact architectures from a larger model via parameter reduc-
tion. In this chapter, we focus on architectures with subtle design and leave the details of network
surgery to Chapter 4. An overview of this section is shown in Figure 2.1.
Compact Architecture Design
Component
Design
Compact Convolution
Depth-wise Separable Convolution
Fire Convolution
Flattened Convolution
Shrinked Convolution
Efﬁcient Attention
Sparse Attention
Attention Approximation
Lightweight Softmax
Compact Embedding
Component
Assembling
Memory Sharing
Static Weight Sharing
Cross-layer Parameter Sharing
Cross-data Parameter Sharing
Dynamic Weight Sharing
Cascading
Early Exit
Skipping
Mixture of Experts (MoE)
Deployment Weight Sharing
Compact-architecture
Search
Figure 2.1: Taxonomy of compact architecture design with representative examples.
2.1
Component Design
In this section, we describe efﬁcient variants of popular components, including convolution, atten-
tion, softmax, and embedding.
9
2.1.1
Compact Convolution
Starting from AlexNet (Krizhevsky et al., 2012), it has been a hot direction to build deeper and
larger CNNs to achieve better performance (Iandola et al., 2014; Simonyan & Zisserman, 2015;
Szegedy et al., 2015; He et al., 2016a; Szegedy et al., 2017). Currently, even a simple CNN baseline
contains hundreds of layers and thousands of channels. To reduce deployment costs, previous studies
proposed efﬁcient variants. Here we list several widely-used variants.
Depthwise Separable Convolution
This architecture has been adopted in Xception (Chollet,
2017) and MobileNet (Howard et al., 2017). Depthwise separable convolution contains two com-
ponents: depthwise convolution and pointwise convolution. The depthwise convolution applies a
single ﬁlter for each input channel. The pointwise convolution is a kind of 1 × 1 convolution. Fol-
lowing this research line, many advanced variants have been proposed (Hoang & Jo, 2018; Sandler
et al., 2018). For example, Wang et al. (2017) also proposed a factorized convolution by unravelling
the standard convolution and arranging the spatial convolution sequentially.
Fire Convolution
Iandola et al. (2016) proposed a vision model SqueezeNet. The ﬁre module is
the key building block. It is similar with depthwise separable convolutions. A ﬁre module contains
two components: a squeeze convolution layer with 1 × 1 ﬁlters and an expand layer with a mixture
of 1 × 1 and 3 × 3 convolution ﬁlters.
Flattened Convolution
It is proposed by Jin et al. (2015) to decrease the redundancy of the ﬁlters.
It separates the 3D convolution ﬁlters into three consecutive 1D ﬁlters: convolution across channels
(lateral), vertical, and horizontal direction.
Shrinked Convolution
Traditional convolutions usually have ﬁxed hyper-parameter settings, like
the number of ﬁlters. Different from these models, MobileNet (Howard et al., 2017) adopts a dy-
namic setting, also called shrinked convolution. It introduces a width multiplier to thin a network
uniformly at each layer. The standard convolutions with M input channels and N output channels
become a shrinked convolution with αM input channels and αN output channels where α ≤1.
2.1.2
Efﬁcient Attention
Attention (Bahdanau et al., 2015) is ﬁrst proposed for handling long-distance dependency in ma-
chine translation. Currently, it has been widely used in tasks like summarization, natural language
understanding, and so on. The key idea is to dynamically attend all tokens at each step. All to-
kens can be directly aligned together, which can address long-distance dependencies to some extent.
Since any two tokens have an attention score, the required computations grow quadratically with the
input length. To address this problem, previous studies proposed several efﬁcient attention variants.
Currently, dop-product-based attention (Vaswani et al., 2017) becomes the dominant choice for NLP
and CV applications. For simpliﬁcation, we refer to attention as dot-product attention.
We roughly classify these variants into two categories: sparse attention that reduces the span of
attention, and attention approximation with different attention estimation formats. Let us review the
original self-attention deﬁnition. Formally, given a sequence of hidden vectors x, we can map it into
different representation space Q, K, and V . Then, attention takes Q, K, and V as inputs and is
responsible for generating vector via the following equation:
Attention(Q, K, V ) = softmax(QKT
√dk
)V
(2.1)
where Q, K, V are 3-dimension tensors, with dimensions of sequence length, head number, hidden
dimension. The computations mainly come from ( QKT
√dk ) and softmax operations. This section
describes several approaches to reduce dot-product computations. We leave the details of efﬁcient
softmax variants in Section 2.1.3.
Sparse Attention
As we can see from Eq. 2.1, all tokens are required to be attended at each step.
Several approaches are proposed to reduce attention length by only attending local tokens at each
step. A natural solution is to reduce the number of attended tokens by assigning some tokens with
10
zero weights. It is the idea of sparse attention (Martins & Astudillo, 2016; Child et al., 2019; Correia
et al., 2019; Dai et al., 2019; Zaheer et al., 2020). Martins & Astudillo (2016) proposed Sparsemax,
which added a L2 regularization to encourage the attention matrix to be sparse. Sparsemax has been
applied to various architectures (Niculae & Blondel, 2017; Maruf et al., 2019; Peters et al., 2019).
Child et al. (2019) introduced heuristic rules and deﬁned two sparse attention variants. One attends
previous l tokens. The other splits a sequence into different spans where each span has l tokens.
Each head attends to every l tokens where l is much smaller than the length of inputs. Sukhbaatar
et al. (2019) believed that the naive sparse attention was somehow arbitrary. They found that some
attention heads focused on the recent tokens, while other heads took information from the whole
context. Motivated by this, they proposed adaptive attention by learning the attention span of each
head. An similar idea is proposed by Correia et al. (2019). They introduced an adaptive sparse
attention approach. They replaced full softmax operations with α softmax that allowed low-scoring
words to receive precisely zero weight. In addition to attention length, the attention head is also an
important sparse factor. Voita et al. (2019) found that only a subset of heads matter and the rest can
be pruned.
Attention Approximation
Kitaev et al. (2020) proposed an efﬁcient attention model Reformer.
It approximates the dot-product attention computation by one that uses locality-sensitive hashing,
reducing the complexity from O(L2) to O(L log L), where L is the length of the sequence. Choro-
manski et al. (2020) further proposed a more efﬁcient model Performer with an unbiased positive
random feature map estimator. Compared to the original attention, Performer is a linear architecture
with compatible performance.
2.1.3
Lightweight Softmax
Softmax layer is a necessary component for deep learning. The key idea is to normalize a vector to
a probability distribution of possible labels. Traditional softmax is computed as:
p(yi|x) =
exp(hi · w)
P
j=1 exp(hj · w)
(2.2)
where yi is the i-th label and w are learnable parameters. hi is the i-th dimension of hidden vector. x
is the input sequence. The denominator requires the dot-product over label candidates. If the task has
a large label set, the denominator will require large computations. Since the complexity of softmax
is proportional to the number of labels and sequence generation (Mikolov et al., 2013a,b) tasks
usually have a large vocabulary size, we take sequence generation as an example to show several
lightweight variants with fewer computations. In sequence generation tasks, token vocabulary is
equal to the label set. Formally, given a hidden vector h and all token embeddings, the softmax for
sequence generation is computed as:
p(yi|x) =
exp(hT vi)
P
j=1 exp(hT vj)
(2.3)
where x is the input and vi is the embedding of the i-th token. Eq. 2.3 shows that the softmax layer
introduces embeddings for all tokens and requires the inner-product between hidden vector and
all embeddings. A large vocabulary will require many computations. Therefore, several efﬁcient
softmax variants have been proposed.
Fewer Parameters
To reduce memory usage, Press & Wolf (2017) proposed to tie input embed-
dings and embeddings in Eq. 2.3. They conducted experiments on machine translation. Results
show that weight sharing can reduce the size of neural translation models without harming trans-
lation results. In addition, reducing the number of labels is another important research direction.
Recently, several studies have been (Kim et al., 2016b; Costa-juss`
a & Fonollosa, 2016) proposed
to generate a sequence in character level, rather than in word level. The number of characters is
largely less than that of words and the computations for softmax can be largely reduced. Simi-
larly, J´
ozefowicz et al. (2016) implemented character-based softmax on language modeling, which
achieved promising results. It is important to note that these character-based methods also bring
longer sequences. Current sequence generation models usually adopt auto-regressive generation
frameworks. The longer sequence brings higher decoding costs. In all, it should be considered case-
by-case whether character-based methods reduce the whole decoding cost. Recently, a trade-off is
11
achieved by sub-word level vocabularies (Sennrich et al., 2016). Sub-word level vocabularies have
a tradeoff granularity between character vocabularies and word vocabularies. Sub-word level vo-
cabularies have more tokens than character-level vocabulary but also have much shorter segmented
sequences. Therefore, sub-word level vocabularies become the popular choice for almost all se-
quence generation tasks.
Fewer Computations
We classify softmax variants with fewer computations into ﬁve categories:
hierarchical softmax, softmax with dynamic embeddings, sampling-based softmax, hashing-based
softmax, and normalization-based softmax. Hierarchical softmax (H-Softmax) (Morin & Bengio,
2005; Mnih & Hinton, 2008) is a kind of softmax variant. To be speciﬁc, it formulates a label set
as a tree and all labels in the set is the leaf node. The complexity can be dropped from O(N) to
O(log(N)) where N is the size of the label set. In this way, the traditional one single probability over
labels is decomposed into a product of a sequence of probability over each tree layer. The regular
softmax can be regarded as a tree of depth 1, with all labels as leaf nodes. The second research
direction focuses on dynamic label embeddings (Chen et al., 2016b). The intuition is that not all
labels require the same parameter size. It assigns variable parameter sizes for different labels. In
particular, the approach assigns more parameters to frequent labels. The embedding size affects the
computation costs. Therefore, this kind of method can reduce the computations required by softmax
operations. In addition, sampling-based softmax aims to estimate the full softmax computations
with sampled label candidates. The key idea is to sample several label embeddings to estimate all
embeddings (Bengio & Senecal, 2003, 2008; Jean et al., 2015). Hoever, it only reduces training costs
while the full softmax is still be computed to obtain a variance-free result during inference. Hashing-
based softmax is another kind of estimation variant. Vijayanarasimhan et al. (2015) proposed a fast
locality-sensitive hashing technique to approximate the actual dot-product. Normalization-based
softmax (Devlin et al., 2014; Andreas & Klein, 2015) aims to avoid explicit denominator. The target
is to output a vector as close as the probability distribution with the sum being 1.
2.1.4
Compact Embeddings
Building token embeddings is the ﬁrst step for NLP tasks. The parameters of embeddings are de-
cided by vocabulary size and embedding length. How to reduce embedding parameters is an im-
portant and interesting topic. Learning compact token vectors is related to learning compressed
neural networks. There have been several techniques for learning compact neural networks, like
pruning, knowledge distillation, low-rank approximation, and quantization. In this part, we only
focus on related approaches for compact embeddings.
We classify these approaches into four
categories: reuse-based approaches, knowledge-distillation-based approaches, low-rank-based ap-
proaches, ﬁne-grained vocabularies.
Reuse-based approaches focus on compositional embeddings (Faruqui et al., 2015; Chen et al.,
2016c; Shu & Nakayama, 2018; Joshi et al., 2019; Shi et al., 2020). For example, Faruqui et al.
(2015) aimed to represent each token embedding as a sparse linear combination of basis vectors. The
size of basis vectors is much less than token embeddings. Similar idea has been proposed by Chen
et al. (2016c). They split the vocabulary into two parts. One part is a base set containing frequent
tokens with ﬁxed size (e.g., 8K), the other part is a set of rare tokens whose embeddings are en-
coded by the base set’s embeddings. Following these studies, Shu & Nakayama (2018) adopted the
quantization approach to construct embeddings with few basis vectors. Recently, this idea has been
adapted to other ﬁelds beyond NLP, like recommendation systems (Shi et al., 2020).
In addition, traditional compression approaches have been applied to compress embeddings. Mou
et al. (2016) used knowledge distillation to transfer knowledge from a big token embedding layer
into a smaller embedding layer. Chen et al. (2018a) used vocabulary-partition (block) based low-
rank matrix approximation to reduce parameter size. Lam (2018) used 1-2 bits per parameter, rather
than traditional 32-bits, for token embedding. Vocabulary size is also an important factor in decid-
ing embedding size. Therefore, ﬁne-grained vocabularies have been proposed to reduce the vocabu-
lary length, like character-level vocabulary (Kim et al., 2016b), subword-level vocabulary (Sennrich
et al., 2016), and byte-level vocabulary (Wang et al., 2020a).
12
2.2
Component Assembling
This part presents several component assembling solutions for efﬁcient architecture design. Many
widely-used architectures are efﬁcient component assembling solutions. CNNs and LSTMs are
representative models. A single ﬁlter in CNNs can handle all input spans. LSTMs adopt the same
parameters for all steps. The key idea of efﬁcient component assembling lies in sharing. We classify
these assembling solutions into four categories: memory sharing, static weight sharing, dynamic
weight sharing, and deployment weight sharing.
2.2.1
Memory Sharing
Memory sharing is a common technique to store a large model on devices with limited memories.
A natural idea is to share the same storage among intermediate forward vectors (Pleiss et al., 2017)
or backward vectors (Chen et al., 2016a; Gruslys et al., 2016). There are also some reversible mod-
els (Gomez et al., 2017; MacKay et al., 2018) where the activation of each layer can be reconstructed
from the next layer to reduce memory requirements during the backward process. The models do
not need to save intermediate activation vectors. Since several vectors share the same storage space,
recomputation is necessary in some cases. To achieve a trade-off between efﬁcient memory usage
and fewer computations, some studies (Wang et al., 2018a) proposed to combine memory sharing
with liveness analysis (Wang et al., 2016). During graph computation, GPUs adopt liveness analysis
to create tensors and free tensors. For large intermediate tensors, frequent allocation/deallocation
operations are time-consuming. Therefore, the runtime can be reduced by directly reusing memory
segments from a huge pre-allocated memory pool. In addition to memory optimization on a sin-
gle node, Rajbhandari et al. (2020) further explored memory sharing on distributed settings across
multiple computation nodes.
2.2.2
Static Weight Sharing
Unlike memory sharing, static weight sharing aims at exploring how to reuse weights for a neural
network. The difference between weights and intermediate vectors is that weights are ﬁxed during
inference and shared by all examples. To save memory, many models choose to reuse parameters
across different layers or different tasks.
Cross-layer Parameter Sharing
Cross-layer parameter sharing is a common technique for parameter efﬁciency. The idea of sharing
parameters across layers has been well explored (Dehghani et al., 2019; Bai et al., 2019; Lan et al.,
2020). Savarese & Maire (2019) proposed a parameter sharing scheme that deﬁned a global bank
of templates. The parameters of each layer of a CNN come from the linear combination of these
templates. Dehghani et al. (2019) proposed a model, called Universal Transformer, where all layers
shared the same parameters. Following these study, Lan et al. (2020) applied cross-layer sharing
mechanism on pre-train/ﬁne-tune settings and Takase & Kiyono (2021) proposed diverse sharing
strategies. Recently,
Plummer et al. (2020) adopted the idea of network architecture search to
automatically learn how to share parameters between all layers in a network.
Cross-task Parameter Sharing
Cross-task parameter sharing is also a popular solution to handle multi-task, multi-domain, or
multi-lingual problems (Ramsundar et al., 2015; Duong et al., 2015; Søgaard & Goldberg, 2016;
Hashimoto et al., 2017; Yang et al., 2017; Raffel et al., 2020). The key idea of cross-task is to en-
able all tasks (or languages/domains) to share parameters. Multi-task learning (Ruder, 2019) has
two popular implementations, including hard and soft parameter sharing. Compared to soft param-
eter sharing where different tasks do not have shared networks, hard parameter sharing uses fewer
parameters. Therefore, we only focus on hard parameter sharing in this work.
To be speciﬁc, cross-task sharing is initially implemented by sharing the hidden layers between
all tasks, while keeping several task-speciﬁc output layers (Yang et al., 2017; Houlsby et al., 2019;
Raffel et al., 2020). For the CV ﬁeld, multi-task solutions often share CNN layers. For the NLP ﬁled,
in addition to naive sharing, researchers also focus on ﬁnding better parameter reusing solutions for
13
different tasks. For example, Søgaard & Goldberg (2016) found that low-level tasks, e.g., part-of-
speech tagging, should share parameters at lower layers. Motivated by these ﬁndings, Hashimoto
et al. (2017) proposed a parameter-sharing network across multiple NLP tasks. Currently, the trend
of developing large-scale models encourages researchers to directly use one single model to support
multiple tasks. T5 (Raffel et al., 2020) is one representative model.
Multilingual is also a special cross-task variant. At the early stage of multilingual models, re-
searchers usually choose to share a part of parameters across different languages (Firat et al., 2016;
Upadhyay et al., 2016; Blackwood et al., 2018). Recently, multilingual approaches usually treated
all languages equally and mixed them together to train a single model (Ha et al., 2016; Firat et al.,
2017; Johnson et al., 2017; Fan et al., 2020a). More recently, adapter-based solutions have been
widely used for modeling task-speciﬁc features beyond shared parameters (Houlsby et al., 2019;
Bapna & Firat, 2019; Pfeiffer et al., 2020).
2.2.3
Dynamic Weight Sharing
Static parameter sharing usually relies on pre-speciﬁed networks.
Researchers deﬁne heuristic
rules based on shared features to decide which layers/components should be shared by different
inputs/tasks. Although this solution is natural, hard sharing usually fails in handling tasks that are
not closely related. Dynamic solutions are proposed to decide which layers/components should be
shared among different input samples. Speciﬁcally, dynamic networks are neural networks with
dynamic computational graphs where the computational topology or parameters are decided on the
ﬂy. Therefore, this kind of network can reduce computation costs and improve the adaptiveness of
networks. In this survey, we describe the overview of general dynamic architectures. If you are in-
terested in other dynamic features, you can ﬁnd surveys focusing on dynamic networks (Han et al.,
2021b). Networks with dynamic architecture can be classiﬁed into the following classes:
• Cascading-style Networks. Multiple basic networks are cascaded in a directed acyclic
graph (DAG) in a from-small-to-big manner, where the model ﬁrst executes smaller net-
works, then larger networks. If a smaller network can handle the input sample, the model
will stop the execution process and does not run execute models.
• Early-exit-style Networks. A single network contains multiple internal classiﬁers, allowing
“easy” samples to exit at shallow layers. The difference with cascading-style networks lies
in early-exiting networks feed the output of previous layer to the next layer while cascading-
style networks cut off this information ﬂow and every network only takes raw samples as
inputs.
• Skipping-style Networks. It accelerates inference by either skipping certain layers, or skip-
ping unimportant input spans in the whole input sequence.
• Mixture-of-experts-style Networks. Multiple experts are provided as candidates in the same
block. Only a small part of experts are used in each block for inference.
Cascading-style dynamic networks have a historical development (Viola & Jones, 2001; Lienhart
& Maydt, 2002; Viola & Jones, 2004). The authors cascade architectures are originally proposed
for unbalanced binary classiﬁcation tasks. They cascaded multiple basic models and fed the in-
put to the next model only if the current model was not conﬁdent of its prediction. For example,
Park et al. (2015) cascaded two VGG networks in a small-ﬁrst manner to obtain a better trade-off
between classiﬁcation accuracy and energy consumption. The smaller model can handle most sam-
ples, which largely reduce inference costs. Bolukbasi et al. (2017) cascaded AlexNet, GoogLeNet,
and ResNet together. Wang et al. (2018c) introduced a cost-aware objective for jointly training crite-
rion functions among basic models. More recently, Li et al. (2020b) proposed a dynamic framework
for accelerating the inference of pre-trained language models, CascadeBERT, which dynamically
selected proper-sized and complete models in a cascading manner.
Early-exiting-style dynamic networks might be the most popular dynamic architecture nowa-
days (Teerapittayanon et al., 2016; Bolukbasi et al., 2017; Gormez & Koyuncu, 2021; Huang et al.,
2018; Yang et al., 2020b; Wang et al., 2021c). With multiple internal classiﬁers on intermediate
layers, a network is capable to give intermediate predictions and make decisions about whether to
execute the forward process or not. If the answer is yes, current state would be fed to the next layer.
Otherwise, the network outputs the intermediate prediction as the ﬁnal prediction. Different from
14
Table 2.1: An overview of widely-used conﬁdence criteria deciding whether the forward process
should be terminated in cascading-style and early-exiting-style networks. In the Formulation col-
umn, 1(·) ∈{0, 1} indicates action {“continue”, “terminate”}. α is the threshold. λ and τ are
hyper-parameters. MLP(·) is a learnable module.
Criterion
Descriptions
Formulation
Conﬁdence-based Criterion
Score margin
(Park et al., 2015)
The gap between the largest and the second
largest values among the predicted probability distribution.
1(ˆ
y1st −ˆ
y2nd < α)
Entropy
(Teerapittayanon et al., 2016)
(Liu et al., 2020a)
(Li et al., 2021)
The entropy or normalized entropy of
the predicted probability distribution.
1(H(ˆ
y) > α)
Max probability
(Kaya et al., 2019)
(Wang et al., 2020d)
The maximum predicted probability.
1(max(ˆ
y) < α)
Counting-based Criterion
Patience
(Zhou et al., 2020)
The number of identical predictions.
cntcls
i
=
cnti−1 + 1
arg max(ˆ
yi) = arg max(ˆ
yi−1)
0
arg max(ˆ
yi) ̸= arg max(ˆ
yi−1) ∨i = 0
cntreg
i
=
cnti−1 + 1
|ˆ
yi −ˆ
yi−1| < τ
0
|ˆ
yi −ˆ
yi−1| ≥τ ∨i = 0
1(cnti < α)
Voting
(Sun et al., 2021)
The number of most predictions.
Vi = maxc{Pi
l=1 1(arg max(ˆ
yi) = yc)}/iλ
1(Vi) < α
Learning-based Criterion
After-prediction
(Bolukbasi et al., 2017)
(Wang et al., 2018c)
(Schuster et al., 2021)
Take the predicted probability distribution as input
and generate the label
deciding whether to execute the forward process.
MLP(ˆ
y)
Before-prediction
(Elbayad et al., 2020)
(Xin et al., 2021)
Take features as input and generate the label
deciding whether to execute the forward process.
MLP(h)
cascading architectures that cuts off the information ﬂow between networks, early-exiting networks
reuse the feature computed by previous layer. For example, BranchyNet (Teerapittayanon et al.,
2016) inserted several branch classiﬁers into a CNN to speedup inference. MSDNet (Huang et al.,
2018) designed an exquisite two-dimensional multi-scale architecture to enable early exiting along
two dimensions and RANet (Yang et al., 2020b) further utilized the spatial redundancy for image
classiﬁcation.
In addition to the CV ﬁeld, existing studies also apply early-existing-style dynamic networks to the
NLP ﬁeld (Xin et al., 2020; Liu et al., 2020a; Zhu, 2021). The Two Stage ﬁne-tuning is the most
representative approach to train early-existing-style dynamic networks in NLP where the backbone
is ﬁne-tuned with the ﬁnal classiﬁer in the ﬁrst stage, and the intermediate classiﬁers are ﬁne-tuned
in the second stage. In addition, joint training is also a trend to tune all parameters together including
basic backbones and intermediate classiﬁers (Schwartz et al., 2020b; Liao et al., 2021; Geng et al.,
2021). In addition to training algorithms, recent researchers also focus on criterion design. For
example, Zhou et al. (2020) and Sun et al. (2021) utilized counting-based criteria to support early
exiting. Without relying on heuristic criteria, several approaches (Xin et al., 2021; Schuster et al.,
2021) directly learned the criteria by introducing a small module to decide whether to execute the
forward process. Due to the simplicity of single-step prediction, dynamic networks are widely
applied to classiﬁcation models. Recently, Elbayad et al. (2020) and Li et al. (2021) extended
multi-exit design to translation tasks and sequence labeling tasks.
Dynamic halting is a special case of early exiting, where the parameters across layers are shared
and, therefore, the ﬁnal classiﬁer can also be shared. Speciﬁcally, these networks infer samples
through a shared layer iteratively, rather than infer through multiple stacked individual layers. One
representative network is proposed by Graves (2016). They proposed the adaptive computation time
(ACT) mechanism for recurrent models to automatically decide how many times (iterations) each
input symbol or token should be computed. Following this work, the ACT mechanism has been ap-
plied to various architectures, like ResNets and Transformers. For example, SACT (Figurnov et al.,
2017) performed dynamic halting in two dimensions, including the coarse ACT among multiple
layers within the same block and the ﬁne-grained ACT on all spatial positions. Universal Trans-
former (Dehghani et al., 2019) shared all layers within the encoder (or decoder) in Transformer.
15
Discussion
In cascading-style or early-exiting style dynamic networks, the key question is to ﬁg-
ure out how conﬁdent the intermediate classiﬁer is. Previous studies proposed various criteria for
judging the reliability of an intermediate prediction. We categorize them into types as shown in
Table 2.1. Score margin is the gap between the largest and the second largest scores in the predicted
probability distribution (Park et al., 2015). Entropy-based criterion is based on the entropy of the
predicted probability distribution (Teerapittayanon et al., 2016; Li et al., 2021; Liu et al., 2020a).
The model executes the forward process only if the entropy is larger than the pre-deﬁned thresh-
old. Max-probability based criterion is the gap between the max value of the predicted probability
distribution and the pre-deﬁned threshold (Kaya et al., 2019; Wang et al., 2020d). Patience-based
criterion terminates the forward process only if the model generates continuously identical pre-
dictions (Zhou et al., 2020). Voting-based criterion is inspired by the ensemble technique, which
terminates the forward process if the most of historic predictions reach an agreement (Sun et al.,
2021). After-prediction based criterion and before-prediction based criterion introduce additional
learning functions to learn whether to execute the forward process. The only difference lies in that
after-prediction based criterion uses the prediction distribution as inputs and before-prediction based
criterion uses the naive hidden vector as inputs.
Skipping-style dynamic networks skip some computations during forward process. These dynamic
networks are capable to obtain higher efﬁciency. This dynamic solution has been widely-use in
various models, like SkipNet (Wang et al., 2018d), ConvNet-AIG (Veit & Belongie, 2020), and
BlockDrop (Wu et al., 2018b). They introduced additional policy networks responsible for deciding
to skip certain layers or not. The main formula for those dynamic networks could be summarized
as:
SkipNet:
xl+1 = zlFl(xl) + (1 −zl)xl
(2.4)
ConvNet-AIG:
xl+1 = zlFl(xl) + xl
(2.5)
BlockDrop:
xl+1 = zlFl(xl) + xl
(2.6)
where xl is the input of the l-th residual unit, Fl(·) is the network layers within the l-th residual
unit except skip connection, and zl ∈{0, 1} is a binary value predicted by the policy network or the
l-th policy module. By utilizing reinforcement learning or the Gumbel re-parameterization trick, the
network can be trainable in an end-to-end way.
Another research skipping-style line chooses to skip inputs given a long input sequence (Yu et al.,
2017; Campos et al., 2018; Yu et al., 2018) or assign fewer computations to unimportant steps (Jer-
nite et al., 2017; Seo et al., 2018) or exit reading (Yu et al., 2017; Liu et al., 2018c; Yu et al., 2018).
1) Skipping unimportant inputs is the natural way.
Campos et al. (2018) introduced Skip-RNN
where a binary gate unit was used to learn to skip current input token or not. If the answer is yes,
Skip-RNN copies current hidden state to the next time step, saving computations on those unim-
portant inputs. LSTM-Jump (Yu et al., 2017) achieved the same goal by directly predicting how
many steps to jump through, or whether to exit reading inputs. Although skipping partial inputs
saves computations largely, these models, like Skip-RNN and LSTM-Jump, suffer from missing or
repeating outputs at skipped positions thus are not suitable for token-level tasks. 2) To address this
problem, assigning fewer computations to unimportant steps is a ﬂexible solution. To this end, Seo
et al. (2018) proposed Skim-RNN that dynamically decided to update the full-sized hidden state or
partial-sized hidden state at each time step. 3) Exiting-style reading is a special kind of skipping
reading (Shen et al., 2017; Yu et al., 2018; Liu et al., 2018c). It decides to truncate the next inputs.
For example, Liu et al. (2018c) applied the exit mechanism to multi-task scenario. ReasoNet (Shen
et al., 2017) adopted the exit mechanism for machine comprehension tasks. Despite good trade-off
between accuracy and inference speed, skipping-style dynamic networks are harder to train, intro-
ducing more tuning overhead.
Mixture-of-experts-style dynamic networks are representative dynamic models (Lepikhin et al.,
2021; Lin et al., 2021; Fedus et al., 2021). In those models, a layer contains multiple experts and only
part of these experts will be activated for each instance. For example, Switch Transformer (Fedus
et al., 2021) is the representative model that has trillion-level parameters. It replaces the normal
feed-forward layer in the Transformer with a switch feed-forward layer, consisting of a routing
module and multiple structure-identical experts. In each switch layer, only a single expert will be
executed for each token. Compared to general dense computation architectures, mixture-of-expert-
style networks provide an affordable and practical way to modify and train large models with sparse
activation.
16
2.2.4
Deployment Sharing
When deploying deep learning models on edge devices, we have to consider realistic constraints,
such as storage, memory, computation, latency, and power consumption. Previous researchers have
designed lightweight and compact models for mobile devices or other edge devices, such as Mo-
bileNets (Howard et al., 2017; Sandler et al., 2018; Howard et al., 2019). However, with different
hardware resources, the optimal neural network architecture varies signiﬁcantly (Cai et al., 2020).
Thus, developing elastic or dynamic models to satisfy different constraints is critical for practical
applications.
In the recent two years, some studies have paid attention to efﬁcient deployment. In these studies,
a super-network is trained together with its massive sub-networks by task-speciﬁc losses. During
inference, the appropriate sub-network is selected to satisfy the resource constraints. By amortizing
the only-once training cost, the total cost of specialized designing is reduced from O(N) to O(1).
During inference, the model can dynamically choose an appropriate network for different devices.
To be speciﬁc, Yu et al. (2019) proposed slimmable neural networks where several widths are pre-
deﬁned, supporting instant and adaptive accuracy-efﬁciency trade-offs by selecting corresponding
width. Following this work, Yu & Huang (2019) further proposed US-Nets to support arbitrary
width selection. Fan et al. (2020b) proposed an elastic network that can select sub-networks of any
depth from one large network without having to ﬁnetune them. Beyond aforementioned studies,
the temporal or input length is also an elastic selection. For example, Kim & Cho (2021) pro-
posed length-adaptive Transformer to support arbitrary progressively length deduction. The Length-
adaptive Transformer can be directly adopted into the downstream task and satisfy any efﬁciency
constraints by searching the corresponding length deduction conﬁgurations.
2.3
Compact-architecture Search
In addition to model design, there are studies working on search efﬁcient networks towards resource-
constraint devices, like mobile. They borrow the idea of neural architecture search and apply it
to design tiny networks. For example, Tan et al. (2019a) proposed a neural architecture search
approach, which explicitly incorporated model latency into the main objective so that the search can
identify a model with a good trade-off between accuracy and latency. On the ImageNet classiﬁcation
task, this approach achieved 75.2% top-1 accuracy with 1.8x faster than MobileNet V2.
Howard
et al. (2019) combined neural architecture search and network design together to develop a stronger
mobile net MobileNet V3. Cai et al. (2019) directly learned the architectures on the target task and
hardware. Wu et al. (2019) proposed a differentiable neural architecture search framework that used
gradient-based methods to optimize ConvNet architectures towards mobile devices.
17
Chapter 3
Energy-Efﬁcient Training
Many advanced approaches have been proposed to reduce training costs for deep learning. In Chap-
ter 2, we describe efﬁcient networks that can reduce computations in a single execution. In this
chapter, we focus on the computations required during the whole training, including weight tun-
ing and hyper-parameter tuning. To be speciﬁc, we survey approaches that aim to accelerate weight
tuning/hyper-parameter tuning by using fewer iterations, including initialization, normalization, pro-
gressive training, and efﬁcient NAS. An overview is shown in Figure 3.1.
Energy-efﬁcient Training
Initialization
Random Initialization
Kaiming Initialization
Xaiver Initialization
Fixup Initialization
LSUV Initialization
Pre-trained Models
for Initialization
Feature based Initialization
Fine-tuning based Initialization
Supervised Initialization
Self-supervised Initialization
Normalization
Batch Normalization
Layer Normalization
Group Normalization
Progressive
Training
Efﬁcient AutoML
Search Space
Continuous
Discrete
Cell block
Meta-architecture
Search Method
RL-based Search
Evolution-based Search
Differentiable Search
Evaluation Method
Early Stop
Weight Sharing
Hypernetworks
Figure 3.1: Taxonomy of energy-efﬁcient training with representative examples.
18
3.1
Initialization
The training of deep learning starts from architecture design and parameter initialization. We have
explored efﬁcient architecture design in Section 2. In this part, we focus on how weight initialization
affects model training.
Table 3.1: Summarization of two common initialization approaches. d and u mean the dimensions of
weight matrix W. Uniform and Normal mean the uniform distribution and Gassuian distribution.
Initialization Approach
Description
Formulation
Kaiming initialization
(He et al., 2015)
Distribution of standard deviation of
q
2
d
W ∼Normal(0, 2
d) or W ∼Uniform(−
q
6
d,
q
6
d)
Xaiver initialization
(Glorot & Bengio, 2010)
Distribution of standard deviation of
q
2
d+u
W ∼Normal(0,
2
d+u) or W ∼Uniform(−
q
6
d+u,
q
6
d+u)
3.1.1
Random Initialization
It is widely accepted that good initialization of weights in a neural network is critical to conver-
gence (Glorot & Bengio, 2010; Krizhevsky et al., 2012; He et al., 2015; Mishkin & Matas, 2016;
Kumar, 2017). At the beginning, deep networks are usually initialized via random weights drawn
from uniform distributions or Gaussian distributions. Many previous studies found that these kinds
of initialization failed in handling very deep models (Glorot & Bengio, 2010; Saxe et al., 2014;
Romero et al., 2015; Hanin & Rolnick, 2018). The problem is caused that the mean/variance of
activations and gradients exponentially with the depth. To enable training with a very deep model,
some advanced initialization solutions, like Kaiming initialization (He et al., 2015), Xaiver initial-
ization (Glorot & Bengio, 2010), LSUV initialization (Mishkin & Matas, 2016) and Fixup initializa-
tion (Zhang et al., 2019), have been proposed. The key idea is to normalize the variance of weights
to make the variance of activation in each layer to be around 1. We list the details of two widely-
used initialization approaches in Table 3.1. Compared to the naive baseline, these initialization
approaches can achieve better performance and faster convergence (Mishkin & Matas, 2016).
3.1.2
Pre-trained Models for Initialization
In addition to random initialization, many approaches borrow models pre-trained from other do-
mains (or other tasks) as initialization. It is widely believed that initialization from existing models
is an effective technique to improve the generalization ability with fewer training iterations. We split
these pre-training initialization into different categories according to different dimensions. First, ac-
cording to whether the borrowed parameters keep unchanged, these methods can be classiﬁed into
feature-based initialization, and ﬁne-tuning-based initialization. Second, according to the knowl-
edge source of pre-trained parameters, these methods can be classiﬁed into supervised initialization
and self-supervised initialization.
Feature-based initialization borrows the parameters (usually from low-layers or mid-layers) as
initialization from other domains/tasks while these parameters keep ﬁxed during training. Generally
speaking, feature based initialization can keep the generalization ability of the borrowed parameters
better and thus is more suitable for extremely few-shot settings.
Fine-tuning-based initialization uses the target data to train all parameters, including new pa-
rameters and borrowed parameters. Fine-tuning based initialization can further optimize the target
objectives via ﬁne-tuning all parameters and thus can better ﬁt training data. It is the most popular
solution nowadays for the NLP ﬁeld.
Supervised initialization is widely investigated in the earlier stage of deep learning. A common
solution is to pre-train the target model on similar tasks/datasets, and then reuse the pre-trained
parameters as initialization for the target task (Huang et al., 2013; Oquab et al., 2014; Yosinski et al.,
2014; Duong et al., 2015; Long et al., 2016). This solution is especially popular for low-resource
settings and is extensively studied on domain adaptation/transfer learning.
The most representative example for supervised initialization is the pre-training of deep CNN back-
bones (Simonyan & Zisserman, 2015; Ren et al., 2015; He et al., 2016a; Simon et al., 2016; He
et al., 2017a; Iglovikov & Shvets, 2018). Fine-tuning pre-trained CNNs on different downstream
19
datasets usually leads to improved performance compared to training from scratch and also reduces
the number of training steps. Previous researches have explored advanced initialization methods. It
is widely accepted that layers near to inputs usually are responsible to capture local features (Zeiler
& Fergus, 2014). Therefore, many studies focus on transferring knowledge via initialization from
low-layer features and mid-layer features. Interestingly, with the increase of large-scale training
data, current trends directly adopt the simplest solution that uses all parameters for initialization (Li
et al., 2020a).
Supervised initialization is also successfully applied to NLP (Socher et al., 2013). It is widely-
accepted that features computed in higher layers of the network usually depend on the speciﬁc
dataset and task. Following this belief, many studies borrow the parameters of low-level layers and
mid-level layers from other domains as initialization (Dong et al., 2015; Luong et al., 2016; Yang
et al., 2017; Lin et al., 2018; Liu et al., 2018b). Recently, the trend of large-scale networks enables
researchers to reuse all parameters from pre-trained networks. Similar with CV, the widely-adopt
setting in NLP is directly reusing all parameters (Johnson et al., 2017; Aharoni et al., 2019; Tan
et al., 2019b; Bapna & Firat, 2019; Lin et al., 2020).
Self-supervised initialization is also a popular direction. With the increasing parameters in state-
of-the-art DNNs, more and more training data are required to achieve better generalization results.
To reduce the requirements of supervised data, previous studies investigated self-supervised pre-
training that exploited unlabeled data to construct supervision signals to learn representations. Since
self-supervised pre-training does not require any human-annotated labels, it is easy to get sufﬁcient
training data. To this end, researchers designed various methods to construct self-supervised training
signals with unlabeled data. Here we take CV and NLP as examples to review recent self-supervised
pre-training models.
For the NLP ﬁelds, using a model pre-trained on self-supervised data as initialization is the most pop-
ular solution. At the start, researchers use language modeling to pre-train word embeddings which
are then used to initialize downstream word embeddings (Joshi et al., 2016; Qi et al., 2018; Ruder
et al., 2019). Glove is one widely-used word embedding toolkit (Pennington et al., 2014) which
trains word embeddings based on global word-word co-occurrence counts. With the development
of representation learning, researchers begin to explore and reuse contextualized models. Contex-
tualized models deﬁne that the representation of a word depends on its contexts and each word has
two representations, ﬁxed word embeddings and contextualized representations. Peters et al. (2018)
proposed the ﬁrst widely-used contextualized representations, ELMo. Following this work, many
advanced contextualized representation models begin to spring up, like BERT (Devlin et al., 2019),
GPT (Schick & Sch¨
utze, 2021), T5 (Raffel et al., 2020). The development of pre-trained networks
also affect the application of CV. In recent years, CV began to explore large-scale self-supervised
models for initialization (Lu et al., 2019; Li et al., 2020a; Chen et al., 2020b). The learning objective
is similar with NLP’s pre-trained networks, either recovering masked/noised regions, or generating
the original image from scratch.
Empirical results demonstrate that these pre-trained networks for initialization can achieve better
performance and faster convergence. However, since current pre-trained networks generally require
downstream tasks to use the exactly same networks, the training time still depends on architecture
execution in addition to convergence speed. Therefore, it should be considered case by case to
conclude whether pre-trained models for initialization reduce downstream training costs in imple-
mentation.
3.2
Normalization
In addition to initialization approaches, normalization is another solution to accelerate training.
Strictly speaking, normalization is a special component. Considering that it can accelerate con-
vergence (Bjorck et al., 2018; Santurkar et al., 2018; Zhang et al., 2019), we describe normalization
in this chapter.
Normalization is a technique to normalize hidden outputs in deep neural networks. Batch normal-
ization (Ioffe & Szegedy, 2015) is the ﬁrst widely-used normalization for deep models. The key
idea is to normalize the hidden vectors of neural networks to the distribution with mean µ = 0 and
standard deviation σ = 1. The hidden vectors usually are tensors and batch normalization is applied
20
on the batch dimension. To be speciﬁc, it generates the output given the hidden output h:
yi = yi −µ
σ + ϵ , µ =
1
|B|
|B|
X
i=1
hb,i, σ =
1
|B|
|B|
X
i=1
q
(hb,i −µ)2
(3.1)
where h is a intermediate tensor where the ﬁrst is batch dimension. |B| is the batch size. y and h are
the output and input of the normalization component. Ioffe & Szegedy (2015) fond that applied to
a state-of-the-art image classiﬁcation model, batch normalization achieved the same accuracy with
14 times fewer training steps, and beat the original model by a signiﬁcant margin.
Following batch normalization, many normalization variants have been proposed, like layer normal-
ization (Ba et al., 2016), group normalization (Wu & He, 2020), weight normalization (Salimans &
Kingma, 2016). These variants have almost the same calculation process except they are applied to
different dimensions or different objectives.
Despite good performance, it is still controversial where the beneﬁts of normalization come. At
the start, normalization is proposed to address internal covariate shift by normalizing layer inputs.
Internal covariate shift is a phenomenon where the distribution of each layer’s inputs changes during
training. The parameters of the higher layer are required to continuously ﬁt for the new distribution
of lower layers, which slows down the training. To keep distribution steady, normalization is pro-
posed to ﬁx the distribution of input to a standard distribution. However, Santurkar et al. (2018)
overturned this belief and they found that the distributional stability of layer inputs had little to do
with the success of batch normalization. Instead, normalization makes the optimization landscape
signiﬁcantly smoother. This smoothness induces more predictive and stable gradients, allowing for
faster training. Motivated by this paper, Xu et al. (2019) proved that normalization indeed normal-
ized backward gradients, which plays an important role in deciding the success of normalization.
3.3
Progressive Training
Progressive training is another strategy to effectively train DNNs. The key idea is constructively
adding layers. Compared to full training, progressive training does not require full gradients to all
parameters, thus can largely reduce computations required for training. In addition, the well-trained
lower layers also accelerate the training of higher layers. Hinton et al. (2006) applied progressive
training to deep belief networks. They trained layers sequentially starting from bottom layers in a
greedy, layer-wise fashion. It is based on an assumption that upper layers represent more “abstract”
concepts whereas lower layers extract “low-level features”. This method is unsupervised because
each layer learns a higher-level representation of the layer below and the training criterion does not
depend on the labels. Following this work, Bengio et al. (2006) extended this method to handle
continuous inputs.
With the development of deep learning, layer-wise progressive training methods are exploited to
train CNNs (Rueda-Plata et al., 2015; Kulkarni & Karande, 2017; Belilovsky et al., 2019) and
RNNs (Xu et al., 2018). Recently, Gong et al. (2019) and Yang et al. (2020a) extended the idea
of layer-wise training to large-scale NLP models by progressively stacking new layers on top of
previously trained layers. Their experimental results show that layer-wise training can successfully
improve the efﬁciency of training large transformer language models with huge amounts of data. To
be speciﬁc, experimental results show that such progressive training policy can achieve more than
110% training speedup without signiﬁcant performance degradation.
3.4
Efﬁcient Hyper-parameter Optimization
During training, hyper-parameter optimization (HPO) is a common and fundamental step for AI
participants to ﬁnd better model settings. Hyper-parameters keep ﬁxed during training, including
but not limited to optimization settings (e.g., learning rate, batch size) and model settings (e.g., the
number of layers). Since deep learning performs like a black-box model and the learning landscape
is non-convex, current optimization approaches usually ﬁnd a random local minimum. Due to the
uncertainty, AI engineers tend to taking a lot of computations to ﬁnd better hyper-parameter settings
on real-world applications (Yu & Zhu, 2020). HPO or autoML is a ﬁeld to automatically ﬁnd the
optimal settings. Considering that previous approaches mainly study architecture settings, we take
21
efﬁcient neural architecture search (NAS) as an example in this survey to review recent progress.
Following previous studies, we split NAS methods into three components: search space, search
strategy, and architecture evaluation. In this survey, we give an overview of efﬁcient NAS. There are
also surveys describing more details of NAS (Elsken et al., 2018).
Figure 3.2: An overview of NAS components.
The search space deﬁnes all architecture candidates. At the ﬁrst, the search space is deﬁned as a
discrete space, including structured space or unstructured space. Considering that network candi-
dates in unstructured space is too massive, researchers usually incorporate inductive bias to build a
structured search space (Liu et al., 2018a; Dong & Yang, 2020; Shu et al., 2020). One of the repre-
sentative methods is cell-based search space. Cell-based search space assumes that each architecture
contains repetitions of ﬁxed structures. In this way, the search space can be limited to the cell space
where the number of candidates is largely reduced. In addition, to enable faster search, differentiable
approaches (Jiang et al., 2019) adopt a continuous search space where edge weights are considered.
The search strategy deﬁnes a policy to explore search space. Random search is one of traditional
search approaches. The key idea is to randomly evaluate architectures and to select the best one
based on their validation results. To reduce wasted evaluation costs, researchers proposed reinforce-
ment learning based search policies (Ying et al., 2019). It is a direction that introduces an architec-
ture generator to generate well-performing architectures. Since random search and reinforcement
learning require validation accuracy as search criterion, these methods usually need expensive com-
putations. To reduce search costs, evolution based search has been proposed (Real et al., 2019). It
is a two-stage search approach. The ﬁrst stage selects several well-performing parent architectures.
The second stage applies mutation on these parent architectures to select the best one. The second
stage starts from pre-trained parent networks and does not require too much computations to train
child networks. Recently, So et al. (2019) applied the evolution search on Transformer networks
and achieved new state-of-the-art results on machine translation and language modeling tasks. Al-
though these approaches can reduce exploration costs, the dependence on validation accuracy still
leads to considerable computation costs. To fully get rid of the dependence on validation accuracy,
several studies (Jiang et al., 2019; Liu et al., 2019a; Dong & Yang, 2019; Zela et al., 2020; Chu
et al., 2020) proposed differentiable search that re-formulated the task in a differentiable manner
and allowed efﬁcient search using gradient descent. In addition, another research line aims to rep-
resent a model into a continuous space where there is a mapping between structures and results.
In this way, the model only learns how to predict the performance of architectures based on their
continuous representations where the downstream training is not required (Luo et al., 2018). To fur-
ther reduce training costs, researchers proposed training-free NAS approaches that directly extracted
features from randomly-initialized models and used these features as evaluation criterion to select
networks (Mellor et al., 2020; Chen et al., 2021a; Abdelfattah et al., 2021; Xu et al., 2021b).
Architecture evaluation takes almost all computations in NAS approaches. At the ﬁrst, full training
is required to evaluate the performance of a network, which is very heavy. Early stop is a widely
used trick to estimate the results of a network. Besides, parameter-sharing is also a popular solution
22
that network candidates can share parameters with each other (Pham et al., 2018). In this way, the
model can reuse pre-trained blocks during downstream training.
Discussion
Current NAS solutions are well-explored in the CV ﬁeld, and widely-used benchmarks
are also based on CV datasets. In the future, it is a promising direction to apply NAS in other ﬁelds,
like NLP, to address more real-world problems. In addition, existing models focus more on models
towards a single task for simpliﬁcation. The multi-task/domain/lingual model is attracting more
attention. Therefore, how to use NAS to search a shared multi-task/domain/lingual model is also a
promising direction. Furthermore, the essential question of NAS is how model architecture affects
downstream results. More understanding studies are expected to reveal the fundamental connection
between architecture and performance.
23
Chapter 4
Energy-Efﬁcient Inference
In this chapter, we describe common network surgery methods for reducing inference costs, includ-
ing pruning, low-rank factorization, quantization, and knowledge distillation. A brief review of these
methods is presented in Figure 4.1 and Table 4.1.
Efﬁcient Inference
Pruning
Pruning Unit
Unstructured
Neurons, Connections
Structured
Filters, Channels, Layers
Scoring Function
Magnitude
Important Coefﬁcients
Gradient-based
Movement Pruning
Scheduling
Single-step Pruning
Iterative Pruning
Lottery Ticket
Low-rank
Factorization
Matrix Factorization
Low-rank Matrix Factorization, SVD
Tensor Factorization
CP, VBMF, Tucker Decomposition, BTD
Quantization
Deterministic
Quantization
Rounding, Vector Quantization
Stochastic
Quantization
Random Rounding, Probabilistic Quantization
Knowledge
Distillation
Distillation Target
Logits-based (Vanilla KD)
Feature-based
Relation-based
Teacher Numbers
Dynamic KD, Multi-teacher KD, Mutual Learning
Figure 4.1: Taxonomy of efﬁcient inference methods with representative examples.
4.1
Model Pruning
Model pruning is a popular solution to reduce redundant parameters in DNNs. Back to 1980s, Han-
son & Pratt (1988) and LeCun et al. (1989) already veriﬁed that parameters are not equally important
to the ﬁnal performance. By removing unimportant weights from a network, we can simultaneously
24
Table 4.1: Different approaches for efﬁcient inference.
Approaches
Descriptions
Characteristics
Pruning
Reduce redundant parameters which
Can be applied to various settings.
are not sensitive to results.
Fine-tuning is optional.
Low-rank Factorization
Use matrix/tensor decomposition to
Matrix decomposition is computationally
approximate the original parameters.
complicated, but can support train from scratch.
Quantization
Reduce the number of bits used to
Easy to implement.
represent weights and activations.
Sensitive to hardware architecture.
Knowledge Distillation
Train a compact neural network with
Easy to implement.
knowledge distilled from a teacher model.
Sensitive to network parameters.
reduce the number of parameters, accelerate training/inference, save training examples, and improve
generalization. This motivates a great amount of studies on pruning neural networks in the past 30
years. Speciﬁcally, given an initial network that is large and accurate, the key idea of pruning is
to remove parameters from the original network to produce a smaller network that can retain the
accuracy of the original model.
We ﬁrst provide a formal deﬁnition of pruning. Let us deﬁne a neural network model as f(X; θ),
which is a function over an input set X and the set of parameters θ. Pruning approaches usually take
a model f(X; θ) as input and then produce a new model f(X; θ′). θ′ is a set of parameters with the
size of θ′ being less than that of θ. Usually, θ′ is a subset of θ.
Algorithm 1 The generic framework of pruning
Require: N is the number of iterations of pruning; X is the dataset
θ′ ←train-to-convergence(f(X; θ))
for i in 1 to N do
θ′ ←prune(score(θ′))
θ′ ←ﬁne-tune(f(X; θ′))
end for
return θ′
Previous pruning approaches mainly follow the work of Han et al. (2015) to produce a pruned
model f(X; θ′) from an original model f(X; θ). We show the generic framework in Algorithm 1.
First, the network is trained to convergence to get pre-trained parameters. Second, each parameter
or structural element in the network is assigned a score. This score indicates the relative importance
to the ﬁnal performance. The network is then pruned based on these scores. Third, since pruning
generally reduces the accuracy of the network, it is a general practice to ﬁne-tune (train further after
pruning) the pruned network. The process of pruning and ﬁne-tuning is usually iterated several
times.
We describe some key components of network pruning algorithms:
• Pruning Unit refers to the basic unit that the algorithm aims to prune. According to prun-
ing units, we can classify modern pruning approaches into two categories, unstructured
pruning, and structured pruning. Some pruning algorithms prune individual parameters
(i.e., unstructured pruning), which produce a sparse neural network. While the resulting
sparse network is smaller in terms of the number of parameters, it is hard to yield speedups
since the pruned weights are not well arranged. In contrast, structured pruning considers
parameters in groups. They keep the dense features of the network by removing entire
weight matrices, ﬁlters, channels, or layers.
• Scoring Function deﬁnes the metric used to prune parameters. Common practices for
parameter scoring usually are based on importance coefﬁcients, network activations, or
gradients. After assigning scores to each part of the parameters, we have two choices to
prune networks. First, we can choose to prune a fraction of the parameters with the locally
lowest scores within each structural sub-component of the network (e.g., layers). Second,
we also can choose parameters with the globally lowest scores within the entire network.
• Scheduling decides the total step that pruning algorithms use to prune parameters. Some
methods prune weights in a single step while another methods use multiple steps to prune
parameters where each step only prunes a part of parameters.
25
• Fine-tuning is usually required for the pruned network to recover the original accuracy.
Many methods choose to ﬁne-tune the pruned network, or re-train the pruned network.
Application
Pruning is ﬁrst applied to fully-connected networks. For example,
LeCun et al.
(1989) analyzed the importance of parameters and showed that small magnitude weights had less
impact on training losses. Speciﬁcally, they computed the saliency of parameters based on the sec-
ond derivative. Then, they pruned parameters with small saliency scores. To recover the original per-
formance of the network, the network was ﬁne-tuned after pruning. Hassibi et al. (1993) extended
this idea by using the inverse of the Hessian as saliency score. In addition to weight pruning, Suzuki
et al. (2001) proposed to prune network connections based on their inﬂuence on training losses, and
then re-train the network to compensate the performance drop. Unlike these approaches, Srinivas
& Babu (2015) argued that similar neurons were redundant. They proposed to remove redundant
neurons, instead of removing individual weight connections one by one.
Currently, many pruning algorithms are applied to CNNs.
Han et al. (2015) proposed a simple
magnitude-based method to remove unimportant connections in fully-connected layers and con-
volutional layers. However, the resulting model, despite being sparse, does not bring signiﬁcant
inference speedups due to the feature of sparsity. To address this problem, several structured prun-
ing algorithms have been proposed to prune dense blocks, like ﬁlters, channels, or layers (Li et al.,
2017; Molchanov et al., 2017; He et al., 2017b; Lin et al., 2017; He et al., 2018; Luo et al., 2019).
In addition to CV models, pruning has been successfully applied to NLP tasks. At the early stage,
several studies successfully pruned recurrent neural networks (RNNs). See et al. (2016) used itera-
tive pruning and retraining to prune a recurrent model for neural translation. Narang et al. (2017a)
pruned RNNs via magnitude based pruning.
Narang et al. (2017b) used iterative ground lasso
regularization to induce block sparsity in RNNs.
Lee et al. (2019) and Zhang & Stadie (2020)
proposed one-shot RNN pruning methods based on connection sensitivity and Jacobian spectrum.
More recently, with the success of Transformer, several studies investigated pruning transformer
models (Michel et al., 2019; Voita et al., 2019; McCarley et al., 2019; Fan et al., 2020b; Wang et al.,
2020c; Sanh et al., 2020). One trend is to use structured pruning since the transformer architecture is
highly parallelized. For instance, Fan et al. (2020b) introduced LayerDrop to prune transformer lay-
ers for efﬁcient inference. Michel et al. (2019) and Voita et al. (2019) revealed that the multi-head
attention mechanism in the transformer architecture led to redundant attention heads. Motivated by
this ﬁnding, they proposed to directly prune attention heads. McCarley et al. (2019) used structured
pruning to compress a BERT-based question answering model. Xu et al. (2020) recently proposed
a progressive module replacing approach by replacing a whole module from the original model
with a compact module to reduce model size. Unlike these studies, Guo et al. (2020) proposed an
unstructured pruning approach, diff pruning, to compress a multi-task model.
While most aforementioned pruning algorithms require re-using or re-training the originally trained
network, a recent research direction (Frankle & Carbin, 2019) suggested that dense, randomly-
initialized, feed-forward networks contained sub-networks (winning tickets), which can reached the
test accuracy of the original network. They also found that a standard pruning technique naturally
uncovered some of the winning tickets and then proposed an algorithm to identify winning tickets at
the early stage of training. Also, Frankle & Carbin (2019) and Liu et al. (2019c) found that once the
“winning ticket” is found, it can be trained from scratch to get an equivalent or better performance
compared to pruned and ﬁne-tuned one. Recently, there are also several studies investigating the
lottery ticket hypothesis for BERT-like models (Prasanna et al., 2020; Chen et al., 2020c). For
example, Prasanna et al. (2020) found that with structured pruning, the “random” sub-networks are
still almost as good as the “good” ones, and even the “worst” ones perform on par with a strong
baseline.
Discussion
Pruning is very effective for reducing the number of parameters in DNNs. With struc-
tured pruning, it can accelerate inference and reduce computations. However, there are also several
limitations of pruning methods. First, it requires iteratively scoring weights and re-training the net-
work for many iterations. Also, pruning often leads to a non-negligible performance drop when
applied to powerful DNNs. The lottery ticket hypothesis provides an interesting direction for more
efﬁcient “pruning” algorithms. In this survey, we give an overview of pruning methods. If you are
interested in this direction, there are several surveys describing more details of pruning (Liang et al.,
2021).
26
4.2
Low-rank Factorization
Tensor (including matrix) operation is the basic building block and contributes the bulk of most
computations and parameters in DNNs. Therefore, compressing tensors or matrices in DNNs is
promising for reducing the number of parameters and computation costs. The motivation of low-
rank factorization is that a large amount of redundant information exist in large weight matrices.
The super-large matrices are generally of low rank and can be factorized into several tiny matrices
to save parameters. For example, we can apply singular value decomposition (SVD) to factorize a
super large matrix. SVD factorizes the original weight matrix into three smaller matrices. Formally,
for any matrix A ∈Rm×n, there exists a factorization, A = USV T where U ∈Rm×r and V T ∈
Rr×n are orthogonal matrices, S ∈Rr×r is a diagonal matrix with only singular values of A on
the diagonal. With SVD decomposition, the spatial complexity can be reduced from O(mn) to
O(r(m + n + 1)).
Application
Similar to pruning algorithms, low-rank factorization is successfully applied to CV
models. Sainath et al. (2013) applied matrix factorization to the ﬁnal weight layer. If the original
weight matrix has the dimension m × n and rank r, the full rank matrix can be factorized in two
weight matrices as m×r and r×n. Their approach reduced the number of parameters and achieved
up to 30–50% speedup. Similarly, Xue et al. (2014) proposed to use SVD decomposition to compress
fully-connected neural networks. Rigamonti et al. (2013) proposed to approximate trained CNNs
with low-rank ﬁlters. Denton et al. (2014) further exploited the linear structure present within the
convolutional ﬁlters. Their approach was able to reduce the memory requirement of the weights in
the ﬁrst two convolutional layers by 2–3 times. While low-rank matrix factorization can optimize
both the spatial and computational complexity of neural networks, the plane view of a matrix limits
the potential for extreme compression.
Tensor factorization algorithms, in contrast, are more ﬂexible and can be employed to achieve an
extremely high compression ratio. Among popular tensor factorization methods, classical prolonga-
tion (CP) (Kolda & Bader, 2009), where each factor matrix has the same rank and the kernel tensor is
a superdialognal tensor, generally achieves better compression performance. Lebedev et al. (2015)
leveraged CP decomposition to compress CNN weight kernels into several sub-kernels to reduce
the number of parameters. Speciﬁcally, non-linear least squares were used to compute low-rank
CP-decomposition of the 4-D tensor into a sum of rank-one tensors. Using this decomposition, the
original convolutional layer was replaced by a sequence of 4 convolutional layers with smaller ﬁl-
ters. Following this idea, Kim et al. (2016a) introduced a one-shot compression method to compress
the whole network. In addition, Chen et al. (2018b) introduced a collective residual unit based on
block term decomposition (BTD), which is a combination of Tucker and CP, to enhance the utiliza-
tion of parameters in residual CNNs. Zhou et al. (2019) conversely used neural networks to learn
an appropriate CP rank for tensor decomposition.
Apart from the applications on CV models, low-rank factorization has also been applied to NLP
models. For example, Grachev et al. (2017) used low-rank factorization to train RNN language
models.
Winata et al. (2019) investigated the use of low-rank factorization as an effective post-
processing compression method for LSTMs. They applied low-rank factorization on ELMo, one of
widely-used pre-trained models. Recently, low-rank factorization has also been applied on Trans-
former models (Ma et al., 2019). Noach & Goldberg (2020) further proposed a two-stage model-
compression method to reduce the inference time cost of BERT, a kind of Transformer-based model.
Their approach decomposed the matrices into smaller matrices and then performed feature distilla-
tion on the internal representation. Also, Lan et al. (2020) applied embedding matrix factorization
along with layer sharing to reduce the amount of parameters.
Discussion
Compared with other popular compression methods, low-rank factorization can effec-
tively reduce the size of models with a large compression ratio while preserving the performance
well. Low-rank factorization is also relatively ﬂexible. However, low-rank factorization also suffers
from the issue of computational efﬁciency because SVD over large weight matrices can be com-
putationally heavy. Also, compared with the compression ratio in terms of model size, low-rank
factorization is less effective for reducing the computational cost and inference time.
27
4.3
Quantization
The goal of quantization is to compress the original network by reducing the number of bits. The
idea of network quantization can be back to early 1990s (Fiesler et al., 1990; Balzer et al., 1991;
Tang & Kwan, 1993). Recently, due to the success of DNNs and their growing sizes, the research of
quantization has received increasing attention. In the beginning of 2010s, Vanhoucke et al. (2011)
discovered that CNNs encoded with 32-bit can be converted to CNNs encoded with 8-bit, which
signiﬁcantly reduced both storage and computation costs.
Generally speaking, quantization techniques can be classiﬁed into two types: deterministic quanti-
zation and stochastic quantization. In deterministic quantization, there is an deterministic mapping
between the quantized value and the real value. In stochastic quantization, the quantized value is
sampled from discrete distributions (Guo, 2018). Usually, post-training quantization is the most
simplest solution by applying quantization on a trained model to reduce inference costs. Despite
simple, post-training quantization may brings dropped performance. quantization-aware training is
proposed to address this problem by ﬁne-tuning the quantized model before inference.
Deterministic Quantization
deﬁnes a deterministic mapping between real weights and quantized
weights. Rounding quantization is the simplest mapping function. The key idea of rounding quan-
tization is to map a high-bit ﬂoating-point number to its nearest ﬁxed-point low-bit number (Gupta
et al., 2015). For example, suppose a number x and the target ﬁxed-point representation [IL, FL].
The number of integer bits IL plus the number of fractional bits FL yields the total number of bits
used to represent the number. This approach considers the following rounding scheme:
Convert(x, [IL, FL]) =



−2IL-1
if x ≤−2IL-1,
2IL-1 −2−FL
if x ≥2IL-1 −2−FL,
Round(x)
otherwise
(4.1)
where
Round(x) =
⌊x⌋
if ⌊x⌋≤x ≤⌊x⌋+ ϵ
2,
⌊x⌋+ ϵ
if ⌊x⌋+ ϵ
2 < x ≤⌊x⌋+ ϵ
(4.2)
where ϵ(= 2−FL) is the smallest positive number that can be represented in this ﬁxed-point format,
⌊x⌋is deﬁned as the largest integer multiple of ϵ smaller than x. Following this approach, more
advanced approaches have been proposed (Rastegari et al., 2016; Polino et al., 2018; Wu et al.,
2018a).
In addition to scalar quantization for individual numbers in weight vectors, there is also a research
line focusing on clustering-based quantization.
Gong et al. (2014) proposed to classify weights
into groups and to use the centroid of each group to replace the actual weights during inference.
Han et al. (2016) employed a similar approach but ﬁne-tuned the quantized centroids for better
performance. Following this idea, Choi et al. (2017) further proposed a Hessian weighted k-means
clustering approach.
Stochastic Quantization
does not deﬁne one-to-one mapping from real weights to quantized
weights. In random rounding, the quantized value is sampled from a discrete distribution param-
eterized by real values. For example, Courbariaux et al. (2015) proposed the following random
rounding function:
xb =
+1
with probability p = σ(x),
−1
with probability 1 −p
where σ is the “hard sigmoid” function:
σ(x) = clip(x + 1
2
, 0, 1) = max(0, min(1, x + 1
2
))
If x is a positive value, we will have a high probability to quantize it to +1, otherwise to −1.
This gives us a more ﬂexible quantization scheme. In probabilistic quantization, the weights are
assumed to be discretely distributed and a learning algorithm is used to infer the parameters of
the distributions. Soudry et al. (2014) proposed an expectation back-propagation algorithm to train
neural networks with binary or ternary weights. They ﬁrst assumed some discrete prior distribution
on the weights and then updated the weights in an online setting based on the Bayesian formula.
28
Quantization-Aware Training
Recently, quantization-aware training (Jacob et al., 2018; Dong
et al., 2017; Fan et al., 2020c) has become the de facto approach towards designing robust quantized
models. It simulated quantization effects in the forward pass of training and the backward pass was
accomplished via straight through estimator (Bengio et al., 2013b). It generally relied on techniques
like gradient clipping to make the training stable. Recently, several studies analyzed and introduced
new quantization-aware training approaches. For example, Fan et al. (2020c) and Dong et al. (2017)
stochastically applied quantization to a portion of the weights at each training step, while Sheng
et al. (2018) and Alizadeh et al. (2019) re-ordered the blocks or layers.
Applications
Network quantization is ﬁrst widely applied to CNNs. In addition to CNNs, quan-
tization has been also applied to other models, like RNN, Transformer.
Ott et al. (2016) ﬁrst in-
vestigated RNN quantization and found that weight binarization did not work well on RNNs. For
simpliﬁcation, they proposed to apply weight quantization for RNN weights and to leave activations
as ﬂoating-point numbers. Hubara et al. (2017) explored different combinations of bit-widths for
weights and activations. He et al. (2016b) proposed to quantize the structure of gates and interlinks
in LSTM and GRU cells. Recently, Wang et al. (2018b) proposed to use a threshold ternary quan-
tization method for weight quantization and a Bernoulli ternary quantization method for activation
quantization.
With the recent success of Transformer, a number of studies investigated the application of quantiza-
tion on Transformers. For example, Bhandare et al. (2019) and Prato et al. (2020) showed that 8-bit
quantization can successfully reduce the size of a Transformer-based model and accelerate inference
without compromising translation quality. Recently, quantization has been applied on Transformer-
based language models (Zafrir et al., 2019; Zhang et al., 2020; Bai et al., 2020; Kim et al., 2021).
Zafrir et al. (2019) ﬁrst applied 8-bit quantization on BERT. Following this work, Kim et al. (2021)
proposed I-BERT, which employed lightweight integer-only approximation methods for nonlinear
operations to quantize BERT. Zhang et al. (2020) proposed TernaryBERT, which ternarized the
weights in a ﬁne-tuned BERT model with both approximation-based and loss-aware ternarization
methods.
Very recently, several studies have investigated the application of quantization on GNNs. Feng et al.
(2020) proposed a GNN-tailored quantization algorithm, and used an automatic bit-selecting ap-
proach to pinpoint the most appropriate quantization bits. Wang et al. (2021a) and Bahri et al. (2020)
further proposed binarized GNNs.
Tailor et al. (2021) proposed Degree-Quant, an architecture-
agnostic method for quantization-aware training on graphs. Moreover, Zhao et al. (2020) proposed
to use neural architecture search to ﬁnd the optimal architecture coupled with the best quantisation
strategy for different components in GNNs.
Discussion
Quantization is very effective for reducing the size of neural networks. However, post-
training quantization often leads to non-neglieable performance drop. In contrast, quantization-
aware training can effectively reduces the performance drop. Incorporating knowledge distillation
for quantization can also improve the performance of quantized models. In this section, we give an
overview of quantization. If you are interested in more details, please refer to surveys (Guo, 2018;
Gholami et al., 2021).
4.4
Knowledge Distillation
The idea of knowledge distillation (KD) is exploiting the knowledge inside a large trained “teacher”
model to help the training of a “student” model (Bucila et al., 2006; Ba & Caruana, 2014; Hin-
ton et al., 2015). In this way, we can use a smaller student model to distill a trained model as a
replacement for inference.
The traditional KD solution is to minimize the difference between the output produced by the teacher
model and that produced by the student model. Formally, given a labeled dataset D of N samples
D = {(x1, y1) , . . . , (xN, yN)}, we can write the loss function of the student network during the
process of knowledge distillation as follows:
LS (D; θS; θT ) = 1
N
N
X
i=1
[αLT (yi, S (xi; θS)) + (1 −α) LKD (T (xi; θT ) , S (xi; θS))]
(4.3)
29
where α is a hyper-parameter to control the relative importance of the two terms; θT and θS are the
parameters of teacher T and student S, respectively. LT refers to the task-speciﬁc loss and LKD
refers to the knowledge distillation loss which measures the similarity of the student and the teacher.
In general, KD exploits the knowledge from the teacher model to help train the student model by
minimizing the discrepancy between the knowledge in the teacher model and that in the student
model. According to the source of teacher knowledge, we can classify KD approaches into three
categories: logits-based KD, feature-based KD, and relation-based KD. According to teacher types,
we also can classify KD approaches into three categories: KD with static teacher, KD with multiple
teachers, and KD with dynamic teacher.
Logits-based KD
focuses on the output class distribution of the teacher model, also referred as
“soft labels”. This is the vanilla form of knowledge distillation (Hinton et al., 2015; Ba & Caruana,
2014). Soft targets generated by the teacher model provide much more information than hard targets.
Therefore, training the student model to ﬁt soft targets can help the student model generalize better
like the teacher model.
Feature-based KD
exploits intermediate features to teach the student model, which is believed
to be important for representation learning (Bengio et al., 2013a). The simplest solution is to min-
imize the distance between intermediate representation of each student layer and its corresponding
teacher layer (Romero et al., 2015; Sun et al., 2019a; Aguilar et al., 2020). It enables the student
model to exploit richer information from the teacher model. Recently, a number of feature-based
KD studies have been proposed. In summary, these studies mainly focused on two key factors:
selection of intermediate representations, and distance metric. The intuition of investigating better
intermediate representations is that the knowledge of teacher should be easy to learn for the student
model (Zhang et al., 2017; Zagoruyko & Komodakis, 2017; Huang & Wang, 2017; Ahn et al., 2019;
Heo et al., 2019; Sun et al., 2019a; Aguilar et al., 2020). For example, Huang & Wang (2017) pro-
posed to match the distributions of neuron selectivity patterns between the teacher and the student
models. Kim et al. (2018) trained a paraphrase model as TFt to extract transferable features from
the teacher’s intermediate representations, and a translator model as TFs to map the student inter-
mediate representation to teacher’s representations. As for distance metrics, L2 distance is the most
widely used distance metric. Besides, L1 distance (Wang et al., 2019b) and KL-divergence (Liu
et al., 2019b; Aguilar et al., 2020) are also used in previous KD approaches.
Relation-based KD aims to minimize the correlation between feature pairs from the teacher model
and the student model (Yim et al., 2017; Srinivas & Fleuret, 2018; Lee et al., 2018; Tung & Mori,
2019; Lee & Song, 2019; Peng et al., 2019). Distance can be seen a special kind of relation measure.
Recently, many approaches have been proposed to explore better relation measures. For example,
MHGD (Lee & Song, 2019) employed a multi-head attention network to encode relations between
any two feature maps in a certain batch of training instances. CCKD (Peng et al., 2019) transferred
correlation between instances with a generalized kernel method based on Taylor series expansion.
KD with Multiple Teachers
Traditional KD methods focus on transferring the knowledge from
one teacher model to the student model. A number of recent studies investigated knowledge transfer
from multiple teachers or an ensemble of teachers. The most popular and straightforward way is to
learn the ensemble of teacher logits (Tarvainen & Valpola, 2017; You et al., 2017). Following this
idea, multiple studies have been proposed to model the diversity of teachers using a weighted average
of teacher logits (Ruder et al., 2017; Lan et al., 2018; Xiang et al., 2020). Apart from averaged
logits, using the ensemble of features from multiple teachers is another line of research. (Park &
Kwak, 2019) proposed to train the student’s feature map to minimize the gap from the feature maps
of multiple teachers with different feature transformation functions. While achieving promising
performance, traditional KD methods using multiple teachers suffer from expensive computations
because they require multiple pre-trained teachers. To alleviate this problem, Zhang et al. (2018)
proposed a deep mutual learning approach, which was an initial form of online KD that has been
developed by various studies (Lan et al., 2018; Anil et al., 2018; Chen et al., 2020a; Chung et al.,
2020; Kim et al., 2020). In online KD, a set of student models, or peers, was trained simultaneously
by learning from each other in a peer-teaching fashion.
KD with Dynamic Teacher
In traditional KD, the teacher model is ﬁxed during KD. However,
this can be sub-optimal because the generalization ability of the student model is dynamic during
training. A number of studies explored KD with an evolving teacher model to keep a reasonable
30
capacity difference between student and teacher (Mirzadeh et al., 2020; Shi et al., 2021; Zhou et al.,
2021b; Park et al., 2021). For example, Jin et al. (2019) designed a sequence of intermediate targets
to impose curriculum-based constraint on the optimization path of the student model for improved
KD. Shi et al. (2021) and Park et al. (2021) proposed to jointly update the teacher model and the
student model with task speciﬁc objectives during KD.
Discussion
Knowledge distillation is a widely-used approach to get a smaller but more competitive
model. However, although the idea of KD is easy to implement, it also has several limitations. First,
the performance of KD is very sensitive to the size gap between the teacher model and the student
model. The discrepancy between the expressive power of the models would make it hard to teach
the student model. Second, it relies on training data and may not be suitable for few-shot or zero-
shot settings. In addition, recent studies (Xu et al., 2021a; Stanton et al., 2021) have revealed that
while knowledge distillation can effectively improve student generalization, there was still a large
discrepancy between the predictive distributions of student and teacher models. It means that there
is still a long way to distill full knowledge in a teacher model to a student model.
31
Chapter 5
Efﬁcient Data Usage
Following the deﬁnition of Green deep learning, this chapter mainly explores how to achieve com-
petitive results with fewer data resources, including active learning and pre-training. It is worth
noticing that although pre-trained models take massive computations during pre-training, they are
widely believed to be a practical solution to release the burden of data in downstream tasks. There-
fore, we also include pre-trained models in this chapter.
Green Data Usage
Active Learning
Uncertainty-based
Diversity-based
Expected Model Change
Pre-training as Few-shot Learners
Self-supervised Pre-training
Contrastive Pre-training
Prompt Pre-training
Figure 5.1: Taxonomy of efﬁcient data usage methods with representative examples.
5.1
Active Learning
Active learning is a research direction aiming at using as few samples as possibles to achieve good
results. It is initially proposed to reduce annotation costs. Nowadays, pool-based active learning is
widely-used to reduce training costs by selecting the most useful examples to train networks. The
intuitive behind active learning is quite simple. The annotated training data do not equally contribute
to the ﬁnal performance. If we can always select the most useful example to train models, the wasted
training on unimportant examples can be largely reduced.
Active learning usually starts from a randomly-initialized model or a pre-trained model. It deﬁnes
several query strategies to select new unlabeled data to query annotation. The new data associated
with labels are then used to train the model. This process keeps running until it reaches the maximum
number of labelled data or other termination conditions are satisﬁed. Previous active learning ap-
proaches mainly focused on query strategies to improve performance. Following the work of Yoo &
Kweon (2019), given a pool of unlabeled data, we classify active learning approaches into three cate-
gories according to the selection criteria: uncertainty-based approaches, diversity-based approaches,
and expected-model-change based approaches.
Uncertainty-based active learning is used to be the most common choice in active learning, using
uncertainty scores to select data (Ranganathan et al., 2017; Schr¨
oder et al., 2021; He et al., 2019;
Shen et al., 2018). The simplest solution is to utilize class posterior probabilities to deﬁne uncer-
tainty. To be speciﬁc, Lewis & Gale (1994) used the probability of a predicted class as uncertainty
score.
Joshi et al. (2009) deﬁned an entropy of class posterior probabilities as uncertainty score.
32
Another research line is to train multiple models to construct a committee, and used the commit-
tee to evaluate uncertainty (Beluch et al., 2018). Recently, uncertainty-based active learning has
been widely applied to various ﬁelds. Ranganathan et al. (2017) applied active learning on image
classiﬁcation which selected the most informative unlabeled samples to train a deep belief network
model.
Shen et al. (2018) applied uncertainty-based active learning to sequence tagging. They
selected sentences for which the length-normalized log probability of the current prediction was the
lowest.
Ducoffe & Precioso (2018) focused on margin-based active learning for deep networks.
Despite promising effectiveness, uncertainty-based sampling can easily lead to insufﬁcient diversity
of batch query samples.
Diversity-based active learning has been proposed to address the challenges of uncertainty based
approaches. For example, Sener & Savarese (2018) deﬁned a core-set to estimate the whole training
data. Nguyen & Smeulders (2004) proposed to incorporate clustering into active learning. It ﬁrst
constructed a classiﬁer on the set of the cluster representatives, and then propagated the classiﬁcation
decision to the other samples via a local noise model.
Expected-model-change based active learning selected data points that would cause the greatest
change to current model. For example, Roy & McCallum (2001) selected examples according to
the reduced error rate on future test examples. Freytag et al. (2014) measured the expected change of
model outputs and incorporated the underlying data distribution. For each example of an unlabeled
set, the expected change of model predictions was calculated and marginalized over the unknown
label.
5.2
Pre-training as Few-shot Learners
It is widely-believed that pre-trained models as initialization is an effective approach to reduce data
requirements in downstream tasks. In this survey, we describe widely-used pre-trained models.
5.2.1
Self-supervised Learning
self-supervised learning (SSL) is the most popular solution to get pre-trained models. We take NLP
as an example to review recent progress of self-supervised learning. Self-supervised objectives can
be classiﬁed into types including masked language modeling (MLM), language modeling (LM), de-
nosing auto-encoding (DAE). In this survey, we give an overview of these models. We refer the
reader to Qiu et al. (2020) and Han et al. (2021a) for more details of pre-trained networks.
Masked Language Modeling
Devlin et al. (2019) proposed to pre-train an Transformer encoder,
i.e., BERT, via MLM objective on unlabeled text. MLM builds a corrupted token sequence where
15% tokens are randomly sampled and replaced with a special token [MASK], then requires the
Transformer to predict the original tokens. Formally, given a sequence x1:L = [x1, x2, · · · , xL] and
the masked token set M, the MLM objective can be formulated as
LMLM = −
X
xt∈M
log p(xt|x\M)
(5.1)
where x\M indicates the unmasked part of the input sequence. With the inherited knowledge,
ﬁne-tuned BERT performs well compared to baselines without pre-training on many classiﬁcation
tasks, including single sentence classiﬁcation (Warstadt et al., 2019; Socher et al., 2013), sentence
pair classiﬁcation or matching (Dolan & Brockett, 2005; Cer et al., 2017), natural language infer-
ence (Williams et al., 2018; Wang et al., 2019a; Bentivogli et al., 2009; Levesque et al., 2012), and
question answering (Rajpurkar et al., 2016, 2018), etc.
Several studies have made effort to improve MLM through developing more effective MLM-like ob-
jectives or exploring more efﬁcient training tricks. SpanBERT (Joshi et al., 2020), ERNIE (Sun et al.,
2019b), and BERT-WWM (Cui et al., 2019) proposed to mask adjacent tokens. ELECTRA (Clark
et al., 2020) proposed a GAN-like replaced token prediction objective which required discriminator
to discriminate whether a token is replaced or not. Instead of masking, StructBERT (Wang et al.,
2020b) learned to predict the shufﬂed span in its original order, which incorporated language struc-
tures into pre-training.
33
Language Modeling
predicts next tokens one by one in an auto-regressive way. Speciﬁcally,
given a sequence x1:L = [x1, x2, · · · , xL], the joint probability of x can be written as
p(x1:L) =
L
Y
t=1
p(xt|x0:t−1)
(5.2)
LLM = −
X
t
log p(xt|x<t)
(5.3)
while x0 is the special [BOS] token which indicates the beginning of a sentence. LM is usually
implemented with a Transformer decoder. GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al.,
2020) are two representative LM-based pre-trained models. The LM objective directly models the
probability of next token given its left context. LM-based pre-trained models as initialization can
largely improve conditional natural language generation tasks like summarization (Rush et al., 2015;
See et al., 2017) and question answering (Rajpurkar et al., 2018; Reddy et al., 2019). Therefore, LM
models usually are used to initialize generative models.
There are lots of follow-up studies improving the original LM objective from various aspects. For
example, inspired by the fact that neural networks could have different reading orders with human,
XLNet (Yang et al., 2019) proposed to use permuted language modeling to predict a sentence in
permuted orders in an auto-regressive way. UniLM (Dong et al., 2019) proposed a preﬁx LM where
next tokens are predicted in an auto-regressive way with all context tokens visible to each other.
ProphetNet (Qi et al., 2020) introduced a future n-gram prediction objective to predict the next n
tokens simultaneously based on previous context at each time step, through a multi-stream attention
similar to XLNet.
De-nosing Auto-encoding
requires a model to recover the original sentence based on the cor-
rupted version. Formally, DAE-like objectives can be formulated as
LDAE = −
X
t
log p(xt|ˆ
x, x<t)
(5.4)
where ˆ
x is the corrupted input. By learning to distinguish which part of content is corrupted in a
text and recovering it in a natural order, DAE-based pre-trained models as initialization can improve
various models, including language understanding and language generation.
Actually, there are also ﬂexible variants to corrupt a sequence, such as shufﬂing, masking, dropping,
rotating, etc. BART (Lewis et al., 2020) corrupted its input sequence with arbitrary noise transforma-
tions. The transformations include token masking, token deletion, sentence permutation, document
rotation, and text inﬁlling. More recently, Zhou et al. (2021a) proposed to use text rewriting instead
of text inﬁlling (e.g., BART, T5) for improving seq2seq pre-trained transformers.
Multilingual Objectives
Another line of SSL objectives is multilingual SSL objectives. For ex-
ample, mBERT (Devlin et al., 2019) 1, mBART (Liu et al., 2020b), mT5 (Xue et al., 2021) pre-
trained the multilingual version BERT, BART, T5 via the multilingual masked language modeling
objective. These models highly improve the results of few-shot or low-shot multilingual learning or
multilingual generation tasks. In addition to these simple variants, recent researchers also designed
more sophisticated cross-lingual objectives, such as cross-lingual word recovery, cross-lingual para-
phrase classiﬁcation, and cross-lingual masked language modeling.
5.2.2
Contrastive Learning
Contrastive learning focuses on pair-wise relationships, aiming at learning closer or similar represen-
tation for positive samples while pushing negative samples away. The recent years have witnessed
the rapid progress of contrastive-based pre-trained models, especially in CV (van den Oord et al.,
2018; He et al., 2020; Chen et al., 2020d, 2021b).
The standard contrastive learning utilizes positive and negative pairs at the same time to construct
its objective. This kind of loss can back to noise contrastive estimation (NCE) loss (Gutmann &
1https://github.com/google-research/bert/blob/master/multilingual.md
34
Hyv¨
arinen, 2010) that is deﬁned as
LNCE = −log
exp(f(q, k+)/τ)
exp(f(q, k+)/τ) + exp(f(q, k−)/τ)
(5.5)
where q is the anchor sample; k+ and k−are its positive sample and negative sample; q, k+ and
k−are vectors generated by a neural network; f(·, ·) is a similarity function; τ is the temperature
controlling the concentration of the induced distribution. When more than one negative samples
exist, the NCE loss becomes the InfoNCE loss:
LInfoNCE = −log
exp(f(q, k+)/τ)
exp(f(q, k+)/τ) + PK
i=1 exp(f(q, ki)/τ)
(5.6)
where ki represents the i-th negative sample of anchor/query sample q; K is the size of negative
samples. van den Oord et al. (2018) proved that minimizing the InfoNCE loss was equivalent to
maximizing the lower bound of mutual information between q and k+:
I(q, k+) ≥log(K) −LInfoNCE
(5.7)
where the more negative samples are, the tighter the lower bound is.
Overall, contrastive losses are easy to implement and contrastive-based pre-trained models generally
have strong transfer ability. In the CV ﬁeld, it largely improve downstream tasks, such as ImageNet
classiﬁcation (Deng et al., 2009), object detection (Everingham et al., 2010), and egmentation (Lin
et al., 2014), etc. MoCo (He et al., 2020; Chen et al., 2020e, 2021b) is one of representative models
which applied advanced contrastive learning methods to train pre-trained models on CV ﬁelds.
Existing state-of-the-art contrastive-based pre-trained models are simple variants of Eq. 5.6 where
positive pairs and negative paris are required. For example, Deep InfoMax applied contrastive losses
to learn image representations via maximizing the mutual information between local patches and the
whole image; CMC (Tian et al., 2020) applied contrastive losses to learn representations where dif-
ferent views of the same scene or instance were sampled as positive pairs. According to Wang &
Isola (2020), the InfoNCE loss optimized two properties of a representation space, including the
alignment of representations between positive samples and the uniformity of the induced distribu-
tion of normalized representations on a hypersphere in the representation space. Nevertheless, the
recent studies such as BYOL (Grill et al., 2020), SwAV (Caron et al., 2020), Simsiam (Chen & He,
2021) found that contrastive learning also works without negative samples. In all, the research of
contrastive-based pre-training is still in rapid development. More understanding studies are expected
to better explain how contrastive losses work.
Previous studies also applied contrastive learning to NLP tasks. For example, CAPT (Luo et al.,
2020) taken a sentence si and its masked version ˜
si as a positive pair.
CERT (Fang & Xie,
2020) utilized back-translation to generate noised positive samples for the source English sentence;
CLEAR (Wu et al., 2020) directly integrated four text augmentation mechanisms including word
deletion, span deletion, reordering, and substitution, and taken two augmented versions of an sen-
tence as positive pairs; DeCLUTR (Giorgi et al., 2021) samples nearby even overlapping spans as
positive pairs; SimCSE (Gao et al., 2021) added noise to encoded representations via dropout and
regard noised representations as positive samples. SimCSE performed pertty well on retrieval tasks
and achieved new SoTA on semantic textual similarity tasks.
5.2.3
Prompt Learning
As the size of pre-trained models grows rapidly, ﬁne-tuning the super large pre-trained model usually
requires massive data to get better generalization ability, thus failing on few-shot/zero-shot applica-
tions. The objective gap between pre-training and ﬁne-tuning stages are one of important reasons
behind failure. Therefore, to improve data efﬁciency, prompt learning is proposed by extracting the
similar/same template for pre-training and ﬁne-tuning stages.
Scao & Rush (2021) demonstrated
that a prompt may be equal to 100 conventional data points, indicating that prompts can greatly
improve the sample efﬁciency.
In the recent survey about prompt learning (Liu et al., 2021a), prompt is deﬁned as x′ = fprompt(x),
where fprompt(·) is the prompting function to rewrite the input x with an task-speciﬁc template.
Suppose that we have a review “I dozed off when watching this movie.” as input x associated with
35
sentiment label y. A prompt example x′ is “I dozed off when watching this movie. It is so [MASK]”.
In this way, we transfer sentiment prediction to a masked language modeling task. As pointed by
Wei et al. (2021), language models at scale like GPT-3 (Brown et al., 2020) contain substantial world
knowledge and can perform a range of NLP tasks, which makes large pre-trained models a kind of
“neural knowledge bases” (Petroni et al., 2019). In fact, prompts can be seen as queries for those
neural knowledge bases. The difﬁculty is (1) how to generate appropriate queries to achieve your
goals, and (2) how to understand or interpret the response in the format of predicted textual content.
Generally, prompts are deﬁned as discrete templates. In addition, there are also studies focusing on
continuous prompts, rather than textual prompts. For example, Preﬁx Tuning (Li & Liang, 2021)
and P-tuning (Liu et al., 2021c) deﬁned virtual tokens where only parameters of virtual tokens were
ﬁne-tuned. The more recent P-tuning V2 (Liu et al., 2021b) used multi-task learning objectives and
obtained competitive even better results than vanilla ﬁne-tuning. SPoT (Vu et al., 2021) further uti-
lized transfer learning to support unseen tasks, where the preﬁx was pre-tuning on related tasks then
used for initialization in unseen task. Overall, these methods are ideologically similar to adaptation
approaches (Houlsby et al., 2019; Bapna & Firat, 2019; Pfeiffer et al., 2020) because both of them
do not change the most of parameters of the pre-trained model. However, the adaptation methods
usually insert adaptors between layers, undermining the original model architecture.
36
Chapter 6
Conclusions and Future Directions
We believe that Green deep learning will be an important research direction in the future. With recent
advances in deep learning, the community have made signiﬁcant progress in developing super-large
models for downstream tasks, making it possible to apply AI models on complicated applications.
Considering that it is the ultimate goal to deploy AI models on real-world devices with high require-
ments on resource usage, how to transfer strong models to resource-constraint devices (e.g., mobile)
becomes a priority goal. In this section, we list several challenges for Green deep learning.
Green deep learning theory. We have harnessed some advanced Green deep learning techniques,
but many questions still remain to be explored. For example, 1) If we already have a well-performing
model, why was additional training required to transfer knowledge to a small model? 2) How many
parameters do we need at least for feasible training and inference? 3) How to design Green learning
algorithms to enable efﬁcient zero-shot learning or few-shot learning like human? 4) How our
model store knowledge and how to make models to achieve lifelong learning without forgetting
learned knowledge during training? 5) Is linear algebra is the only basic theory for deep learning
and whether can we develop a new operation system beyond linear algebra? In this survey, we just
show limited questions due to page limitation. The community still have a long way to go in the
theory of Green deep learning
Green deep learning under extreme computation constraints.
Deploying models on edge de-
vices enjoys multiple advantages. First, it can avoid uploading user information to the cloud amid the
tide of protecting user privacy. Second, it can empower more applications with high requirements on
latency if deployed a light-weight model on edge devices. While recent advances in machine learn-
ing greatly facilitate efﬁcient training and inference in the cloud, edge devices bring new challenges
caused by extremely strict memory and computation constraints. Therefore, how to design advanced
training and inference algorithms towards tiny devices is also an important and challenging direc-
tion. First, algorithm-hardware cooperation is a promising direction to satisfy speed requirements
when deploying large models. For example, Lightseq (Wang et al., 2021b) and Faster Transformer
are two representative models that use CUDA implementation to accelerate Transformer execution.
Second, edge-cloud cooperation is also a practical solution by combing powerful and elastic cloud
computing and immediate edge computing. An intuitive solution is to design a dynamic network
where edge devices can handle simple samples with smaller models and the cloud is responsible for
handing complicated samples with larger models.
37
Bibliography
Mohamed S. Abdelfattah, Abhinav Mehrotra, Lukasz Dudziak, and Nicholas D. Lane. Zero-cost
proxies for lightweight NAS. ICLR, 2021.
Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao, Xing Fan, and Chenlei Guo. Knowledge dis-
tillation from internal representations. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intel-
ligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Confer-
ence, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence,
EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 7350–7357. AAAI Press, 2020.
Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation.
In NAACL-HLT (1), pp. 3874–3884. Association for Computational Linguistics, 2019.
Sungsoo Ahn, Shell Xu Hu, Andreas C. Damianou, Neil D. Lawrence, and Zhenwen Dai. Variational
information distillation for knowledge transfer. In IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 9163–9171.
Computer Vision Foundation / IEEE, 2019.
Milad Alizadeh, Javier Fern´
andez-Marqu´
es, Nicholas D. Lane, and Yarin Gal. An empirical study
of binary neural networks’ optimisation. In 7th International Conference on Learning Represen-
tations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Jacob Andreas and Dan Klein. When and why are log-linear models self-normalizing?
In HLT-
NAACL, pp. 244–249. The Association for Computational Linguistics, 2015.
Rohan Anil, Gabriel Pereyra, Alexandre Passos, R´
obert Orm´
andi, George E. Dahl, and Geoffrey E.
Hinton. Large scale distributed neural network training through online distillation. In 6th Inter-
national Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30
- May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Zoubin Ghahramani, Max
Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neu-
ral Information Processing Systems 27: Annual Conference on Neural Information Processing
Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2654–2662, 2014.
Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.
Layer normalization.
CoRR,
abs/1607.06450, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In ICLR, 2015.
Mehdi Bahri, Ga´
etan Bahl, and Stefanos Zafeiriou.
Binary graph neural networks.
CoRR,
abs/2012.15823, 2020.
Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael R. Lyu, and
Irwin King. Binarybert: Pushing the limit of BERT quantization. CoRR, abs/2012.15701, 2020.
Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In NeurIPS, pp. 688–699,
2019.
Wolfgang Balzer, Masanobu Takahashi, Jun Ohta, and Kazuo Kyuma. Weight quantization in boltz-
mann machines. Neural Networks, 4(3):405–409, 1991.
Ankur Bapna and Orhan Firat.
Simple, scalable adaptation for neural machine translation.
In
EMNLP/IJCNLP (1), pp. 1538–1548. Association for Computational Linguistics, 2019.
38
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon.
Greedy layerwise learning can
scale to imagenet. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the
36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 583–593. PMLR,
2019.
William H. Beluch, Tim Genewein, Andreas N¨
urnberger, and Jan M. K¨
ohler. The power of en-
sembles for active learning in image classiﬁcation. In CVPR, pp. 9368–9377. Computer Vision
Foundation / IEEE Computer Society, 2018.
Yoshua Bengio and Jean-S´
ebastien Senecal. Quick training of probabilistic neural nets by impor-
tance sampling. In AISTATS. Society for Artiﬁcial Intelligence and Statistics, 2003.
Yoshua Bengio and Jean-S´
ebastien Senecal. Adaptive importance sampling to accelerate training of
a neural probabilistic language model. IEEE Trans. Neural Networks, 19(4):713–722, 2008.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of
deep networks. In Bernhard Sch¨
olkopf, John C. Platt, and Thomas Hofmann (eds.), Advances in
Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on
Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7,
2006, pp. 153–160. MIT Press, 2006.
Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. Representation learning: A review and
new perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798–1828, 2013a.
Yoshua Bengio, Nicholas L´
eonard, and Aaron C. Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013b.
Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. The
ﬁfth PASCAL recognizing textual entailment challenge. In Proceedings of the Second Text Anal-
ysis Conference, TAC 2009, Gaithersburg, Maryland, USA, November 16-17, 2009. NIST, 2009.
Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada, Vivek Menon, Sun Choi, Kushal Datta, and
Vikram Saletore. Efﬁcient 8-bit quantization of transformer neural machine language translation
model. CoRR, abs/1906.00532, 2019.
Johan Bjorck, Carla P. Gomes, Bart Selman, and Kilian Q. Weinberger. Understanding batch nor-
malization. In NeurIPS, pp. 7705–7716, 2018.
Graeme W. Blackwood, Miguel Ballesteros, and Todd Ward. Multilingual neural machine trans-
lation with task-speciﬁc attention. In COLING, pp. 3112–3122. Association for Computational
Linguistics, 2018.
Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive neural networks
for efﬁcient inference. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th In-
ternational Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August
2017, volume 70 of Proceedings of Machine Learning Research, pp. 527–536. PMLR, 2017.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neu-
ral Information Processing Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Tina Eliassi-
Rad, Lyle H. Ungar, Mark Craven, and Dimitrios Gunopulos (eds.), Proceedings of the Twelfth
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Philadel-
phia, PA, USA, August 20-23, 2006, pp. 535–541. ACM, 2006.
Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task
and hardware. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han.
Once-for-all: Train one
network and specialize it for efﬁcient deployment. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
39
V´
ıctor Campos, Brendan Jou, Xavier Gir´
o-i-Nieto, Jordi Torres, and Shih-Fu Chang. Skip RNN:
learning to skip state updates in recurrent neural networks. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Confer-
ence Track Proceedings. OpenReview.net, 2018.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information Pro-
cessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Daniel M. Cer, Mona T. Diab, Eneko Agirre, I˜
nigo Lopez-Gazpio, and Lucia Specia. Semeval-
2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In
Steven Bethard, Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel M. Cer, and
David Jurgens (eds.), Proceedings of the 11th International Workshop on Semantic Evaluation,
SemEval@ACL 2017, Vancouver, Canada, August 3-4, 2017, pp. 1–14. Association for Compu-
tational Linguistics, 2017.
Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, and Chun Chen. Online knowledge distillation
with diverse peers. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020,
The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The
Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York,
NY, USA, February 7-12, 2020, pp. 3430–3437. AAAI Press, 2020a.
Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,
Chunjing Xu, Chao Xu, and Wen Gao.
Pre-trained image processing transformer.
CoRR,
abs/2012.00364, 2020b.
Patrick H. Chen, Si Si, Yang Li, Ciprian Chelba, and Cho-Jui Hsieh. Groupreduce: Block-wise
low-rank approximation for neural language model shrinking. In NeurIPS, pp. 11011–11021,
2018a.
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and
Michael Carbin.
The lottery ticket hypothesis for pre-trained BERT networks.
In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin
(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020c.
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear
memory cost. CoRR, abs/1604.06174, 2016a.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for
contrastive learning of visual representations. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings
of Machine Learning Research, pp. 1597–1607. PMLR, 2020d.
Wenlin Chen, David Grangier, and Michael Auli. Strategies for training large vocabulary neural
language models. In ACL (1). The Association for Computer Linguistics, 2016b.
Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four
GPU hours: A theoretically inspired perspective. ICLR, 2021a.
Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp.
15750–15758. Computer Vision Foundation / IEEE, 2021.
Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. CoRR, abs/2003.04297, 2020e.
Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision
transformers. CoRR, abs/2104.02057, 2021b.
Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, and Zhi Jin. Compressing neural language models by
sparse word representations. In ACL (1). The Association for Computer Linguistics, 2016c.
Yunpeng Chen, Xiaojie Jin, Bingyi Kang, Jiashi Feng, and Shuicheng Yan. Sharing residual units
through collective tensor factorization to improve deep neural networks. In IJCAI, pp. 635–641.
ijcai.org, 2018b.
40
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. CoRR, abs/1904.10509, 2019.
Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Towards the limit of network quantization.
In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
Franc
¸ois Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, pp.
1800–1807. IEEE Computer Society, 2017.
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam´
as
Sarl´
os, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy
Colwell, and Adrian Weller. Rethinking attention with performers. CoRR, abs/2009.14794, 2020.
Xiangxiang Chu, Bo Zhang, and Xudong Li.
Noisy differentiable architecture search.
CoRR,
abs/2005.03566, 2020.
Inseop Chung, Seonguk Park, Jangho Kim, and Nojun Kwak. Feature-map-level online adversarial
knowledge distillation. In Proceedings of the 37th International Conference on Machine Learn-
ing, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning
Research, pp. 2006–2015. PMLR, 2020.
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.
ELECTRA: pre-
training text encoders as discriminators rather than generators. In 8th International Conference
on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
view.net, 2020.
Ronan Collobert, Jason Weston, L´
eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P.
Kuksa. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537,
2011.
Gonc
¸alo M. Correia, Vlad Niculae, and Andr´
e F. T. Martins. Adaptively sparse transformers. In
EMNLP/IJCNLP (1), pp. 2174–2184. Association for Computational Linguistics, 2019.
Marta R. Costa-juss`
a and Jos´
e A. R. Fonollosa. Character-based neural machine translation. In ACL
(2). The Association for Computer Linguistics, 2016.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neu-
ral networks with binary weights during propagations. In Corinna Cortes, Neil D. Lawrence,
Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural Information
Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015,
December 7-12, 2015, Montreal, Quebec, Canada, pp. 3123–3131, 2015.
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu.
Pre-training with whole word masking for chinese BERT. CoRR, abs/1906.08101, 2019.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdi-
nov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. In ACL (1), pp.
2978–2988. Association for Computational Linguistics, 2019.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal
transformers. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pp. 248–255.
IEEE Computer Society, 2009.
Emily L. Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting lin-
ear structure within convolutional networks for efﬁcient evaluation. In Zoubin Ghahramani, Max
Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neu-
ral Information Processing Systems 27: Annual Conference on Neural Information Processing
Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 1269–1277, 2014.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard M. Schwartz, and John
Makhoul. Fast and robust neural network joint models for statistical machine translation. In ACL
(1), pp. 1370–1380. The Association for Computer Linguistics, 2014.
41
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and
Short Papers), pp. 4171–4186, 2019.
William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju
Island, Korea, October 2005, 2005. Asian Federation of Natural Language Processing, 2005.
Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. Multi-task learning for multiple
language translation. In ACL (1), pp. 1723–1732. The Association for Computer Linguistics,
2015.
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
and Hsiao-Wuen Hon. Uniﬁed language model pre-training for natural language understanding
and generation. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´
e-
Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Sys-
tems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada, pp. 13042–13054, 2019.
Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four GPU hours. In IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA,
June 16-20, 2019, pp. 1761–1770, 2019.
Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture
search. In ICLR. OpenReview.net, 2020.
Yinpeng Dong, Jianguo Li, and Renkun Ni. Learning accurate low-bit deep neural networks with
stochastic quantization. In British Machine Vision Conference 2017, BMVC 2017, London, UK,
September 4-7, 2017. BMVA Press, 2017.
Melanie Ducoffe and Fr´
ed´
eric Precioso. Adversarial active learning for deep networks: a margin
based approach. CoRR, abs/1802.09841, 2018.
Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. Low resource dependency parsing: Cross-
lingual parameter sharing in a neural network parser. In ACL (2), pp. 845–850. The Association
for Computer Linguistics, 2015.
Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. In 8th
International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020. OpenReview.net, 2020.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. CoRR,
abs/1808.05377, 2018.
Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisser-
man. The pascal visual object classes (VOC) challenge. Int. J. Comput. Vis., 88(2):303–338,
2010.
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Man-
deep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch,
Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. Beyond
english-centric multilingual machine translation. CoRR, abs/2010.11125, 2020a.
Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with
structured dropout. In ICLR. OpenReview.net, 2020b.
Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R´
emi Gribonval, Herv´
e J´
egou, and
Armand Joulin.
Training with quantization noise for extreme model compression.
CoRR,
abs/2004.07320, 2020c.
Hongchao Fang and Pengtao Xie. CERT: contrastive self-supervised learning for language under-
standing. CoRR, abs/2005.12766, 2020.
Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah A. Smith. Sparse overcom-
plete word vector representations. In ACL (1), pp. 1491–1500. The Association for Computer
Linguistics, 2015.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efﬁcient sparsity. CoRR, abs/2101.03961, 2021.
42
Boyuan Feng, Yuke Wang, Xu Li, Shu Yang, Xueqiao Peng, and Yufei Ding. Sgquant: Squeezing
the last bit on graph neural networks with specialized quantization. In 32nd IEEE International
Conference on Tools with Artiﬁcial Intelligence, ICTAI 2020, Baltimore, MD, USA, November
9-11, 2020, pp. 1044–1052. IEEE, 2020.
Emile Fiesler, Amar Choudry, and H. John Caulﬁeld. Weight discretization paradigm for optical
neural networks. In Hartmut Bartelt (ed.), Optical Interconnections and Networks, volume 1281,
pp. 164 – 173. International Society for Optics and Photonics, SPIE, 1990.
Michael Figurnov, Maxwell D. Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry P. Vetrov,
and Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In 2017
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,
July 21-26, 2017, pp. 1790–1799. IEEE Computer Society, 2017.
Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way, multilingual neural machine trans-
lation with a shared attention mechanism. In HLT-NAACL, pp. 866–875. The Association for
Computational Linguistics, 2016.
Orhan Firat, Kyunghyun Cho, Baskaran Sankaran, Fatos T. Yarman-Vural, and Yoshua Bengio.
Multi-way, multilingual neural machine translation. Comput. Speech Lang., 45:236–252, 2017.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neu-
ral networks. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Alexander Freytag, Erik Rodner, and Joachim Denzler. Selecting inﬂuential examples: Active learn-
ing with expected model output changes. In ECCV (4), volume 8692 of Lecture Notes in Computer
Science, pp. 562–577. Springer, 2014.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence
embeddings. CoRR, abs/2104.08821, 2021.
Shijie Geng, Peng Gao, Zuohui Fu, and Yongfeng Zhang. Romebert: Robust training of multi-exit
BERT. CoRR, abs/2101.09755, 2021.
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. A
survey of quantization methods for efﬁcient neural network inference. CoRR, abs/2103.13630,
2021.
Sucheta Ghosh, Richard Johansson, Giuseppe Riccardi, and Sara Tonelli. Shallow discourse parsing
with conditional random ﬁelds. In Fifth International Joint Conference on Natural Language
Processing, IJCNLP 2011, Chiang Mai, Thailand, November 8-13, 2011, pp. 1071–1079. The
Association for Computer Linguistics, 2011.
John M. Giorgi, Osvald Nitski, Bo Wang, and Gary D. Bader. Declutr: Deep contrastive learning for
unsupervised textual representations. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Nav-
igli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguis-
tics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP
2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 879–895. Association for
Computational Linguistics, 2021.
Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In AISTATS, volume 9 of JMLR Proceedings, pp. 249–256. JMLR.org, 2010.
Aidan N. Gomez, Mengye Ren, Raquel Urtasun, and Roger B. Grosse. The reversible residual
network: Backpropagation without storing activations. In NIPS, pp. 2214–2224, 2017.
Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tie-Yan Liu. Efﬁcient training
of BERT by progressively stacking. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research,
pp. 2337–2346. PMLR, 2019.
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir D. Bourdev. Compressing deep convolutional
networks using vector quantization. CoRR, abs/1412.6115, 2014.
Alperen Gormez and Erdem Koyuncu. Class means as an early exit decision mechanism. CoRR,
abs/2103.01148, 2021.
43
Artem M. Grachev, Dmitry I. Ignatov, and Andrey V. Savchenko. Neural networks compression
for language modeling. In PReMI, volume 10597 of Lecture Notes in Computer Science, pp.
351–357. Springer, 2017.
Alex Graves. Adaptive computation time for recurrent neural networks. CoRR, abs/1603.08983,
2016.
Jean-Bastien Grill, Florian Strub, Florent Altch´
e, Corentin Tallec, Pierre H. Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo ´
Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
Bilal Piot, Koray Kavukcuoglu, R´
emi Munos, and Michal Valko. Bootstrap your own latent - A
new approach to self-supervised learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Had-
sell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Process-
ing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, 2020.
Audrunas Gruslys, R´
emi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-efﬁcient
backpropagation through time. In NIPS, pp. 4125–4133, 2016.
Demi Guo, Alexander M. Rush, and Yoon Kim.
Parameter-efﬁcient transfer learning with diff
pruning. CoRR, abs/2012.07463, 2020.
Yunhui Guo.
A survey on methods and theories of quantized neural networks.
CoRR,
abs/1808.04752, 2018.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In Francis R. Bach and David M. Blei (eds.), Proceedings of the
32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015,
volume 37 of JMLR Workshop and Conference Proceedings, pp. 1737–1746. JMLR.org, 2015.
Michael Gutmann and Aapo Hyv¨
arinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Yee Whye Teh and D. Mike Titterington (eds.), Proceed-
ings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, AISTATS
2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, volume 9 of JMLR Proceedings, pp.
297–304. JMLR.org, 2010.
Thanh-Le Ha, Jan Niehues, and Alexander H. Waibel. Toward multilingual neural machine transla-
tion with universal encoder and decoder. CoRR, abs/1611.04798, 2016.
Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for
efﬁcient neural network. In NIPS, pp. 1135–1143, 2015.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In Yoshua Bengio and Yann LeCun (eds.),
4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico,
May 2-4, 2016, Conference Track Proceedings, 2016.
Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang,
Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu,
Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu. Pre-trained
models: Past, present and future. CoRR, abs/2106.07139, 2021a.
Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural
networks: A survey. CoRR, abs/2102.04906, 2021b.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.
In NeurIPS, pp. 569–579, 2018.
Stephen Jose Hanson and Lorien Y. Pratt. Comparing biases for minimal network construction
with back-propagation. In David S. Touretzky (ed.), Advances in Neural Information Processing
Systems 1, [NIPS Conference, Denver, Colorado, USA, 1988], pp. 177–185. Morgan Kaufmann,
1988.
Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A joint many-
task model: Growing a neural network for multiple NLP tasks. In EMNLP, pp. 1923–1933.
Association for Computational Linguistics, 2017.
Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon: Extensions and
performance comparison. In NIPS, pp. 263–270. Morgan Kaufmann, 1993.
44
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In ICCV, pp. 1026–1034. IEEE Computer
Society, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las
Vegas, NV, USA, June 27-30, 2016, pp. 770–778. IEEE Computer Society, 2016a.
Kaiming He, Georgia Gkioxari, Piotr Doll´
ar, and Ross B. Girshick. Mask R-CNN. In ICCV, pp.
2980–2988. IEEE Computer Society, 2017a.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for
unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision
and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 9726–9735. IEEE,
2020.
Qinyao He, He Wen, Shuchang Zhou, Yuxin Wu, Cong Yao, Xinyu Zhou, and Yuheng Zou. Effective
quantization methods for recurrent neural networks. CoRR, abs/1611.10176, 2016b.
Tao He, Xiaoming Jin, Guiguang Ding, Lan Yi, and Chenggang Yan. Towards better uncertainty
sampling: Active learning with multiple views for deep convolutional neural network. In ICME,
pp. 1360–1365. IEEE, 2019.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural net-
works. In ICCV, pp. 1398–1406. IEEE Computer Society, 2017b.
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. AMC: automl for model com-
pression and acceleration on mobile devices. In Computer Vision - ECCV 2018 - 15th European
Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VII, pp. 815–832, 2018.
Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young Choi. Knowledge transfer via distillation
of activation boundaries formed by hidden neurons. In The Thirty-Third AAAI Conference on
Artiﬁcial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelli-
gence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial
Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pp. 3779–3787.
AAAI Press, 2019.
Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief
nets. Neural Comput., 18(7):1527–1554, 2006.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network.
CoRR, abs/1503.02531, 2015.
Van-Thanh Hoang and Kang-Hyun Jo. Pydmobilenet: Improved version of mobilenets with pyramid
depthwise separable convolution. CoRR, abs/1811.07083, 2018.
Sepp Hochreiter and J¨
urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–
1780, 1997.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, An-
drea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for
NLP. In ICML, volume 97 of Proceedings of Machine Learning Research, pp. 2790–2799. PMLR,
2019.
Andrew Howard, Ruoming Pang, Hartwig Adam, Quoc V. Le, Mark Sandler, Bo Chen, Weijun
Wang, Liang-Chieh Chen, Mingxing Tan, Grace Chu, Vijay Vasudevan, and Yukun Zhu. Search-
ing for mobilenetv3. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV
2019, Seoul, Korea (South), October 27 - November 2, 2019, pp. 1314–1324. IEEE, 2019.
Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for
mobile vision applications. CoRR, abs/1704.04861, 2017.
Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q. Wein-
berger. Multi-scale dense networks for resource efﬁcient image classiﬁcation. In 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,
2018, Conference Track Proceedings. OpenReview.net, 2018.
Jui-Ting Huang, Jinyu Li, Dong Yu, Li Deng, and Yifan Gong. Cross-language knowledge transfer
using multilingual deep neural network with shared hidden layers. In ICASSP, pp. 7304–7308.
IEEE, 2013.
45
Liang Huang, Suphan Fayong, and Yang Guo. Structured perceptron with inexact search. In Hu-
man Language Technologies: Conference of the North American Chapter of the Association of
Computational Linguistics, Proceedings, June 3-8, 2012, Montr´
eal, Canada, pp. 142–151. The
Association for Computational Linguistics, 2012.
Zehao Huang and Naiyan Wang.
Like what you like: Knowledge distill via neuron selectivity
transfer. CoRR, abs/1707.01219, 2017.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. J. Mach.
Learn. Res., 18:187:1–187:30, 2017.
Forrest N. Iandola, Matthew W. Moskewicz, Sergey Karayev, Ross B. Girshick, Trevor Darrell,
and Kurt Keutzer.
Densenet: Implementing efﬁcient convnet descriptor pyramids.
CoRR,
abs/1404.1869, 2014.
Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt
Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size.
CoRR, abs/1602.07360, 2016.
Vladimir Iglovikov and Alexey Shvets. Ternausnet: U-net with VGG11 encoder pre-trained on
imagenet for image segmentation. CoRR, abs/1801.05746, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML, volume 37 of JMLR Workshop and Conference Pro-
ceedings, pp. 448–456. JMLR.org, 2015.
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew G. Howard,
Hartwig Adam, and Dmitry Kalenichenko.
Quantization and training of neural networks for
efﬁcient integer-arithmetic-only inference. In 2018 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 2704–2713.
IEEE Computer Society, 2018.
Paras Jain, Xiangxi Mo, Ajay Jain, Alexey Tumanov, Joseph E. Gonzalez, and Ion Stoica. The ooo
VLIW JIT compiler for GPU inference. CoRR, abs/1901.10008, 2019.
S´
ebastien Jean, KyungHyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target
vocabulary for neural machine translation. In ACL (1), pp. 1–10. The Association for Computer
Linguistics, 2015.
Yacine Jernite, Edouard Grave, Armand Joulin, and Tom´
as Mikolov. Variable computation in recur-
rent neural networks. In 5th International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
Yufan Jiang, Chi Hu, Tong Xiao, Chunliang Zhang, and Jingbo Zhu. Improved differentiable archi-
tecture search for language modeling and named entity recognition. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China,
November 3-7, 2019, pp. 3583–3588, 2019.
Jonghoon Jin, Aysegul Dundar, and Eugenio Culurciello. Flattened convolutional neural networks
for feedforward acceleration. In ICLR (Workshop), 2015.
Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan, and Xiaolin
Hu. Knowledge distillation via route constrained optimization. In 2019 IEEE/CVF International
Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2,
2019, pp. 1345–1354. IEEE, 2019.
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil
Thorat, Fernanda B. Vi´
egas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey
Dean. Google’s multilingual neural machine translation system: Enabling zero-shot translation.
Trans. Assoc. Comput. Linguistics, 5:339–351, 2017.
Aditya Joshi, Vaibhav Tripathi, Kevin Patel, Pushpak Bhattacharyya, and Mark James Carman. Are
word embedding-based features useful for sarcasm detection? In EMNLP, pp. 1006–1011. The
Association for Computational Linguistics, 2016.
Ajay J. Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. Multi-class active learning for image
classiﬁcation. In CVPR, pp. 2372–2379. IEEE Computer Society, 2009.
46
Mandar Joshi, Eunsol Choi, Omer Levy, Daniel S. Weld, and Luke Zettlemoyer. pair2vec: Compo-
sitional word-pair embeddings for cross-sentence inference. In NAACL-HLT (1), pp. 3597–3608.
Association for Computational Linguistics, 2019.
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Span-
bert: Improving pre-training by representing and predicting spans. Trans. Assoc. Comput. Lin-
guistics, 8:64–77, 2020.
Rafal J´
ozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the
limits of language modeling. CoRR, abs/1602.02410, 2016.
Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. Shallow-deep networks: Understanding and
mitigating network overthinking. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Pro-
ceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp.
3301–3310. PMLR, 2019.
Gyuwan Kim and Kyunghyun Cho. Length-adaptive transformer: Train once with length drop,
use anytime with search. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.),
Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,
(Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 6501–6511. Association for Com-
putational Linguistics, 2021.
Jangho Kim, Seonguk Park, and Nojun Kwak. Paraphrasing complex network: Network compres-
sion via factor transfer. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,
Nicol`
o Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing
Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
December 3-8, 2018, Montr´
eal, Canada, pp. 2765–2774, 2018.
Jangho Kim, Minsung Hyun, Inseop Chung, and Nojun Kwak. Feature fusion for online mutual
knowledge distillation. In 25th International Conference on Pattern Recognition, ICPR 2020,
Virtual Event / Milan, Italy, January 10-15, 2021, pp. 4619–4625. IEEE, 2020.
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. I-BERT: integer-
only BERT quantization. CoRR, abs/2101.01321, 2021.
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Com-
pression of deep convolutional neural networks for fast and low power mobile applications. In
ICLR (Poster), 2016a.
Yoon Kim, Yacine Jernite, David A. Sontag, and Alexander M. Rush. Character-aware neural lan-
guage models. In AAAI, pp. 2741–2749. AAAI Press, 2016b.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In ICLR.
OpenReview.net, 2020.
Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM Review, 51
(3):455–500, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. In Advances in Neural Information Processing Systems 25: 26th
Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting
held December 3-6, 2012, Lake Tahoe, Nevada, United States, pp. 1106–1114, 2012.
Mandar Kulkarni and Shirish Subhash Karande. Layer-wise training of deep networks using kernel
similarity. CoRR, abs/1703.07115, 2017.
Siddharth Krishna Kumar. On weight initialization in deep neural networks. CoRR, abs/1704.08863,
2017.
Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the
carbon emissions of machine learning. CoRR, abs/1910.09700, 2019.
Maximilian Lam. Word2bits - quantized word vectors. CoRR, abs/1803.05651, 2018.
Xu Lan, Xiatian Zhu, and Shaogang Gong. Knowledge distillation by on-the-ﬂy native ensemble.
In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`
o Cesa-Bianchi,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montr´
eal, Canada, pp. 7528–7538, 2018.
47
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. ALBERT: A lite BERT for self-supervised learning of language representations. In ICLR.
OpenReview.net, 2020.
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V. Oseledets, and Victor S. Lempitsky.
Speeding-up convolutional neural networks using ﬁne-tuned cp-decomposition. In ICLR (Poster),
2015.
Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In NIPS, pp. 598–605.
Morgan Kaufmann, 1989.
Yann LeCun, L´
eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Snip: single-shot network pruning
based on connection sensitivity. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.
Seung Hyun Lee, Dae Ha Kim, and Byung Cheol Song. Self-supervised knowledge distillation
using singular value decomposition. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu,
and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Munich, Ger-
many, September 8-14, 2018, Proceedings, Part VI, volume 11210 of Lecture Notes in Computer
Science, pp. 339–354. Springer, 2018.
Seunghyun Lee and Byung Cheol Song. Graph-based knowledge distillation by multi-head attention
network. In 30th British Machine Vision Conference 2019, BMVC 2019, Cardiff, UK, September
9-12, 2019, pp. 141. BMVA Press, 2019.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional
computation and automatic sharding. In 9th International Conference on Learning Representa-
tions, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.
Hector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In
Gerhard Brewka, Thomas Eiter, and Sheila A. McIlraith (eds.), Principles of Knowledge Rep-
resentation and Reasoning: Proceedings of the Thirteenth International Conference, KR 2012,
Rome, Italy, June 10-14, 2012. AAAI Press, 2012.
David D. Lewis and William A. Gale. A sequential algorithm for training text classiﬁers. In SIGIR,
pp. 3–12. ACM/Springer, 1994.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
BART: denoising sequence-to-sequence pre-
training for natural language generation, translation, and comprehension. In Dan Jurafsky, Joyce
Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 7871–7880.
Association for Computational Linguistics, 2020.
Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. Unicoder-vl: A universal encoder
for vision and language by cross-modal pre-training. In AAAI, pp. 11336–11344. AAAI Press,
2020a.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for
efﬁcient convnets. In 5th International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.
Lei Li, Yankai Lin, Shuhuai Ren, Deli Chen, Xuancheng Ren, Peng Li, Jie Zhou, and Xu Sun.
Accelerating pre-trained language models via calibrated cascade. CoRR, abs/2012.14682, 2020b.
Qi Li and Heng Ji. Incremental joint extraction of entity mentions and relations. In Proceedings
of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June
22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers, pp. 402–412. The Association for
Computer Linguistics, 2014.
Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. In
Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual
Meeting of the Association for Computational Linguistics and the 11th International Joint Con-
ference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual
Event, August 1-6, 2021, pp. 4582–4597. Association for Computational Linguistics, 2021.
48
Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu, and Xuanjing Huang. Acceler-
ating BERT inference for sequence labeling via early-exit. In Chengqing Zong, Fei Xia, Wenjie
Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on Natural Language
Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp.
189–199. Association for Computational Linguistics, 2021.
Tailin Liang, John Glossner, Lei Wang, and Shaobo Shi. Pruning and quantization for deep neural
network acceleration: A survey. CoRR, abs/2101.09671, 2021.
Kaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su, Xu Sun, and Bin He. A global past-future early exit
method for accelerating inference of pre-trained language models. In Kristina Toutanova, Anna
Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T¨
ur, Iz Beltagy, Steven Bethard, Ryan Cotterell,
Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 2013–2023. Association for Computational
Linguistics, 2021.
Rainer Lienhart and Jochen Maydt. An extended set of haar-like features for rapid object detection.
In Proceedings. international conference on image processing, volume 1, pp. I–I. IEEE, 2002.
Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In Advances in Neural In-
formation Processing Systems 30: Annual Conference on Neural Information Processing Systems
2017, December 4-9, 2017, Long Beach, CA, USA, pp. 2181–2191, 2017.
Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang,
Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou, Zhikang Li, Xiaodong Deng, Jie Liu,
Jinbao Xue, Huiling Zhou, Jianxin Ma, Jin Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, and
Hongxia Yang. M6: A chinese multimodal pretrainer. CoRR, abs/2103.00823, 2021.
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Doll´
ar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In David J. Fleet,
Tom´
as Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision - ECCV 2014 - 13th
European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, volume
8693 of Lecture Notes in Computer Science, pp. 740–755. Springer, 2014.
Ying Lin, Shengqi Yang, Veselin Stoyanov, and Heng Ji. A multi-lingual multi-task architecture
for low-resource sequence labeling. In ACL (1), pp. 799–809. Association for Computational
Linguistics, 2018.
Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu, Jiangtao Feng, Hao Zhou, and Lei Li. Pre-
training multilingual neural machine translation by leveraging alignment information. In EMNLP
(1), pp. 2649–2663. Association for Computational Linguistics, 2020.
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan L.
Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In ECCV (1),
volume 11205 of Lecture Notes in Computer Science, pp. 19–35. Springer, 2018a.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search. In
7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,
May 6-9, 2019, 2019a.
Iou-Jen Liu, Jian Peng, and Alexander G. Schwing. Knowledge ﬂow: Improve upon your teachers.
In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019. OpenReview.net, 2019b.
Liyuan Liu, Jingbo Shang, Xiang Ren, Frank Fangzheng Xu, Huan Gui, Jian Peng, and Jiawei Han.
Empower sequence labeling with task-aware neural language model. In AAAI, pp. 5253–5260.
AAAI Press, 2018b.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-
train, prompt, and predict: A systematic survey of prompting methods in natural language pro-
cessing. CoRR, abs/2107.13586, 2021a.
Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. Fastbert: a self-distilling
BERT with adaptive inference time. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.
Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 6035–6044. Association for Computational
Linguistics, 2020a.
49
Xianggen Liu, Lili Mou, Haotian Cui, Zhengdong Lu, and Sen Song. JUMPER: learning when to
make classiﬁcation decisions in reading. CoRR, abs/1807.02314, 2018c.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt
tuning can be comparable to ﬁne-tuning universally across scales and tasks, 2021b.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT
understands, too. CoRR, abs/2103.10385, 2021c.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis,
and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Trans.
Assoc. Comput. Linguistics, 8:726–742, 2020b.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. In ICLR (Poster). OpenReview.net, 2019c.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan. Unsupervised domain adaptation
with residual transfer networks. In NIPS, pp. 136–144, 2016.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolin-
guistic representations for vision-and-language tasks. In NeurIPS, pp. 13–23, 2019.
Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, and Xu Sun. CAPT: contrastive pre-
training for learning denoised sequence representations. CoRR, abs/2010.06351, 2020.
Jian-Hao Luo, Hao Zhang, Hong-Yu Zhou, Chen-Wei Xie, Jianxin Wu, and Weiyao Lin. Thinet:
Pruning CNN ﬁlters for a thinner net. IEEE Trans. Pattern Anal. Mach. Intell., 41(10):2525–2538,
2019.
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization.
In NeurIPS, pp. 7827–7838, 2018.
Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task
sequence to sequence learning. In ICLR (Poster), 2016.
Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and Dawei Song. A
tensorized transformer for language modeling. In NeurIPS, pp. 2229–2239, 2019.
Matthew MacKay, Paul Vicol, Jimmy Ba, and Roger B. Grosse. Reversible recurrent neural net-
works. In NeurIPS, pp. 9043–9054, 2018.
Andr´
e F. T. Martins and Ram´
on Fernandez Astudillo. From softmax to sparsemax: A sparse model
of attention and multi-label classiﬁcation. In ICML, volume 48 of JMLR Workshop and Confer-
ence Proceedings, pp. 1614–1623. JMLR.org, 2016.
Sameen Maruf, Andr´
e F. T. Martins, and Gholamreza Haffari. Selective attention for context-aware
neural machine translation. In NAACL-HLT (1), pp. 3092–3102. Association for Computational
Linguistics, 2019.
JS McCarley, Rishav Chakravarti, and Avirup Sil.
Structured pruning of a bert-based question
answering model. arXiv preprint arXiv:1910.06360, 2019.
Ryan T. McDonald, Keith B. Hall, and Gideon Mann. Distributed training strategies for the struc-
tured perceptron. In Human Language Technologies: Conference of the North American Chapter
of the Association of Computational Linguistics, Proceedings, June 2-4, 2010, Los Angeles, Cal-
ifornia, USA, pp. 456–464. The Association for Computational Linguistics, 2010.
Joseph Mellor, Jack Turner, Amos J. Storkey, and Elliot J. Crowley. Neural architecture search
without training. CoRR, abs/2006.04647, 2020.
Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In Ad-
vances in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
14014–14024, 2019.
Tom´
as Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word represen-
tations in vector space. In ICLR (Workshop Poster), 2013a.
Tom´
as Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.
Distributed
representations of words and phrases and their compositionality. In NIPS, pp. 3111–3119, 2013b.
50
Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan
Ghasemzadeh. Improved knowledge distillation via teacher assistant. In The Thirty-Fourth AAAI
Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of
Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Ad-
vances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 5191–
5198. AAAI Press, 2020.
Dmytro Mishkin and Jiri Matas. All you need is a good init. In ICLR (Poster), 2016.
Andriy Mnih and Geoffrey E. Hinton. A scalable hierarchical distributed language model. In NIPS,
pp. 1081–1088. Curran Associates, Inc., 2008.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efﬁcient inference. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings,
2017.
Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In
AISTATS. Society for Artiﬁcial Intelligence and Statistics, 2005.
Lili Mou, Ran Jia, Yan Xu, Ge Li, Lu Zhang, and Zhi Jin. Distilling word embeddings: An encoding
approach. In CIKM, pp. 1977–1980. ACM, 2016.
Sharan Narang, Greg Diamos, Shubho Sengupta, and Erich Elsen. Exploring sparsity in recur-
rent neural networks. In 5th International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017a.
Sharan Narang, Eric Undersander, and Gregory F. Diamos. Block-sparse recurrent neural networks.
CoRR, abs/1711.02782, 2017b.
Hieu Tat Nguyen and Arnold W. M. Smeulders. Active learning using pre-clustering. In ICML,
volume 69 of ACM International Conference Proceeding Series. ACM, 2004.
Vlad Niculae and Mathieu Blondel. A regularized framework for sparse and structured neural atten-
tion. In NIPS, pp. 3338–3348, 2017.
Matan Ben Noach and Yoav Goldberg. Compressing pre-trained language models by matrix decom-
position. In Kam-Fai Wong, Kevin Knight, and Hua Wu (eds.), Proceedings of the 1st Conference
of the Asia-Paciﬁc Chapter of the Association for Computational Linguistics and the 10th Inter-
national Joint Conference on Natural Language Processing, AACL/IJCNLP 2020, Suzhou, China,
December 4-7, 2020, pp. 884–889. Association for Computational Linguistics, 2020.
Maxime Oquab, L´
eon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level im-
age representations using convolutional neural networks. In CVPR, pp. 1717–1724. IEEE Com-
puter Society, 2014.
Joachim Ott, Zhouhan Lin, Ying Zhang, Shih-Chii Liu, and Yoshua Bengio. Recurrent neural net-
works with limited numerical precision. CoRR, abs/1611.07065, 2016.
Dae Young Park, Moon-Hyun Cha, Changwook Jeong, Daesin Kim, and Bohyung Han. Learning
student-friendly teacher networks for knowledge distillation. CoRR, abs/2102.07650, 2021.
Eunhyeok Park, Dongyoung Kim, Soobeom Kim, Yong-Deok Kim, Gunhee Kim, Sungroh Yoon,
and Sungjoo Yoo. Big/little deep neural network for ultra low power inference. In Gabriela Nico-
lescu and Andreas Gerstlauer (eds.), 2015 International Conference on Hardware/Software Code-
sign and System Synthesis, CODES+ISSS 2015, Amsterdam, Netherlands, October 4-9, 2015, pp.
124–132. IEEE, 2015.
Seonguk Park and Nojun Kwak. FEED: feature-level ensemble for knowledge distillation. CoRR,
abs/1909.10754, 2019.
Baoyun Peng, Xiao Jin, Dongsheng Li, Shunfeng Zhou, Yichao Wu, Jiaheng Liu, Zhaoning Zhang,
and Yu Liu. Correlation congruence for knowledge distillation. In 2019 IEEE/CVF International
Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2,
2019, pp. 5006–5015. IEEE, 2019.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In EMNLP, pp. 1532–1543. ACL, 2014.
Ben Peters, Vlad Niculae, and Andr´
e F. T. Martins. Sparse sequence-to-sequence models. In ACL
(1), pp. 1504–1519. Association for Computational Linguistics, 2019.
51
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In NAACL-HLT, pp. 2227–2237.
Association for Computational Linguistics, 2018.
Fabio Petroni, Tim Rockt¨
aschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,
and Alexander H. Miller. Language models as knowledge bases? In Kentaro Inui, Jing Jiang,
Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 2463–2473.
Association for Computational Linguistics, 2019.
Jonas Pfeiffer, Andreas R¨
uckl´
e, Clifton Poth, Aishwarya Kamath, Ivan Vulic, Sebastian Ruder,
Kyunghyun Cho, and Iryna Gurevych. Adapterhub: A framework for adapting transformers. In
EMNLP (Demos), pp. 46–54. Association for Computational Linguistics, 2020.
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efﬁcient neural architecture
search via parameters sharing. In International Conference on Machine Learning, pp. 4095–
4104. PMLR, 2018.
Geoff Pleiss, Danlu Chen, Gao Huang, Tongcheng Li, Laurens van der Maaten, and Kilian Q. Wein-
berger. Memory-efﬁcient implementation of densenets. CoRR, abs/1707.06990, 2017.
Bryan A Plummer, Nikoli Dryden, Julius Frost, Torsten Hoeﬂer, and Kate Saenko. Neural parameter
allocation search. arXiv preprint arXiv:2006.10598, 2020.
Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quan-
tization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.
Sai Prasanna, Anna Rogers, and Anna Rumshisky. When BERT plays the lottery, all tickets are
winning. In EMNLP (1), pp. 3208–3229. Association for Computational Linguistics, 2020.
Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh. Fully quantized transformer for machine
translation. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Online Event,
16-20 November 2020, pp. 1–14. Association for Computational Linguistics, 2020.
Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. In EACL (2),
pp. 157–163. Association for Computational Linguistics, 2017.
Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and
Ming Zhou. Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training. In
Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Lin-
guistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings
of ACL, pp. 2401–2410, 2020.
Ye Qi, Devendra Singh Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. When
and why are pre-trained word embeddings useful for neural machine translation? In NAACL-HLT
(2), pp. 529–535. Association for Computational Linguistics, 2018.
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained
models for natural language processing: A survey. CoRR, abs/2003.08271, 2020.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations
toward training trillion parameter models. In SC, pp. 20. IEEE/ACM, 2020.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for
machine comprehension of text. In Jian Su, Xavier Carreras, and Kevin Duh (eds.), Proceedings
of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pp. 2383–2392. The Association for Computational
Linguistics, 2016.
52
Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions
for squad. In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting
of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20,
2018, Volume 2: Short Papers, pp. 784–789. Association for Computational Linguistics, 2018.
Bharath Ramsundar, Steven M. Kearnes, Patrick Riley, Dale Webster, David E. Konerding, and
Vijay S. Pande. Massively multitask networks for drug discovery. CoRR, abs/1502.02072, 2015.
Hiranmayi Ranganathan, Hemanth Venkateswara, Shayok Chakraborty, and Sethuraman Pan-
chanathan. Deep active learning for image classiﬁcation. In ICIP, pp. 3934–3938. IEEE, 2017.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classiﬁcation using binary convolutional neural networks. In Bastian Leibe, Jiri Matas, Nicu Sebe,
and Max Welling (eds.), Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam,
The Netherlands, October 11-14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in
Computer Science, pp. 525–542. Springer, 2016.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for image
classiﬁer architecture search. In The Thirty-Third AAAI Conference on Artiﬁcial Intelligence,
AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence Conference, IAAI
2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2019,
Honolulu, Hawaii, USA, January 27 - February 1, 2019, pp. 4780–4789, 2019.
Siva Reddy, Danqi Chen, and Christopher D. Manning. Coqa: A conversational question answering
challenge. Trans. Assoc. Comput. Linguistics, 7:249–266, 2019.
Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time
object detection with region proposal networks. In NIPS, pp. 91–99, 2015.
Roberto Rigamonti, Amos Sironi, Vincent Lepetit, and Pascal Fua. Learning separable ﬁlters. In
2013 IEEE Conference on Computer Vision and Pattern Recognition, Portland, OR, USA, June
23-28, 2013, pp. 2754–2761. IEEE Computer Society, 2013.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. In Yoshua Bengio and Yann LeCun (eds.), 3rd
International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May
7-9, 2015, Conference Track Proceedings, 2015.
Nicholas Roy and Andrew McCallum. Toward optimal active learning through monte carlo estima-
tion of error reduction. ICML, Williamstown, 2:441–448, 2001.
Sebastian Ruder. Neural transfer learning for natural language processing. PhD thesis, NUI Gal-
way, 2019.
Sebastian Ruder, Parsa Ghaffari, and John G. Breslin. Knowledge adaptation: Teaching to adapt.
CoRR, abs/1702.02052, 2017.
Sebastian Ruder, Ivan Vulic, and Anders Søgaard. A survey of cross-lingual word embedding mod-
els. J. Artif. Intell. Res., 65:569–631, 2019.
Diego Rueda-Plata, Ra´
ul Ramos-Poll´
an, and Fabio A. Gonz´
alez.
Supervised greedy layer-wise
training for deep convolutional networks with small datasets. In Manuel N´
u˜
nez, Ngoc Thanh
Nguyen, David Camacho, and Bogdan Trawinski (eds.), Computational Collective Intelligence -
7th International Conference, ICCCI 2015, Madrid, Spain, September 21-23, 2015. Proceedings,
Part I, volume 9329 of Lecture Notes in Computer Science, pp. 275–284. Springer, 2015.
Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive
sentence summarization. In Llu´
ıs M`
arquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and
Yuval Marton (eds.), Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp. 379–389. The
Association for Computational Linguistics, 2015.
Tara N. Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-
rank matrix factorization for deep neural network training with high-dimensional output targets.
In ICASSP, pp. 6655–6659. IEEE, 2013.
Tim Salimans and Diederik P. Kingma.
Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. In NIPS, pp. 901, 2016.
53
Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
Mobilenetv2: Inverted residuals and linear bottlenecks. In 2018 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp.
4510–4520. Computer Vision Foundation / IEEE Computer Society, 2018.
Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by
ﬁne-tuning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-
ization help optimization? In NeurIPS, pp. 2488–2498, 2018.
Pedro Savarese and Michael Maire. Learning implicitly recurrent cnns through parameter sharing.
In ICLR (Poster). OpenReview.net, 2019.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. In ICLR, 2014.
Teven Le Scao and Alexander M. Rush. How many data points is a prompt worth? In NAACL-HLT,
pp. 2627–2636. Association for Computational Linguistics, 2021.
Timo Schick and Hinrich Sch¨
utze. It’s not just size that matters: Small language models are also
few-shot learners. In NAACL-HLT, pp. 2339–2352. Association for Computational Linguistics,
2021.
Christopher Schr¨
oder, Andreas Niekler, and Martin Potthast. Uncertainty-based query strategies for
active learning with transformers. CoRR, abs/2107.05687, 2021.
Tal Schuster, Adam Fisch, Tommi S. Jaakkola, and Regina Barzilay. Consistent accelerated infer-
ence via conﬁdent adaptive transformers. CoRR, abs/2104.08803, 2021.
Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. Commun. ACM, 63(12):
54–63, 2020a.
Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, and Noah A. Smith. The
right tool for the job: Matching model and instance complexities. In Dan Jurafsky, Joyce Chai,
Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 6640–6651.
Association for Computational Linguistics, 2020b.
Abigail See, Minh-Thang Luong, and Christopher D. Manning. Compression of neural machine
translation models via pruning. In Proceedings of the 20th SIGNLL Conference on Computational
Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, pp. 291–301,
2016.
Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with
pointer-generator networks. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada,
July 30 - August 4, Volume 1: Long Papers, pp. 1073–1083. Association for Computational Lin-
guistics, 2017.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. In ICLR (Poster). OpenReview.net, 2018.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In ACL (1). The Association for Computer Linguistics, 2016.
Min Joon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. Neural speed reading via skim-
rnn. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC,
Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.
Yanyao Shen, Hyokun Yun, Zachary C. Lipton, Yakov Kronrod, and Animashree Anandkumar.
Deep active learning for named entity recognition. In ICLR (Poster). OpenReview.net, 2018.
Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. Reasonet: Learning to stop reading
in machine comprehension. In Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17, 2017, pp.
1047–1055. ACM, 2017.
54
Tao Sheng, Chen Feng, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, and Mickey Aleksic.
A
quantization-friendly separable convolution for mobilenets. CoRR, abs/1803.08607, 2018.
Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. Compositional em-
beddings using complementary partitions for memory-efﬁcient recommendation systems.
In
KDD, pp. 165–175. ACM, 2020.
Wenxian Shi, Yuxuan Song, Hao Zhou, Bohan Li, and Lei Li.
Learning from deep model via
exploring local targets, 2021. URL https://openreview.net/forum?id=5slGDu_bVc6.
Raphael Shu and Hideki Nakayama. Compressing word embeddings via deep compositional code
learning. In ICLR (Poster). OpenReview.net, 2018.
Yao Shu, Wei Wang, and Shaofeng Cai. Understanding architectures learnt by cell-based neural
architecture search. In ICLR. OpenReview.net, 2020.
Marcel Simon, Erik Rodner, and Joachim Denzler. Imagenet pre-trained models with batch normal-
ization. CoRR, abs/1612.01452, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015.
David R. So, Quoc V. Le, and Chen Liang. The evolved transformer. In Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-
fornia, USA, pp. 5877–5886, 2019.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA,
A meeting of SIGDAT, a Special Interest Group of the ACL, pp. 1631–1642. ACL, 2013.
Anders Søgaard and Yoav Goldberg. Deep multi-task learning with low level tasks supervised at
lower layers. In ACL (2). The Association for Computer Linguistics, 2016.
Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free training
of multilayer neural networks with continuous or discrete weights. In Zoubin Ghahramani, Max
Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neu-
ral Information Processing Systems 27: Annual Conference on Neural Information Processing
Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 963–971, 2014.
Suraj Srinivas and R. Venkatesh Babu. Data-free parameter pruning for deep neural networks. In
BMVC, pp. 31.1–31.12. BMVA Press, 2015.
Suraj Srinivas and Franc
¸ois Fleuret. Knowledge transfer with jacobian matching. In Jennifer G.
Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsm¨
assan, Stockholm, Sweden, July 10-15, 2018, volume 80 of
Proceedings of Machine Learning Research, pp. 4730–4738. PMLR, 2018.
Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander A. Alemi, and Andrew Gordon Wil-
son. Does knowledge distillation really work? CoRR, abs/2106.05945, 2021.
Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep
learning in NLP. In Proceedings of the 57th Conference of the Association for Computational
Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp.
3645–3650, 2019.
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention
span in transformers. In ACL (1), pp. 331–335. Association for Computational Linguistics, 2019.
Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for BERT model
compression. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of
the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong,
China, November 3-7, 2019, pp. 4322–4331. Association for Computational Linguistics, 2019a.
Tianxiang Sun, Yunhua Zhou, Xiangyang Liu, Xinyu Zhang, Hao Jiang, Zhao Cao, Xuanjing Huang,
and Xipeng Qiu. Early exiting with ensemble internal classiﬁers. CoRR, abs/2105.13792, 2021.
Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang
Zhu, Hao Tian, and Hua Wu. ERNIE: enhanced representation through knowledge integration.
CoRR, abs/1904.09223, 2019b.
55
Charles Sutton and Andrew McCallum. An introduction to conditional random ﬁelds. Found. Trends
Mach. Learn., 4(4):267–373, 2012.
Kenji Suzuki, Isao Horiba, and Noboru Sugie. A simple neural network pruning algorithm with
application to ﬁlter synthesis. Neural Process. Lett., 13(1):43–53, 2001.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
In CVPR, pp. 1–9. IEEE Computer Society, 2015.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi.
Inception-v4,
inception-resnet and the impact of residual connections on learning. In AAAI, pp. 4278–4284.
AAAI Press, 2017.
Shyam Anil Tailor, Javier Fernandez-Marques, and Nicholas Donald Lane.
Degree-quant:
Quantization-aware training for graph neural networks. In International Conference on Learning
Representations, 2021.
Sho Takase and Shun Kiyono. Lessons on parameter sharing across layers in transformers. CoRR,
abs/2104.06022, 2021.
Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu Jia, En-Yu Yang, Marco Donato, Victor
Sanh, Paul N. Whatmough, Alexander M. Rush, David Brooks, and Gu-Yeon Wei. Edgebert:
Sentence-level energy optimizations for latency-aware multi-task NLP inference. In MICRO, pp.
830–844. ACM, 2021.
Mingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural
networks. 97:6105–6114, 2019.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
Quoc V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In CVPR, pp. 2820–
2828. Computer Vision Foundation / IEEE, 2019a.
Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-Yan Liu. Multilingual neural machine trans-
lation with knowledge distillation. In ICLR (Poster). OpenReview.net, 2019b.
C.Z. Tang and H.K. Kwan.
Multilayer feedforward neural networks with single powers-of-two
weights. IEEE Transactions on Signal Processing, 41(8):2724–2727, 1993. doi: 10.1109/78.
229903.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged con-
sistency targets improve semi-supervised deep learning results. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.
1195–1204, 2017.
Surat Teerapittayanon, Bradley McDanel, and H. T. Kung. Branchynet: Fast inference via early
exiting from deep neural networks. In 23rd International Conference on Pattern Recognition,
ICPR 2016, Canc´
un, Mexico, December 4-8, 2016, pp. 2464–2469. IEEE, 2016.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Andrea Vedaldi,
Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision - ECCV 2020 -
16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI, volume
12356 of Lecture Notes in Computer Science, pp. 776–794. Springer, 2020.
Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In 2019 IEEE/CVF
International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -
November 2, 2019, pp. 1365–1374. IEEE, 2019.
Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and Dan Roth. Cross-lingual models of word em-
beddings: An empirical comparison. In ACL (1). The Association for Computer Linguistics,
2016.
A¨
aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. CoRR, abs/1807.03748, 2018.
Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. Improving the speed of neural networks on
CPUs. In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2011.
56
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998–6008, 2017.
Andreas Veit and Serge J. Belongie. Convolutional networks with adaptive inference graphs. Int. J.
Comput. Vis., 128(3):730–741, 2020.
Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga, and Jay Yagnik. Deep networks with
large output spaces. In ICLR (Workshop), 2015.
Paul Viola and Michael Jones. Rapid object detection using a boosted cascade of simple features.
In Proceedings of the 2001 IEEE computer society conference on computer vision and pattern
recognition. CVPR 2001, volume 1, pp. I–I. Ieee, 2001.
Paul Viola and Michael J Jones. Robust real-time face detection. International journal of computer
vision, 57(2):137–154, 2004.
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head
self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of
the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy,
July 28- August 2, 2019, Volume 1: Long Papers, pp. 5797–5808, 2019.
Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. Spot: Better frozen model
adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th
International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019. OpenReview.net, 2019a.
Changhan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine translation with byte-level sub-
words. In AAAI, pp. 9154–9160. AAAI Press, 2020a.
Hanchen Wang, Defu Lian, Ying Zhang, Lu Qin, Xiangjian He, Yiguang Lin, and Xuemin Lin.
Binarized graph neural network. World Wide Web, 24(3):825–848, 2021a.
Linnan Wang, Wei Wu, Zenglin Xu, Jianxiong Xiao, and Yi Yang. BLASX: A high performance
level-3 BLAS library for heterogeneous multi-gpu computing. In ICS, pp. 20:1–20:11. ACM,
2016.
Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu,
and Tim Kraska. Superneurons: dynamic GPU memory management for training deep neural
networks. In PPOPP, pp. 41–53. ACM, 2018a.
Min Wang, Baoyuan Liu, and Hassan Foroosh. Factorized convolutional neural networks. In ICCV
Workshops, pp. 545–553. IEEE Computer Society, 2017.
Peiqi Wang, Xinfeng Xie, Lei Deng, Guoqi Li, Dongsheng Wang, and Yuan Xie. Hitnet: Hybrid
ternary recurrent neural network. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kris-
ten Grauman, Nicol`
o Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information
Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018,
NeurIPS 2018, December 3-8, 2018, Montr´
eal, Canada, pp. 602–612, 2018b.
Tao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. Distilling object detectors with ﬁne-grained
feature imitation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019,
Long Beach, CA, USA, June 16-20, 2019, pp. 4933–4942. Computer Vision Foundation / IEEE,
2019b.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In Proceedings of the 37th International Conference on
Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of
Machine Learning Research, pp. 9929–9939. PMLR, 2020.
Wei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi Bao, Liwei Peng, and Luo Si. Struct-
bert: Incorporating language structures into pre-training for deep language understanding. In 8th
International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020. OpenReview.net, 2020b.
Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, and Lei Li. LightSeq: A high performance
inference library for transformers. In Proceedings of the 2021 Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics: Human Language Technologies:
57
Industry Papers (NAACL-HLT), pp. 113–120. Association for Computational Linguistics, June
2021b.
Xin Wang, Yujia Luo, Daniel Crankshaw, Alexey Tumanov, Fisher Yu, and Joseph E. Gonzalez. IDK
cascades: Fast deep learning by learning not to overthink. In Amir Globerson and Ricardo Silva
(eds.), Proceedings of the Thirty-Fourth Conference on Uncertainty in Artiﬁcial Intelligence, UAI
2018, Monterey, California, USA, August 6-10, 2018, pp. 580–590. AUAI Press, 2018c.
Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E. Gonzalez. Skipnet: Learning dy-
namic routing in convolutional networks. In Vittorio Ferrari, Martial Hebert, Cristian Sminchis-
escu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Munich,
Germany, September 8-14, 2018, Proceedings, Part XIII, volume 11217 of Lecture Notes in Com-
puter Science, pp. 420–436. Springer, 2018d.
Yong Wang, Longyue Wang, Victor O. K. Li, and Zhaopeng Tu. On the sparsity of neural machine
translation models. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 1060–1066, 2020c.
Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. Glance and focus:
a dynamic approach to reducing spatial redundancy in image classiﬁcation. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information Pro-
cessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020d.
Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao Huang. Not all images are worth 16x16
words: Dynamic vision transformers with adaptive sequence length. CoRR, abs/2105.15075,
2021c.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments.
Trans. Assoc. Comput. Linguistics, 7:625–641, 2019.
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. CoRR,
abs/2109.01652, 2021.
Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In Marilyn A. Walker, Heng Ji, and Amanda Stent
(eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans,
Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pp. 1112–1122. Association for Com-
putational Linguistics, 2018.
Genta Indra Winata, Andrea Madotto, Jamin Shin, Elham J. Barezi, and Pascale Fung. On the effec-
tiveness of low-rank matrix factorization for LSTM model compression. CoRR, abs/1908.09982,
2019.
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian,
Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient convnet design via
differentiable neural architecture search. In CVPR, pp. 10734–10742. Computer Vision Founda-
tion / IEEE, 2019.
Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep
neural networks. In 6th International Conference on Learning Representations, ICLR 2018, Van-
couver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
2018a.
Yuxin Wu and Kaiming He. Group normalization. Int. J. Comput. Vis., 128(3):742–755, 2020.
Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. CLEAR: contrastive
learning for sentence representation. CoRR, abs/2012.15466, 2020.
Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S. Davis, Kristen Grauman,
and Rog´
erio Schmidt Feris. Blockdrop: Dynamic inference paths in residual networks. In 2018
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT,
USA, June 18-22, 2018, pp. 8817–8826. Computer Vision Foundation / IEEE Computer Society,
2018b.
Liuyu Xiang, Guiguang Ding, and Jungong Han. Learning from multiple experts: Self-paced knowl-
edge distillation for long-tailed classiﬁcation. In Andrea Vedaldi, Horst Bischof, Thomas Brox,
58
and Jan-Michael Frahm (eds.), Computer Vision - ECCV 2020 - 16th European Conference, Glas-
gow, UK, August 23-28, 2020, Proceedings, Part V, volume 12350 of Lecture Notes in Computer
Science, pp. 247–263. Springer, 2020.
Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for
accelerating BERT inference. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault
(eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,
ACL 2020, Online, July 5-10, 2020, pp. 2246–2251. Association for Computational Linguistics,
2020.
Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. Berxit: Early exiting for BERT with better ﬁne-
tuning and extension to regression. In Paola Merlo, J¨
org Tiedemann, and Reut Tsarfaty (eds.),
Proceedings of the 16th Conference of the European Chapter of the Association for Computa-
tional Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pp. 91–104. Associa-
tion for Computational Linguistics, 2021.
Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. Bert-of-theseus: Compress-
ing BERT by progressive module replacing. In Bonnie Webber, Trevor Cohn, Yulan He, and
Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 7859–7869. Association for Com-
putational Linguistics, 2020.
Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian J. McAuley, and Furu Wei.
Be-
yond preserved accuracy: Evaluating loyalty and robustness of BERT compression.
CoRR,
abs/2109.03228, 2021a.
Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and
improving layer normalization. In NeurIPS, pp. 4383–4393, 2019.
Jingjing Xu, Liang Zhao, Junyang Lin, Rundong Gao, Xu Sun, and Hongxia Yang. KNAS: green
neural architecture search. In ICML, volume 139 of Proceedings of Machine Learning Research,
pp. 11613–11625. PMLR, 2021b.
Kaisheng Xu, Xu Shen, Ting Yao, Xinmei Tian, and Tao Mei. Greedy layer-wise training of long
short term memory networks. In 2018 IEEE International Conference on Multimedia & Expo
Workshops, ICME Workshops 2018, San Diego, CA, USA, July 23-27, 2018, pp. 1–6. IEEE Com-
puter Society, 2018.
Jian Xue, Jinyu Li, Dong Yu, Mike Seltzer, and Yifan Gong. Singular value decomposition based
low-footprint speaker adaptation and personalization for deep neural network. In IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing, ICASSP 2014, Florence, Italy,
May 4-9, 2014, pp. 6359–6363, 2014.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. In
Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T¨
ur, Iz Beltagy, Steven
Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 483–498. As-
sociation for Computational Linguistics, 2021.
Cheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He, and Jingqiao Zhang. Progres-
sively stacking 2.0: A multi-stage layerwise training method for BERT training speedup. CoRR,
abs/2011.13635, 2020a.
Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang.
Resolution adaptive
networks for efﬁcient inference. In 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 2366–2375. Computer Vision
Foundation / IEEE, 2020b.
Zhilin Yang, Ruslan Salakhutdinov, and William W. Cohen. Transfer learning for sequence tagging
with hierarchical recurrent networks. In ICLR (Poster). OpenReview.net, 2017.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V.
Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Process-
ing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 5754–5764,
2019.
59
Junho Yim, Donggyu Joo, Ji-Hoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast
optimization, network minimization and transfer learning. In 2017 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 7130–
7138. IEEE Computer Society, 2017.
Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, and Frank Hutter. Nas-
bench-101: Towards reproducible neural architecture search. In Proceedings of the 36th Interna-
tional Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
USA, pp. 7105–7114, 2019.
Donggeun Yoo and In So Kweon. Learning loss for active learning. In CVPR, pp. 93–102. Computer
Vision Foundation / IEEE, 2019.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In NIPS, pp. 3320–3328, 2014.
Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from multiple teacher networks. In
Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, Halifax, NS, Canada, August 13 - 17, 2017, pp. 1285–1294. ACM, 2017.
Adams Wei Yu, Hongrae Lee, and Quoc V. Le. Learning to skim text. In Regina Barzilay and Min-
Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pp.
1880–1890. Association for Computational Linguistics, 2017.
Jiahui Yu and Thomas S. Huang. Universally slimmable networks and improved training tech-
niques. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul,
Korea (South), October 27 - November 2, 2019, pp. 1803–1811. IEEE, 2019.
Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas S. Huang. Slimmable neural networks.
In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019. OpenReview.net, 2019.
Keyi Yu, Yang Liu, Alexander G. Schwing, and Jian Peng. Fast and accurate text classiﬁcation:
Skimming, rereading and early stopping. In 6th International Conference on Learning Represen-
tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceed-
ings. OpenReview.net, 2018.
Tong Yu and Hong Zhu. Hyper-parameter optimization: A review of algorithms and applications.
CoRR, abs/2003.05689, 2020.
Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8BERT: quantized 8bit BERT.
CoRR, abs/1910.06188, 2019.
Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the per-
formance of convolutional neural networks via attention transfer. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net, 2017.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Onta˜
n´
on, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Trans-
formers for longer sequences. In NeurIPS, 2020.
Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
ECCV (1), volume 8689 of Lecture Notes in Computer Science, pp. 818–833. Springer, 2014.
Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter.
Understanding and robustifying differentiable architecture search. In 8th International Confer-
ence on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.
Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without
normalization. In ICLR (Poster). OpenReview.net, 2019.
Matthew Shunshi Zhang and Bradly C. Stadie.
One-shot pruning of recurrent neural networks
by jacobian spectrum evaluation. In 8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.
Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert:
Distillation-aware ultra-low bit BERT. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu
60
(eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-
ing, EMNLP 2020, Online, November 16-20, 2020, pp. 509–521. Association for Computational
Linguistics, 2020.
Ying Zhang, Tao Xiang, Timothy M. Hospedales, and Huchuan Lu. Deep mutual learning. In 2018
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT,
USA, June 18-22, 2018, pp. 4320–4328. IEEE Computer Society, 2018.
Zhi Zhang, Guanghan Ning, and Zhihai He. Knowledge projection for deep neural networks. CoRR,
abs/1710.09505, 2017.
Yiren Zhao, Duo Wang, Daniel Bates, Robert D. Mullins, Mateja Jamnik, and Pietro Li`
o. Learned
low precision graph neural networks. CoRR, abs/2009.09232, 2020.
Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Da-
long Du, Chang Huang, and Philip H. S. Torr. Conditional random ﬁelds as recurrent neural
networks. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago,
Chile, December 7-13, 2015, pp. 1529–1537, 2015.
Mingyi Zhou, Yipeng Liu, Zhen Long, Longxi Chen, and Ce Zhu. Tensor rank learning in CP
decomposition via convolutional neural network. Signal Process. Image Commun., 73:12–21,
2019.
Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian J. McAuley, Ke Xu, and Furu Wei. BERT loses
patience: Fast and robust inference with early exit. In Hugo Larochelle, Marc’Aurelio Ranzato,
Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Wangchunshu Zhou, Tao Ge, Ke Xu, and Furu Wei. Improving sequence-to-sequence pre-training
via sequence span rewriting. CoRR, abs/2101.00416, 2021a.
Wangchunshu Zhou, Canwen Xu, and Julian J. McAuley. Meta learning for knowledge distillation.
CoRR, abs/2106.04570, 2021b.
Wei Zhu. Leebert: Learned early exit for BERT with cross-level optimization. In Chengqing Zong,
Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International Joint Conference on Natural
Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6,
2021, pp. 2968–2980. Association for Computational Linguistics, 2021.
61
